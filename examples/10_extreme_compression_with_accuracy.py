"""
Reality Stone ê·¹í•œ ì••ì¶• + ì •í™•ë„ ë³´ì¡´ í…ŒìŠ¤íŠ¸
ë‹¤ë‹¨ê³„ ì••ì¶• ì „ëµìœ¼ë¡œ ë†’ì€ ì••ì¶•ë¥ ê³¼ ì •í™•ë„ ë™ì‹œ ë‹¬ì„±

ëª©í‘œ: 50%+ ì••ì¶•ë¥  + 90%+ ì •í™•ë„ ë³´ì¡´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
import warnings
warnings.filterwarnings("ignore")


class IntelligentLayerSelector:
    """ì§€ëŠ¥ì  ë ˆì´ì–´ ì„ íƒ ë° ê·¸ë£¹í™”"""
    
    @staticmethod
    def analyze_layer_redundancy(model, sample_inputs):
        """ë ˆì´ì–´ ê°„ ì¤‘ë³µì„± ë¶„ì„"""
        redundancy_scores = {}
        
        with torch.no_grad():
            # ê° ë ˆì´ì–´ì˜ ì¶œë ¥ ìˆ˜ì§‘
            layer_outputs = []
            
            def hook_fn(idx):
                def hook(module, input, output):
                    layer_outputs.append((idx, output[0].detach()))
                return hook
            
            # í›… ë“±ë¡
            hooks = []
            for i, layer in enumerate(model.transformer.h):
                hook = layer.register_forward_hook(hook_fn(i))
                hooks.append(hook)
            
            # ìˆœì „íŒŒ
            _ = model(sample_inputs)
            
            # í›… ì œê±°
            for hook in hooks:
                hook.remove()
            
            # ë ˆì´ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
            for i in range(len(layer_outputs) - 1):
                idx1, output1 = layer_outputs[i]
                idx2, output2 = layer_outputs[i + 1]
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                similarity = F.cosine_similarity(
                    output1.flatten(1),
                    output2.flatten(1),
                    dim=1
                ).mean().item()
                
                redundancy_scores[(idx1, idx2)] = similarity
        
        return redundancy_scores
    
    @staticmethod
    def select_fusion_groups(redundancy_scores, importance_scores, target_compression=0.5):
        """ìµœì ì˜ ìœµí•© ê·¸ë£¹ ì„ íƒ"""
        
        # ë†’ì€ ì¤‘ë³µì„± + ë‚®ì€ ì¤‘ìš”ë„ ë ˆì´ì–´ë“¤ì„ ìš°ì„  ìœµí•©
        fusion_candidates = []
        
        for (idx1, idx2), redundancy in redundancy_scores.items():
            avg_importance = (importance_scores.get(idx1, 0) + importance_scores.get(idx2, 0)) / 2
            
            # ìœµí•© ì ìˆ˜: ë†’ì€ ì¤‘ë³µì„±, ë‚®ì€ ì¤‘ìš”ë„ì¼ìˆ˜ë¡ ë†’ìŒ
            fusion_score = redundancy * (1 - avg_importance)
            fusion_candidates.append(((idx1, idx2), fusion_score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        fusion_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # ê·¸ë£¹ ìƒì„±
        fusion_groups = []
        used_layers = set()
        
        for (idx1, idx2), score in fusion_candidates:
            if idx1 not in used_layers and idx2 not in used_layers:
                # ì¸ì ‘í•œ ë ˆì´ì–´ë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ í™•ì¥
                group = [idx1, idx2]
                used_layers.update(group)
                
                # ì—°ì†ëœ ë ˆì´ì–´ ì¶”ê°€
                while True:
                    next_idx = group[-1] + 1
                    if next_idx < 12 and next_idx not in used_layers:
                        # ë‹¤ìŒ ë ˆì´ì–´ì™€ì˜ ì¤‘ë³µì„± í™•ì¸
                        if (group[-1], next_idx) in redundancy_scores:
                            if redundancy_scores[(group[-1], next_idx)] > 0.8:
                                group.append(next_idx)
                                used_layers.add(next_idx)
                            else:
                                break
                        else:
                            break
                    else:
                        break
                
                fusion_groups.append(group)
        
        return fusion_groups


class MultiStageCompressionLayer(nn.Module):
    """ë‹¤ë‹¨ê³„ ì••ì¶• ë ˆì´ì–´ - ê·¹í•œ ì••ì¶• + ì •í™•ë„ ë³´ì¡´"""
    
    def __init__(self, mlp_layers, layer_indices, stage_configs):
        super().__init__()
        
        self.layer_indices = layer_indices
        self.num_stages = len(stage_configs)
        
        print(f"\nğŸš€ Multi-Stage Extreme Compression Layer")
        print(f"   ìœµí•© ë ˆì´ì–´: {layer_indices} ({len(layer_indices)}ê°œ)")
        print(f"   ì••ì¶• ë‹¨ê³„: {self.num_stages}ë‹¨ê³„")
        
        # Stage 1: ë ˆì´ì–´ ìœµí•© (FFT + ìœ„ìƒ ë³´ì •)
        fused_weights = self._stage1_layer_fusion(mlp_layers)
        
        # Stage 2: ì°¨ì› ì¶•ì†Œ (Adaptive SVD)
        compressed_weights = self._stage2_dimension_reduction(fused_weights, stage_configs[1])
        
        # Stage 3: ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ (ì„ íƒì )
        if self.num_stages >= 3:
            compressed_weights = self._stage3_quantization_aware(compressed_weights, stage_configs[2])
        
        # ìµœì¢… ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ì €ì¥
        self.c_fc_U, self.c_fc_S, self.c_fc_V = compressed_weights['c_fc']
        self.c_proj_U, self.c_proj_S, self.c_proj_V = compressed_weights['c_proj']
        
        # ë°”ì´ì–´ìŠ¤ (í‰ê·  + í•™ìŠµê°€ëŠ¥í•œ ë³´ì •)
        if mlp_layers[0].c_fc.bias is not None:
            bias_stack = torch.stack([mlp.c_fc.bias.data for mlp in mlp_layers])
            self.c_fc_bias = nn.Parameter(torch.mean(bias_stack, dim=0))
            self.c_fc_bias_correction = nn.Parameter(torch.zeros_like(self.c_fc_bias) * 0.01)
        else:
            self.register_parameter('c_fc_bias', None)
            self.register_parameter('c_fc_bias_correction', None)
            
        if mlp_layers[0].c_proj.bias is not None:
            bias_stack = torch.stack([mlp.c_proj.bias.data for mlp in mlp_layers])
            self.c_proj_bias = nn.Parameter(torch.mean(bias_stack, dim=0))
            self.c_proj_bias_correction = nn.Parameter(torch.zeros_like(self.c_proj_bias) * 0.01)
        else:
            self.register_parameter('c_proj_bias', None)
            self.register_parameter('c_proj_bias_correction', None)
        
        self.activation = nn.GELU()
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ ìŠ¤ì¼€ì¼
        self.residual_scale = nn.Parameter(torch.tensor(0.1))
        
        # ì••ì¶• í†µê³„
        self._calculate_compression_stats(mlp_layers)
    
    def _stage1_layer_fusion(self, mlp_layers):
        """Stage 1: ê³ ê¸‰ ë ˆì´ì–´ ìœµí•©"""
        print("\n   ğŸ“Š Stage 1: ë ˆì´ì–´ ìœµí•©")
        
        fused_weights = {}
        
        for weight_name in ['c_fc', 'c_proj']:
            weights = []
            for mlp in mlp_layers:
                if weight_name == 'c_fc':
                    weights.append(mlp.c_fc.weight.data.clone())
                else:
                    weights.append(mlp.c_proj.weight.data.clone())
            
            # FFT ë³€í™˜
            fft_weights = []
            for w in weights:
                fft_w = torch.fft.fft2(w.float())
                fft_weights.append(fft_w)
            
            # ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ìœ¼ë¡œ ì¤‘ìš” ì£¼íŒŒìˆ˜ ì‹ë³„
            magnitude_stack = torch.stack([torch.abs(f) for f in fft_weights])
            avg_magnitude = torch.mean(magnitude_stack, dim=0)
            
            # ë™ì  ì„ê³„ê°’ (ìƒìœ„ 80% ì—ë„ˆì§€ ë³´ì¡´)
            magnitude_flat = avg_magnitude.flatten()
            sorted_mags, _ = torch.sort(magnitude_flat, descending=True)
            cumsum = torch.cumsum(sorted_mags, dim=0)
            total_energy = cumsum[-1]
            threshold_idx = torch.where(cumsum >= 0.8 * total_energy)[0][0]
            threshold = sorted_mags[threshold_idx]
            
            # ì£¼íŒŒìˆ˜ ë§ˆìŠ¤í¬
            freq_mask = avg_magnitude >= threshold
            
            # ê°€ì¤‘ ìœµí•© (ê¹Šì€ ë ˆì´ì–´ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
            depth_weights = torch.softmax(torch.arange(len(weights), dtype=torch.float32), dim=0)
            
            fused_fft = torch.zeros_like(fft_weights[0])
            phase_consensus = torch.zeros_like(fft_weights[0])
            
            for i, (fft_w, depth_w) in enumerate(zip(fft_weights, depth_weights)):
                fused_fft += fft_w * freq_mask * depth_w
                phase_consensus += torch.angle(fft_w) * depth_w
            
            # ìœ„ìƒ ë³´ì •
            magnitude = torch.abs(fused_fft)
            fused_fft = magnitude * torch.exp(1j * phase_consensus)
            
            # IFFT
            fused_weight = torch.fft.ifft2(fused_fft).real
            
            print(f"      {weight_name}: {len(weights)}ê°œ ë ˆì´ì–´ ìœµí•©")
            print(f"      ì£¼íŒŒìˆ˜ ë³´ì¡´ìœ¨: {freq_mask.sum().item() / freq_mask.numel():.1%}")
            
            fused_weights[weight_name] = fused_weight
        
        return fused_weights
    
    def _stage2_dimension_reduction(self, fused_weights, config):
        """Stage 2: ì ì‘ì  ì°¨ì› ì¶•ì†Œ"""
        print("\n   ğŸ“Š Stage 2: ì°¨ì› ì¶•ì†Œ")
        
        compressed = {}
        
        for name, weight in fused_weights.items():
            # SVD ë¶„í•´
            U, S, V = torch.svd(weight)
            
            # ì—ë„ˆì§€ ê¸°ë°˜ rank ê²°ì •
            energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
            
            # ëª©í‘œ ì—ë„ˆì§€ ë³´ì¡´ìœ¨
            target_energy = config.get('energy_threshold', 0.95)
            rank = torch.sum(energy < target_energy).item() + 1
            
            # ë¯¸ë¶„ ê¸°ë°˜ ìµœì  rank ì°¾ê¸°
            if rank > 10:
                energy_diff = energy[1:] - energy[:-1]
                # ì—ë„ˆì§€ ì¦ê°€ìœ¨ì´ ê¸‰ê²©íˆ ê°ì†Œí•˜ëŠ” ì§€ì 
                second_diff = energy_diff[1:] - energy_diff[:-1]
                elbow_points = torch.where(second_diff > second_diff.mean() + 2 * second_diff.std())[0]
                
                if len(elbow_points) > 0:
                    optimal_rank = elbow_points[0].item() + 2
                    rank = min(rank, optimal_rank)
            
            # ìµœì†Œ/ìµœëŒ€ ì œì•½
            min_rank = max(int(min(weight.shape) * 0.02), 16)  # ìµœì†Œ 2% ë˜ëŠ” 16
            max_rank = int(min(weight.shape) * 0.5)  # ìµœëŒ€ 50%
            rank = max(min_rank, min(rank, max_rank))
            
            print(f"      {name}: {min(weight.shape)} â†’ {rank} ({rank/min(weight.shape):.1%})")
            print(f"      ì—ë„ˆì§€ ë³´ì¡´: {energy[rank-1]:.3f}")
            
            compressed[name] = (
                nn.Parameter(U[:, :rank].to(weight.dtype)),
                nn.Parameter(S[:rank].to(weight.dtype)),
                nn.Parameter(V[:, :rank].to(weight.dtype))
            )
        
        return compressed
    
    def _stage3_quantization_aware(self, compressed_weights, config):
        """Stage 3: ì–‘ìí™” ì¸ì‹ ì••ì¶•"""
        print("\n   ğŸ“Š Stage 3: ì–‘ìí™” ì¤€ë¹„")
        
        # íŠ¹ì´ê°’ì— ëŒ€í•œ ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜
        for name in compressed_weights:
            U, S, V = compressed_weights[name]
            
            # íŠ¹ì´ê°’ ì–‘ìí™” (8ë¹„íŠ¸ ì‹œë®¬ë ˆì´ì…˜)
            S_min, S_max = S.min(), S.max()
            S_quantized = torch.round((S - S_min) / (S_max - S_min) * 255) / 255 * (S_max - S_min) + S_min
            
            compressed_weights[name] = (U, S_quantized, V)
            
            print(f"      {name}: íŠ¹ì´ê°’ ì–‘ìí™” ì™„ë£Œ")
        
        return compressed_weights
    
    def _calculate_compression_stats(self, mlp_layers):
        """ì••ì¶• í†µê³„ ê³„ì‚°"""
        # ì›ë³¸ íŒŒë¼ë¯¸í„°
        original_params = 0
        for mlp in mlp_layers:
            original_params += mlp.c_fc.weight.numel()
            original_params += mlp.c_proj.weight.numel()
            if mlp.c_fc.bias is not None:
                original_params += mlp.c_fc.bias.numel()
            if mlp.c_proj.bias is not None:
                original_params += mlp.c_proj.bias.numel()
        
        # ì••ì¶•ëœ íŒŒë¼ë¯¸í„°
        compressed_params = 0
        compressed_params += self.c_fc_U.numel() + self.c_fc_S.numel() + self.c_fc_V.numel()
        compressed_params += self.c_proj_U.numel() + self.c_proj_S.numel() + self.c_proj_V.numel()
        if self.c_fc_bias is not None:
            compressed_params += self.c_fc_bias.numel() + self.c_fc_bias_correction.numel()
        if self.c_proj_bias is not None:
            compressed_params += self.c_proj_bias.numel() + self.c_proj_bias_correction.numel()
        compressed_params += 1  # residual_scale
        
        self.compression_ratio = compressed_params / original_params
        self.params_saved = original_params - compressed_params
        
        print(f"\n   ğŸ’¾ ì••ì¶• ê²°ê³¼:")
        print(f"      ì›ë³¸: {original_params:,} íŒŒë¼ë¯¸í„°")
        print(f"      ì••ì¶•: {compressed_params:,} íŒŒë¼ë¯¸í„°")
        print(f"      ì ˆì•½: {self.params_saved:,} ({(1-self.compression_ratio)*100:.1f}%)")
    
    def forward(self, x):
        """ìˆœì „íŒŒ with ì”ì°¨ ì—°ê²°"""
        # ì…ë ¥ ì €ì¥ (ì”ì°¨ìš©)
        residual = x
        
        # c_fc ì ìš©
        c_fc_weight = torch.mm(self.c_fc_U * self.c_fc_S.unsqueeze(0), self.c_fc_V.T)
        bias = self.c_fc_bias + self.c_fc_bias_correction if self.c_fc_bias is not None else None
        h = F.linear(x, c_fc_weight.T, bias)
        h = self.activation(h)
        
        # c_proj ì ìš©
        c_proj_weight = torch.mm(self.c_proj_U * self.c_proj_S.unsqueeze(0), self.c_proj_V.T)
        bias = self.c_proj_bias + self.c_proj_bias_correction if self.c_proj_bias is not None else None
        output = F.linear(h, c_proj_weight.T, bias)
        
        # ìŠ¤ì¼€ì¼ëœ ì”ì°¨ ì—°ê²°
        output = output + self.residual_scale * residual
        
        return output


def apply_extreme_compression(model, tokenizer):
    """ê·¹í•œ ì••ì¶• ì ìš©"""
    
    print(f"\nğŸš€ ê·¹í•œ ì••ì¶• + ì •í™•ë„ ë³´ì¡´ ì „ëµ ì‹œì‘")
    
    # 1. ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¶„ì„
    sample_texts = [
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ì˜ í•µì‹¬ ê¸°ìˆ ì´ë‹¤.",
        "í•œêµ­ì˜ ì „í†µ ìŒì‹ì¸ ê¹€ì¹˜ëŠ” ë°œíš¨ ì‹í’ˆì´ë‹¤.",
        "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë©° ìµœëŒ€ ë„ì‹œì´ë‹¤.",
        "ê¸°ê³„í•™ìŠµì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•œë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê²Œ í•œë‹¤."
    ]
    
    inputs = tokenizer(sample_texts, return_tensors="pt", padding=True, truncation=True, max_length=32)
    
    # 2. ë ˆì´ì–´ ë¶„ì„
    print("\nğŸ“Š ë ˆì´ì–´ ë¶„ì„ ì¤‘...")
    
    # ì¤‘ìš”ë„ ë¶„ì„ - ì§ì ‘ êµ¬í˜„
    class SimpleImportanceAnalyzer:
        @staticmethod
        def analyze_layer_importance(model, sample_inputs, layer_indices):
            importance_scores = {}
            
            with torch.no_grad():
                original_output = model(sample_inputs)
                
                for idx in layer_indices:
                    temp_model = copy.deepcopy(model)
                    
                    # ë ˆì´ì–´ë¥¼ identityë¡œ ëŒ€ì²´
                    class IdentityMLP(nn.Module):
                        def forward(self, x):
                            return x * 0.1
                    
                    temp_model.transformer.h[idx].mlp = IdentityMLP()
                    
                    modified_output = temp_model(sample_inputs)
                    
                    # KL divergence
                    kl_div = F.kl_div(
                        F.log_softmax(modified_output.logits, dim=-1),
                        F.softmax(original_output.logits, dim=-1),
                        reduction='batchmean'
                    ).item()
                    
                    importance_scores[idx] = kl_div
                    
                    del temp_model
            
            return importance_scores
    
    all_layers = list(range(len(model.transformer.h)))
    importance_scores = SimpleImportanceAnalyzer.analyze_layer_importance(
        model, inputs.input_ids, all_layers
    )
    
    # ì¤‘ë³µì„± ë¶„ì„
    redundancy_scores = IntelligentLayerSelector.analyze_layer_redundancy(
        model, inputs.input_ids
    )
    
    # 3. ìµœì  ìœµí•© ê·¸ë£¹ ì„ íƒ
    fusion_groups = IntelligentLayerSelector.select_fusion_groups(
        redundancy_scores, importance_scores, target_compression=0.7
    )
    
    print("\nğŸ“¦ ì„ íƒëœ ìœµí•© ê·¸ë£¹:")
    for i, group in enumerate(fusion_groups):
        avg_importance = sum(importance_scores.get(idx, 0) for idx in group) / len(group)
        print(f"   ê·¸ë£¹ {i+1}: ë ˆì´ì–´ {group} (í‰ê·  ì¤‘ìš”ë„: {avg_importance:.3f})")
    
    # 4. ê·¸ë£¹ë³„ ì••ì¶• ì ìš©
    total_params_saved = 0
    
    for group in fusion_groups:
        if len(group) >= 2:
            # ë‹¤ë‹¨ê³„ ì••ì¶• ì„¤ì •
            stage_configs = [
                {},  # Stage 1: FFT fusion (ê¸°ë³¸ ì„¤ì •)
                {'energy_threshold': 0.93 if len(group) <= 3 else 0.90},  # Stage 2: SVD
                {}   # Stage 3: Quantization aware
            ]
            
            mlp_layers = [model.transformer.h[i].mlp for i in group]
            
            # ì••ì¶• ë ˆì´ì–´ ìƒì„±
            compressed_layer = MultiStageCompressionLayer(
                mlp_layers, group, stage_configs
            )
            
            total_params_saved += compressed_layer.params_saved
            
            # ëª¨ë¸ì— ì ìš©
            model.transformer.h[group[0]].mlp = compressed_layer
            
            # ë‚˜ë¨¸ì§€ ë ˆì´ì–´ ì œê±°
            for i in reversed(group[1:]):
                del model.transformer.h[i]
    
    return model, total_params_saved


def test_extreme_compression():
    """ê·¹í•œ ì••ì¶• í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ¯ Reality Stone ê·¹í•œ ì••ì¶• + ì •í™•ë„ ë³´ì¡´ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ì›ë³¸ ëª¨ë¸ í†µê³„
    original_params = sum(p.numel() for p in model.parameters())
    original_layers = len(model.transformer.h)
    original_size_mb = original_params * 4 / (1024**2)  # float32 ê¸°ì¤€
    
    print(f"\nğŸ“Š ì›ë³¸ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {original_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_size_mb:.1f}MB")
    
    # ì›ë³¸ ì •í™•ë„ ì¸¡ì •
    print(f"\nğŸ“‹ ì›ë³¸ ëª¨ë¸ ì •í™•ë„ í…ŒìŠ¤íŠ¸")
    
    test_cases = [
        ("í•œêµ­ì˜ ìˆ˜ë„ëŠ”", ["ì„œìš¸", "Seoul"]),
        ("ì¸ê³µì§€ëŠ¥ì€", ["AI", "ê¸°ìˆ ", "ì»´í“¨í„°", "ë¯¸ë˜"]),
        ("ê¹€ì¹˜ëŠ”", ["ìŒì‹", "í•œêµ­", "ë°œíš¨", "ë°°ì¶”"]),
        ("ê¸°ê³„í•™ìŠµ", ["ë¨¸ì‹ ëŸ¬ë‹", "ë°ì´í„°", "í•™ìŠµ", "AI", "ì¸ê³µì§€ëŠ¥"]),
        ("ì„œìš¸ì€", ["í•œêµ­", "ìˆ˜ë„", "ë„ì‹œ", "ëŒ€í•œë¯¼êµ­"]),
        ("íŒŒì´ì¬ì€", ["í”„ë¡œê·¸ë˜ë°", "ì–¸ì–´", "Python", "ì½”ë”©"]),
        ("ìì—°ì–´ì²˜ë¦¬", ["NLP", "ì–¸ì–´", "í…ìŠ¤íŠ¸", "AI"])
    ]
    
    def evaluate_accuracy(model, test_cases):
        correct = 0
        for prompt, expected_keywords in test_cases:
            try:
                inputs = tokenizer(prompt, return_tensors="pt")
                with torch.no_grad():
                    outputs = model.generate(
                        inputs.input_ids,
                        max_length=30,
                        temperature=0.8,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # í‚¤ì›Œë“œ ë§¤ì¹­
                matched = any(keyword in generated for keyword in expected_keywords)
                if matched:
                    correct += 1
                
                print(f"   '{prompt}' â†’ '{generated[:50]}...' ({'âœ…' if matched else 'âŒ'})")
                
            except Exception as e:
                print(f"   '{prompt}' â†’ ì˜¤ë¥˜: {e} (âŒ)")
        
        accuracy = correct / len(test_cases)
        print(f"   ì •í™•ë„: {accuracy:.1%} ({correct}/{len(test_cases)})")
        
        return accuracy
    
    original_accuracy = evaluate_accuracy(model, test_cases)
    
    # ê·¹í•œ ì••ì¶• ì ìš©
    print(f"\nğŸš€ ê·¹í•œ ì••ì¶• ì ìš© ì¤‘...")
    compressed_model = copy.deepcopy(model)
    compressed_model, params_saved = apply_extreme_compression(compressed_model, tokenizer)
    
    # ì••ì¶• í›„ í†µê³„
    compressed_params = sum(p.numel() for p in compressed_model.parameters())
    compressed_layers = len(compressed_model.transformer.h)
    compressed_size_mb = compressed_params * 4 / (1024**2)
    
    compression_ratio = 1 - (compressed_params / original_params)
    size_reduction = original_size_mb - compressed_size_mb
    
    print(f"\nğŸ“Š ì••ì¶• í›„ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {original_layers} â†’ {compressed_layers} ({original_layers - compressed_layers}ê°œ ì œê±°)")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {compressed_params:,}")
    print(f"   í¬ê¸°: {original_size_mb:.1f}MB â†’ {compressed_size_mb:.1f}MB")
    
    # ì••ì¶• í›„ ì •í™•ë„ ì¸¡ì •
    print(f"\nğŸ“‹ ì••ì¶• ëª¨ë¸ ì •í™•ë„ í…ŒìŠ¤íŠ¸")
    compressed_accuracy = evaluate_accuracy(compressed_model, test_cases)
    
    accuracy_retention = compressed_accuracy / original_accuracy if original_accuracy > 0 else 0
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ† ê·¹í•œ ì••ì¶• ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    print(f"ğŸ“Š ì••ì¶• ì„±ê³¼:")
    print(f"   ì••ì¶•ë¥ : {compression_ratio:.1%} (ì›ë³¸ ëŒ€ë¹„ {compression_ratio*100:.1f}% ì••ì¶•)")
    print(f"   íŒŒë¼ë¯¸í„° ì ˆì•½: {original_params - compressed_params:,}ê°œ")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {size_reduction:.1f}MB")
    print(f"   ë ˆì´ì–´ ì ˆì•½: {original_layers - compressed_layers}ê°œ")
    
    print(f"\nğŸ“ˆ ì •í™•ë„ ë³´ì¡´:")
    print(f"   ì›ë³¸ ì •í™•ë„: {original_accuracy:.1%}")
    print(f"   ì••ì¶• ì •í™•ë„: {compressed_accuracy:.1%}")
    print(f"   ì •í™•ë„ ë³´ì¡´ìœ¨: {accuracy_retention:.1%}")
    
    print(f"\nğŸ’¡ í˜ì‹ ì  ì„±ê³¼:")
    if compression_ratio >= 0.5 and accuracy_retention >= 0.8:
        print(f"   ğŸ‰ ëª©í‘œ ë‹¬ì„±! 50%+ ì••ì¶• + 80%+ ì •í™•ë„ ë³´ì¡´")
        print(f"   âœ… ë‹¤ë‹¨ê³„ ì••ì¶• ì „ëµ ì„±ê³µ")
        print(f"   âœ… ì§€ëŠ¥ì  ë ˆì´ì–´ ì„ íƒ íš¨ê³¼ì ")
        print(f"   âœ… ì”ì°¨ ì—°ê²°ë¡œ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”")
    elif compression_ratio >= 0.4 and accuracy_retention >= 0.9:
        print(f"   ğŸ¯ ìš°ìˆ˜í•œ ì„±ê³¼! 40%+ ì••ì¶• + 90%+ ì •í™•ë„ ë³´ì¡´")
        print(f"   âœ… ì•ˆì •ì ì¸ ì••ì¶• ë‹¬ì„±")
    else:
        print(f"   ğŸ’ª ì••ì¶• ì„±ê³µ, ì¶”ê°€ ìµœì í™” ê°€ëŠ¥")
    
    print(f"\nâœ… ê·¹í•œ ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_extreme_compression() 