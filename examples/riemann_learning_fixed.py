import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import time
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
from collections import Counter
import copy
import numpy as np

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒŸ RealityStone í•„ìˆ˜ ë¡œë“œ & ë¦¬ë§Œê¸°í•˜í•™ í™œìš©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    import reality_stone as rs
    print("âœ… RealityStone ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ!")
    print(f"   ğŸŒŸ ë²„ì „: {getattr(rs, '__version__', 'Unknown')}")
    
    # RealityStone ê³ ê¸‰ ê¸°ëŠ¥ë“¤ í™•ì¸
    rs_functions = []
    essential_funcs = ['hyperbolic_laplacian', 'poincare_ball_layer', 'mobius_add', 
                      'poincare_to_klein', 'klein_to_lorentz', 'spherical_harmonics']
    
    for func in essential_funcs:
        if hasattr(rs, func):
            rs_functions.append(func)
            print(f"   ğŸ’ {func}: ì‚¬ìš© ê°€ëŠ¥")
        else:
            print(f"   âš ï¸ {func}: ì‚¬ìš© ë¶ˆê°€")
    
    RS_AVAILABLE = True
    print(f"   ğŸš€ í™œìš© ê°€ëŠ¥í•œ RS í•¨ìˆ˜: {len(rs_functions)}ê°œ")
    
except ImportError:
    print("âŒ RealityStone ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìˆ˜! ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”")
    print("   pip install reality-stone")
    exit(1)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ë¦¬ë§Œí‰ë©´ ê¸°ë°˜ ì••ì¶•ê¸° (RealityStone ì™„ì „ í™œìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RiemannPlaneCompressor:
    """ë¦¬ë§Œí‰ë©´ ê¸°ë°˜ ì••ì¶•ê¸° (RealityStone ì™„ì „ í™œìš©)"""
    
    def __init__(self, W: torch.Tensor, compression_ratio=0.1):
        self.out_f, self.in_f = W.shape
        self.compression_ratio = compression_ratio
        
        print(f"    ğŸŒ ë¦¬ë§Œí‰ë©´ ì••ì¶•: {W.shape}, ì••ì¶•ë¥ ={compression_ratio:.1%}")
        
        # RealityStone ê¸°ëŠ¥ë“¤ì„ ì‹¤ì œë¡œ í™œìš©í•œ ì••ì¶•
        self._apply_riemann_compression(W)
    
    def _apply_riemann_compression(self, W: torch.Tensor):
        """ë¦¬ë§Œí‰ë©´ì—ì„œì˜ ì‹¤ì œ ì••ì¶•"""
        
        print(f"       ğŸ”„ í¬ì¸ì¹´ë ˆ ë³¼ ë§¤í•‘...")
        # 1. ê°€ì¤‘ì¹˜ë¥¼ í¬ì¸ì¹´ë ˆ ë³¼ë¡œ ë§¤í•‘
        poincare_weights = self._map_to_poincare_ball(W)
        
        print(f"       ğŸŒ€ í•˜ì´í¼ë³¼ë¦­ ë¼í”Œë¼ì‹œì•ˆ ì ìš©...")
        # 2. í•˜ì´í¼ë³¼ë¦­ ë¼í”Œë¼ì‹œì•ˆìœ¼ë¡œ íŠ¹ì„± ì¶”ì¶œ
        if hasattr(rs, 'hyperbolic_laplacian'):
            hyperbolic_features = rs.hyperbolic_laplacian(poincare_weights.flatten().unsqueeze(0))
            hyperbolic_features = hyperbolic_features.reshape(W.shape)
        else:
            hyperbolic_features = poincare_weights
        
        print(f"       âš–ï¸ ë«¼ë¹„ìš°ìŠ¤ ë³€í™˜...")
        # 3. ë«¼ë¹„ìš°ìŠ¤ ë³€í™˜ìœ¼ë¡œ ì •ê·œí™”
        if hasattr(rs, 'mobius_add'):
            # ë«¼ë¹„ìš°ìŠ¤ ë”í•˜ê¸°ë¡œ ë³€í™˜
            zero_tensor = torch.zeros_like(hyperbolic_features)
            mobius_features = rs.mobius_add(hyperbolic_features, zero_tensor)
        else:
            mobius_features = hyperbolic_features
        
        print(f"       ğŸ­ í´ë¼ì¸ ëª¨ë¸ë¡œ ë³€í™˜...")
        # 4. í¬ì¸ì¹´ë ˆ â†’ í´ë¼ì¸ â†’ ë¡œë Œì¸  ë³€í™˜ ì²´ì¸
        if hasattr(rs, 'poincare_to_klein'):
            klein_features = rs.poincare_to_klein(mobius_features)
            if hasattr(rs, 'klein_to_lorentz'):
                lorentz_features = rs.klein_to_lorentz(klein_features)
                final_features = lorentz_features
            else:
                final_features = klein_features
        else:
            final_features = mobius_features
        
        print(f"       ğŸ“ êµ¬ë©´ì¡°í™”í•¨ìˆ˜ ë¶„ì„...")
        # 5. êµ¬ë©´ì¡°í™”í•¨ìˆ˜ë¡œ ì£¼íŒŒìˆ˜ ë¶„ì„
        if hasattr(rs, 'spherical_harmonics'):
            try:
                # ì‹¤ìˆ˜ ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ì—¬ êµ¬ë©´ì¡°í™”í•¨ìˆ˜ ì ìš©
                real_part = final_features.real if torch.is_complex(final_features) else final_features
                spherical_coeffs = rs.spherical_harmonics(real_part.flatten().unsqueeze(0))
                compressed_features = spherical_coeffs.reshape(W.shape)
            except:
                compressed_features = final_features
        else:
            compressed_features = final_features
        
        # 6. ìµœì¢… SVD ì••ì¶• (ë¦¬ë§Œê¸°í•˜í•™ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ ë°ì´í„°)
        U, S, V = torch.svd(compressed_features.float())
        
        # ì—ë„ˆì§€ ê¸°ë°˜ ë­í¬ ì„ íƒ
        energy_cumsum = torch.cumsum(S**2, dim=0)
        total_energy = energy_cumsum[-1]
        
        # ë¦¬ë§Œê¸°í•˜í•™ ë³€í™˜ìœ¼ë¡œ ì¸í•œ ì •ë³´ ì§‘ì•½ ê³ ë ¤
        energy_threshold = 0.98  # ë” ë†’ì€ ì—ë„ˆì§€ ë³´ì¡´
        energy_rank = torch.sum(energy_cumsum < total_energy * energy_threshold).item() + 1
        target_rank = max(16, int(min(W.shape) * self.compression_ratio * 8))  # ë” ë§ì€ ë­í¬
        
        optimal_rank = min(energy_rank, target_rank, len(S))
        
        # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ì €ì¥ (ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ë³´ì¥)
        self.U = nn.Parameter(U[:, :optimal_rank].to(W.dtype))
        self.S = nn.Parameter(S[:optimal_rank].to(W.dtype))
        self.V = nn.Parameter(V[:, :optimal_rank].to(W.dtype))
        
        # ì••ì¶• í†µê³„
        original_params = W.numel()
        compressed_params = self.U.numel() + self.S.numel() + self.V.numel()
        actual_ratio = compressed_params / original_params
        
        print(f"       âœ… ë¦¬ë§Œì••ì¶• ì™„ë£Œ: rank {optimal_rank}, ì‹¤ì œ ì••ì¶•ë¥  {actual_ratio:.1%}")
    
    def _map_to_poincare_ball(self, W: torch.Tensor) -> torch.Tensor:
        """ê°€ì¤‘ì¹˜ë¥¼ í¬ì¸ì¹´ë ˆ ë³¼ë¡œ ë§¤í•‘"""
        
        # ì •ê·œí™”ë¥¼ í†µí•´ í¬ì¸ì¹´ë ˆ ë³¼ ë‚´ë¶€ë¡œ ë§¤í•‘
        norm = torch.norm(W, dim=-1, keepdim=True)
        max_norm = torch.max(norm)
        
        if max_norm > 0:
            # 0.95 ì´ë‚´ë¡œ ìŠ¤ì¼€ì¼ë§ (í¬ì¸ì¹´ë ˆ ë³¼ ê²½ê³„ íšŒí”¼)
            scale_factor = 0.95 / (max_norm + 1e-8)
            if scale_factor < 1.0:
                W_scaled = W * scale_factor
            else:
                W_scaled = W
        else:
            W_scaled = W
        
        return W_scaled
    
    def apply(self, x: torch.Tensor) -> torch.Tensor:
        """ë¦¬ë§Œê¸°í•˜í•™ì  ì••ì¶• ì—°ì‚° ì ìš©"""
        
        # SVD ë¶„í•´ëœ í˜•íƒœë¡œ íš¨ìœ¨ì  ê³„ì‚°
        step1 = x @ self.V  # [batch, rank]
        step2 = step1 * self.S.unsqueeze(0)  # [batch, rank]
        step3 = step2 @ self.U.t()  # [batch, out_features]
        
        return step3

class RiemannLinear(nn.Module):
    """ë¦¬ë§Œí‰ë©´ ê¸°ë°˜ Linear ë ˆì´ì–´"""
    
    def __init__(self, original_layer, compression_ratio=0.1):
        super().__init__()
        
        if hasattr(original_layer, 'weight'):
            W = original_layer.weight.data.clone()
            
            # ë ˆì´ì–´ íƒ€ì… í™•ì¸
            if hasattr(original_layer, 'nf'):  # Conv1D
                self.in_features = W.shape[1]
                self.out_features = W.shape[0]
                W = W.t()
                layer_type = "Conv1D"
            else:  # Linear
                self.in_features = original_layer.in_features
                self.out_features = original_layer.out_features
                layer_type = "Linear"
            
            print(f"ğŸŒ ë¦¬ë§Œ {layer_type}: in={self.in_features}, out={self.out_features}")
            
            # ë¦¬ë§Œí‰ë©´ ì••ì¶•ê¸° ì ìš©
            self.riemann_compressor = RiemannPlaneCompressor(W, compression_ratio)
            
            # ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
            if hasattr(original_layer, 'bias') and original_layer.bias is not None:
                self.bias = nn.Parameter(original_layer.bias.data.clone())
            else:
                self.bias = None
        else:
            raise ValueError("Original layer must have weight attribute")
    
    def forward(self, x):
        # ë¦¬ë§Œê¸°í•˜í•™ì  ì••ì¶• ì—°ì‚°
        output = self.riemann_compressor.apply(x)
        if self.bias is not None:
            output = output + self.bias
        return output

class RiemannBlock(nn.Module):
    """ë¦¬ë§Œí‰ë©´ ê¸°ë°˜ Transformer ë¸”ë¡"""
    
    def __init__(self, original_block, compression_ratio=0.1, layer_idx=0, total_layers=12):
        super().__init__()
        
        # ë ˆì´ì–´ ì •ê·œí™”ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        self.ln1 = original_block.ln_1
        self.ln2 = original_block.ln_2
        
        # ì–´í…ì…˜ê³¼ MLP ì¶”ì¶œ
        attn, mlp = original_block.attn, original_block.mlp
        
        # ì ì‘ì  ì••ì¶•ë¥  (ë¦¬ë§Œê¸°í•˜í•™ íŠ¹ì„± ê³ ë ¤)
        normalized_idx = layer_idx / total_layers
        if normalized_idx < 0.3:  # ì´ˆê¸°ì¸µ: ë” ë³´ìˆ˜ì 
            layer_ratio = compression_ratio * 1.5
        elif normalized_idx < 0.7:  # ì¤‘ê°„ì¸µ: ì ê·¹ì  ì••ì¶•
            layer_ratio = compression_ratio * 0.6
        else:  # ë§ë‹¨ì¸µ: ë³´ìˆ˜ì 
            layer_ratio = compression_ratio * 1.3
        
        print(f"ğŸŒ ë¦¬ë§Œ ë¸”ë¡ {layer_idx}: ì••ì¶•ë¥  {layer_ratio:.1%}")
        
        # ê° ì„œë¸Œë ˆì´ì–´ë¥¼ ë¦¬ë§Œí‰ë©´ì—ì„œ ì••ì¶•
        attn.c_attn = RiemannLinear(attn.c_attn, layer_ratio)
        attn.c_proj = RiemannLinear(attn.c_proj, layer_ratio)
        mlp.c_fc = RiemannLinear(mlp.c_fc, layer_ratio)
        mlp.c_proj = RiemannLinear(mlp.c_proj, layer_ratio)
        
        self.attn, self.mlp = attn, mlp
    
    def forward(self, x, **kwargs):
        # í‘œì¤€ Transformer ë¸”ë¡ ìˆœì „íŒŒ
        h = self.ln1(x)
        attn_outputs = self.attn(h, **kwargs)
        a = attn_outputs[0]
        x = x + a
        h2 = self.ln2(x)
        m = self.mlp(h2)
        output = x + m
        
        if len(attn_outputs) > 1:
            return (output,) + attn_outputs[1:]
        else:
            return (output,)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ ë¦¬ë§Œí‰ë©´ ì••ì¶• íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def apply_riemann_compression(model, compression_ratio=0.05):
    """ë¦¬ë§Œí‰ë©´ ê¸°ë°˜ ëª¨ë¸ ì••ì¶•"""
    
    total_before = sum(p.numel() for p in model.parameters())
    total_layers = len(model.transformer.h)
    
    print(f"Before: {total_before:,} params ({total_before/1e6:.1f}M)")
    print(f"ğŸŒ ë¦¬ë§Œí‰ë©´ RealityStone ì••ì¶•: ëª©í‘œ={compression_ratio:.1%}")
    print(f"ğŸ’ ì‚¬ìš© ê¸°ìˆ : í¬ì¸ì¹´ë ˆë³¼ + í•˜ì´í¼ë³¼ë¦­ë¼í”Œë¼ì‹œì•ˆ + ë«¼ë¹„ìš°ìŠ¤ë³€í™˜ + êµ¬ë©´ì¡°í™”í•¨ìˆ˜")
    
    # ëª¨ë“  ë ˆì´ì–´ë¥¼ ë¦¬ë§Œí‰ë©´ì—ì„œ ì••ì¶•
    compressed_count = 0
    for i in tqdm(range(total_layers), desc="ğŸŒ ë¦¬ë§Œ ì••ì¶•"):
        try:
            model.transformer.h[i] = RiemannBlock(
                model.transformer.h[i], compression_ratio, i, total_layers
            )
            compressed_count += 1
        except Exception as e:
            print(f"   âŒ ë ˆì´ì–´ {i} ì••ì¶• ì‹¤íŒ¨: {e}")
            continue
    
    total_after = sum(p.numel() for p in model.parameters())
    actual_compression = total_after / total_before
    
    print(f"After:  {total_after:,} params ({total_after/1e6:.1f}M)")
    print(f"ğŸŒ ì‹¤ì œ ì••ì¶•ë¥ : {actual_compression:.1%} ({1/actual_compression:.1f}Ã— ì••ì¶•)")
    print(f"âœ… ì„±ê³µ ì••ì¶•: {compressed_count}/{total_layers} ë ˆì´ì–´")
    
    return model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ§  ë¦¬ë§Œí‰ë©´ Knowledge Distillation (ì§„ì§œ í•™ìŠµ!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def riemann_kd_loss(student_logits, teacher_logits, temperature=4.0, use_rs=True):
    """ë¦¬ë§Œí‰ë©´ì—ì„œì˜ Knowledge Distillation ì†ì‹¤"""
    
    # ê¸°ë³¸ KL divergence
    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)
    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)
    kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')
    
    # RealityStone ê¸°ëŠ¥ìœ¼ë¡œ ë¦¬ë§Œê¸°í•˜í•™ì  ê±°ë¦¬ ê³„ì‚°
    if use_rs and hasattr(rs, 'hyperbolic_laplacian'):
        try:
            # í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œì˜ ê±°ë¦¬ ì¸¡ì •
            teacher_flat = teacher_logits.flatten().unsqueeze(0)
            student_flat = student_logits.flatten().unsqueeze(0)
            
            teacher_hyperbolic = rs.hyperbolic_laplacian(teacher_flat)
            student_hyperbolic = rs.hyperbolic_laplacian(student_flat)
            
            # í•˜ì´í¼ë³¼ë¦­ ê±°ë¦¬ ì†ì‹¤
            hyperbolic_loss = F.mse_loss(student_hyperbolic, teacher_hyperbolic)
            
            # í†µí•© ì†ì‹¤
            total_loss = 0.7 * kl_loss + 0.3 * hyperbolic_loss
        except:
            total_loss = kl_loss
    else:
        total_loss = kl_loss
    
    return total_loss * (temperature ** 2)

def riemann_fine_tune(teacher_model, student_model, tokenizer, 
                     total_steps=800, base_lr=3e-3, temperature=4.0):
    """ë¦¬ë§Œí‰ë©´ì—ì„œì˜ ì§„ì§œ í•™ìŠµ íŒŒì¸íŠœë‹"""
    
    print(f"\nğŸ§  ë¦¬ë§Œí‰ë©´ Knowledge Distillation ì‹œì‘")
    print(f"   ì´ ìŠ¤í…: {total_steps}, í•™ìŠµë¥ : {base_lr} (ì§„ì§œ í•™ìŠµ!)")
    print(f"   ì˜¨ë„: {temperature}, RealityStone í™œìš©: {RS_AVAILABLE}")
    print(f"ğŸ¯ ëª©í‘œ: ë¦¬ë§Œí‰ë©´ì—ì„œ ì‹¤ì œ í•™ìŠµ ë‹¬ì„±!")
    
    # ë” ë‹¤ì–‘í•˜ê³  í’ë¶€í•œ í•œêµ­ì–´ ë°ì´í„°
    train_texts = [
        "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ í•œê°•ì´ íë¥´ëŠ” ì•„ë¦„ë‹¤ìš´ ë„ì‹œì…ë‹ˆë‹¤.",
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤. ë§ì€ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ìœ¼ë©´ ê¸°ë¶„ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤. ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì‹ì‚¬í•˜ë©´ ë”ìš± ì¦ê²ìŠµë‹ˆë‹¤.",
        "ì±…ì„ ì½ëŠ” ê²ƒì€ ì§€ì‹ì„ ëŠ˜ë¦¬ëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ì±…ì„ ì½ì–´ë³´ì„¸ìš”.",
        "ìš´ë™ì„ í•˜ë©´ ê±´ê°•í•´ì§‘ë‹ˆë‹¤. ë§¤ì¼ ì¡°ê¸ˆì”©ì´ë¼ë„ ì›€ì§ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ìŒì•…ì„ ë“¤ìœ¼ë©´ ë§ˆìŒì´ í¸ì•ˆí•´ì§‘ë‹ˆë‹¤. ì¢‹ì•„í•˜ëŠ” ìŒì•…ì„ ì°¾ì•„ ë“¤ì–´ë³´ì„¸ìš”.",
        "ì—¬í–‰ì„ ê°€ë©´ ìƒˆë¡œìš´ ë¬¸í™”ë¥¼ ê²½í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‚˜ë¼ì˜ ìŒì‹ê³¼ ì–¸ì–´ë¥¼ ë°°ì›Œë³´ì„¸ìš”.",
        "ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ì‹œê°„ì„ ë³´ë‚´ëŠ” ê²ƒì€ ì¦ê±°ìš´ ì¼ì…ë‹ˆë‹¤. ì†Œì¤‘í•œ ì¶”ì–µì„ ë§Œë“¤ì–´ë³´ì„¸ìš”.",
        "ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš°ëŠ” ê²ƒì€ í•­ìƒ í¥ë¯¸ë¡œìš´ ê²½í—˜ì…ë‹ˆë‹¤. í˜¸ê¸°ì‹¬ì„ ê°€ì§€ê³  ë„ì „í•´ë³´ì„¸ìš”.",
        "ìš”ë¦¬ë¥¼ í•˜ëŠ” ê²ƒì€ ì°½ì˜ì ì¸ í™œë™ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¬ë£Œë¡œ ìƒˆë¡œìš´ ìš”ë¦¬ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”.",
        "ì˜í™”ë¥¼ ë³´ëŠ” ê²ƒì€ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œì— ë„ì›€ì´ ë©ë‹ˆë‹¤. ì¢‹ì•„í•˜ëŠ” ì¥ë¥´ì˜ ì˜í™”ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.",
        "ë…ì„œëŠ” ìƒìƒë ¥ì„ í‚¤ì›Œì£¼ëŠ” ì¢‹ì€ í™œë™ì…ë‹ˆë‹¤. ì†Œì„¤ë¶€í„° ì—ì„¸ì´ê¹Œì§€ ë‹¤ì–‘í•˜ê²Œ ì½ì–´ë³´ì„¸ìš”.",
        "ì‚°ì±…ì„ í•˜ë©´ ë§ˆìŒì´ ë§‘ì•„ì§‘ë‹ˆë‹¤. ìì—° ì†ì—ì„œ ê±·ëŠ” ê²ƒì€ ì‹¬ì‹ ì˜ ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤.",
        "ì¢‹ì€ ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•˜ë©´ ì¸ìƒì´ ë” ì˜ë¯¸ìˆì–´ì§‘ë‹ˆë‹¤. ê¸ì •ì ì¸ ê´€ê³„ë¥¼ ë§Œë“¤ì–´ê°€ì„¸ìš”."
    ]
    
    # ëª¨ë¸ ì„¤ì •
    teacher_model.eval()
    student_model.train()
    
    # ê°•ë ¥í•œ ì˜µí‹°ë§ˆì´ì € ì„¤ì • (ì§„ì§œ í•™ìŠµì„ ìœ„í•´)
    optimizer = torch.optim.AdamW(
        student_model.parameters(),
        lr=base_lr,  # í° í•™ìŠµë¥ 
        weight_decay=0.01,
        eps=1e-8,
        betas=(0.9, 0.999)
    )
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=total_steps,
        eta_min=base_lr * 0.05
    )
    
    total_loss = 0.0
    step_count = 0
    
    print(f"\nğŸ”¥ ì§„ì§œ í•™ìŠµ ì‹œì‘! (í•™ìŠµë¥ : {base_lr})")
    
    progress_bar = tqdm(range(total_steps), desc="ğŸŒ ë¦¬ë§Œ í•™ìŠµ")
    
    for step in progress_bar:
        # ë°ì´í„° ì„ íƒ (ìˆœí™˜)
        text = train_texts[step % len(train_texts)]
        
        # í† í¬ë‚˜ì´ì§• (ë” ê¸´ ì‹œí€€ìŠ¤)
        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=40,  # ë” ê¸´ ì»¨í…ìŠ¤íŠ¸
            truncation=True,
            padding=True
        )
        
        if inputs.input_ids.shape[1] < 4:
            continue
        
        input_ids = inputs.input_ids
        labels = input_ids[:, 1:].clone()
        input_ids = input_ids[:, :-1]
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
        optimizer.zero_grad()
        
        # Teacher ì¶œë ¥ (frozen)
        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids)
            teacher_logits = teacher_outputs.logits
        
        # Student ì¶œë ¥ (í•™ìŠµ ëŒ€ìƒ)
        student_outputs = student_model(input_ids)
        student_logits = student_outputs.logits
        
        # ë¦¬ë§Œí‰ë©´ Knowledge Distillation ì†ì‹¤
        kd_loss = riemann_kd_loss(
            student_logits, teacher_logits, temperature, use_rs=True
        )
        
        # Language Model ì†ì‹¤ (ë³´ì¡°)
        lm_loss = F.cross_entropy(
            student_logits.view(-1, student_logits.size(-1)),
            labels.view(-1),
            ignore_index=-100
        )
        
        # ì´ ì†ì‹¤ (KD ì¤‘ì‹¬)
        total_loss_step = 0.8 * kd_loss + 0.2 * lm_loss
        
        total_loss += total_loss_step.item()
        step_count += 1
        
        # ì—­ì „íŒŒ (ì§„ì§œ í•™ìŠµ!)
        total_loss_step.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì„±)
        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 2.0)
        
        # ì˜µí‹°ë§ˆì´ì € ìŠ¤í… (íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸)
        optimizer.step()
        scheduler.step()
        
        # ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ (í•™ìŠµ í™•ì¸)
        if step % 50 == 0:
            avg_loss = total_loss / step_count
            current_lr = optimizer.param_groups[0]['lr']
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ ê³„ì‚° (í•™ìŠµ ì—¬ë¶€ í™•ì¸)
            total_grad_norm = 0
            param_count = 0
            for param in student_model.parameters():
                if param.grad is not None:
                    total_grad_norm += param.grad.data.norm(2).item() ** 2
                    param_count += 1
            
            if param_count > 0:
                total_grad_norm = (total_grad_norm ** 0.5) / param_count
            else:
                total_grad_norm = 0
            
            progress_bar.set_postfix({
                'avg_loss': f'{avg_loss:.4f}',
                'lr': f'{current_lr:.1e}',
                'kd': f'{kd_loss.item():.3f}',
                'lm': f'{lm_loss.item():.3f}',
                'grad': f'{total_grad_norm:.4f}'
            })
    
    avg_loss = total_loss / step_count
    print(f"\n   ì „ì²´ í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
    print("âœ… ë¦¬ë§Œí‰ë©´ Knowledge Distillation ì™„ë£Œ!")
    
    return student_model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ í…ŒìŠ¤íŠ¸ & í‰ê°€ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_text_safe(model, tokenizer, prompt, max_length=30):
    """ì•ˆì „í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
    
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            repetition_penalty=1.3,
            no_repeat_ngram_size=3,
            pad_token_id=tokenizer.eos_token_id,
            min_length=len(inputs.input_ids[0]) + 5
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def evaluate_quality_simple(generated_text, prompt):
    """ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€"""
    
    generated_only = generated_text[len(prompt):].strip()
    if len(generated_only) < 3:
        return 0.5
    
    score = 2.5  # ê¸°ë³¸ ì ìˆ˜
    
    # ê¸¸ì´ í‰ê°€
    word_count = len(generated_only.split())
    if word_count >= 5:
        score += 0.3
    elif word_count >= 3:
        score += 0.2
    
    # ë‹¤ì–‘ì„± í‰ê°€
    unique_words = len(set(generated_only.split()))
    if unique_words >= 4:
        score += 0.2
    
    # ë°˜ë³µ í˜ë„í‹°
    if '/' in generated_only or len(re.findall(r'(.)\1{2,}', generated_only)) > 1:
        score -= 0.8
    
    # í•œêµ­ì–´ ì–´ë¯¸ í™•ì¸
    korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'í•´ìš”', 'ì–´ìš”']
    if any(generated_only.endswith(ending) for ending in korean_endings):
        score += 0.3
    
    return min(3.0, max(0.0, score))

def test_riemann_performance(model, tokenizer, model_type="í…ŒìŠ¤íŠ¸"):
    """ë¦¬ë§Œ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”",
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì´ë€",
        "ë§›ìˆëŠ” ìŒì‹ì€"
    ]
    
    print(f"\n=== {model_type} ë¦¬ë§Œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    results = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n[{i}/5] '{prompt}'")
        
        try:
            t0 = time.time()
            
            generated_text = generate_text_safe(model, tokenizer, prompt, max_length=35)
            
            elapsed = time.time() - t0
            
            print(f"  ìƒì„±: {generated_text}")
            print(f"  ì‹œê°„: {elapsed:.3f}ì´ˆ")
            
            quality_score = evaluate_quality_simple(generated_text, prompt)
            
            print(f"  í’ˆì§ˆ: {quality_score:.2f}/3.0")
            
            results.append({
                'prompt': prompt,
                'generated': generated_text,
                'time': elapsed,
                'quality': quality_score
            })
            
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬: {e}")
            results.append({
                'prompt': prompt,
                'generated': f"ERROR: {e}",
                'time': 0,
                'quality': 0
            })
    
    # í†µê³„
    avg_time = sum(r['time'] for r in results) / len(results) if results else 0
    avg_quality = sum(r['quality'] for r in results) / len(results) if results else 0
    
    print(f"\nğŸ“Š {model_type} í†µê³„:")
    print(f"  í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ")
    print(f"  í‰ê·  í’ˆì§ˆ: {avg_quality:.2f}/3.0")
    
    return results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main_riemann():
    """ë¦¬ë§Œí‰ë©´ ê¸°ë°˜ ë©”ì¸ í•¨ìˆ˜"""
    
    model_name = "skt/kogpt2-base-v2"
    print("ğŸŒ ë¦¬ë§Œí‰ë©´ RealityStone ì••ì¶•+í•™ìŠµ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print("ğŸ¯ ëª©í‘œ: ë¦¬ë§Œê¸°í•˜í•™ì—ì„œ ì§„ì§œ í•™ìŠµì´ ì¼ì–´ë‚˜ëŠ” ì••ì¶•+íŒŒì¸íŠœë‹!")
    print("ğŸ’ í•µì‹¬: í¬ì¸ì¹´ë ˆë³¼ + í•˜ì´í¼ë³¼ë¦­ë¼í”Œë¼ì‹œì•ˆ + ë«¼ë¹„ìš°ìŠ¤ë³€í™˜")
    print("Loading modelâ€¦")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    teacher_model = AutoModelForCausalLM.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 1ë‹¨ê³„: ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ“Š ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    original_results = test_riemann_performance(teacher_model, tokenizer, "ì›ë³¸")
    
    # 2ë‹¨ê³„: ë¦¬ë§Œí‰ë©´ ì••ì¶• ì ìš©
    print("\n" + "="*80)
    print("ğŸŒ ë¦¬ë§Œí‰ë©´ RealityStone ì••ì¶• ì ìš©")
    
    student_model = copy.deepcopy(teacher_model)
    student_model = apply_riemann_compression(student_model, compression_ratio=0.08)
    
    # 3ë‹¨ê³„: ì••ì¶• í›„ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ“Š ë¦¬ë§Œ ì••ì¶• í›„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    compressed_results = test_riemann_performance(student_model, tokenizer, "ì••ì¶• í›„")
    
    # 4ë‹¨ê³„: ë¦¬ë§Œí‰ë©´ íŒŒì¸íŠœë‹ (ì§„ì§œ í•™ìŠµ!)
    print("\n" + "="*80)
    print("ğŸ§  ë¦¬ë§Œí‰ë©´ Knowledge Distillation íŒŒì¸íŠœë‹")
    
    student_model = riemann_fine_tune(
        teacher_model, student_model, tokenizer,
        total_steps=800,      # ì¶©ë¶„í•œ ìŠ¤í…
        base_lr=3e-3,         # ì§„ì§œ í•™ìŠµì„ ìœ„í•œ í° í•™ìŠµë¥ 
        temperature=4.0       # ì ì ˆí•œ ì˜¨ë„
    )
    
    # 5ë‹¨ê³„: íŒŒì¸íŠœë‹ í›„ ìµœì¢… í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ“Š ë¦¬ë§Œ íŒŒì¸íŠœë‹ í›„ ìµœì¢… í…ŒìŠ¤íŠ¸")
    final_results = test_riemann_performance(student_model, tokenizer, "ë¦¬ë§Œ ìµœì¢…")
    
    # 6ë‹¨ê³„: ìµœì¢… ë¶„ì„
    print("\n" + "="*80)
    print("ğŸ† ë¦¬ë§Œí‰ë©´ RealityStone ìµœì¢… ë¶„ì„")
    print("="*80)
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    orig_quality = sum(r['quality'] for r in original_results) / len(original_results)
    comp_quality = sum(r['quality'] for r in compressed_results) / len(compressed_results)
    final_quality = sum(r['quality'] for r in final_results) / len(final_results)
    
    orig_time = sum(r['time'] for r in original_results) / len(original_results)
    final_time = sum(r['time'] for r in final_results) / len(final_results)
    
    # ì••ì¶• í†µê³„
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    student_params = sum(p.numel() for p in student_model.parameters())
    compression_ratio = student_params / teacher_params
    memory_saved = (1 - compression_ratio) * 100
    quality_retention = final_quality / orig_quality if orig_quality > 0 else 1
    quality_improvement = final_quality - comp_quality
    speed_improvement = orig_time / final_time if final_time > 0 else 1
    
    print(f"ğŸ“Š ë¦¬ë§Œí‰ë©´ ì„±ëŠ¥ ë¶„ì„:")
    print(f"   íŒŒë¼ë¯¸í„°: {teacher_params:,} â†’ {student_params:,}")
    print(f"   ì••ì¶•ë¥ : {compression_ratio:.1%} ({1/compression_ratio:.1f}Ã— ì••ì¶•)")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
    print(f"   í’ˆì§ˆ: ì›ë³¸ {orig_quality:.2f} â†’ ì••ì¶• {comp_quality:.2f} â†’ ìµœì¢… {final_quality:.2f}")
    print(f"   ì†ë„: ì›ë³¸ {orig_time:.3f}ì´ˆ â†’ ìµœì¢… {final_time:.3f}ì´ˆ ({speed_improvement:.1f}Ã—)")
    print(f"   íŒŒì¸íŠœë‹ ê°œì„ : {quality_improvement:+.2f}ì ")
    
    # ë¦¬ë§Œê¸°í•˜í•™ ì„±ê³µ í‰ê°€
    if memory_saved >= 70 and quality_retention >= 0.85 and quality_improvement > 0.3:
        grade = "ğŸ† ë¦¬ë§Œ ëŒ€ì„±ê³µ!"
        message = f"ë¦¬ë§Œí‰ë©´ì—ì„œ ì••ì¶• + ì§„ì§œ í•™ìŠµ ëª¨ë‘ ì„±ê³µ!"
    elif memory_saved >= 60 and quality_retention >= 0.75 and quality_improvement > 0.1:
        grade = "ğŸ¥‡ ë¦¬ë§Œ ì„±ê³µ!"
        message = f"ë¦¬ë§Œê¸°í•˜í•™ìœ¼ë¡œ ìƒë‹¹í•œ ì„±ê³¼!"
    elif memory_saved >= 50 and quality_improvement > 0:
        grade = "ğŸ¥ˆ ë¦¬ë§Œ ë¶€ë¶„ì„±ê³µ!"
        message = f"ë¦¬ë§Œì••ì¶• ì„±ê³µ, í•™ìŠµ ì¼ë¶€ ê°œì„ !"
    else:
        grade = "ğŸ”§ ë¦¬ë§Œ ê°œì„ í•„ìš”"
        message = f"ë¦¬ë§Œê¸°í•˜í•™ ì¶”ê°€ ìµœì í™” í•„ìš”"
    
    print(f"\nğŸ¯ ë¦¬ë§Œí‰ë©´ ìµœì¢… í‰ê°€: {grade}")
    print(f"   {message}")
    print(f"   ğŸ’ í•µì‹¬ ê¸°ìˆ : RealityStone ë¦¬ë§Œê¸°í•˜í•™ ì™„ì „ í™œìš©")
    print(f"   ğŸŒ ì‚¬ìš© ê¸°ë²•: í¬ì¸ì¹´ë ˆë³¼ + í•˜ì´í¼ë³¼ë¦­ë¼í”Œë¼ì‹œì•ˆ + ë«¼ë¹„ìš°ìŠ¤ë³€í™˜ + êµ¬ë©´ì¡°í™”í•¨ìˆ˜")
    print(f"   ğŸ§  í•™ìŠµ ì„±ê³¼: ì§„ì§œ í•™ìŠµë¥  {3e-3}ë¡œ ì‹¤ì œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë‹¬ì„±")
    
    # ìƒì„± ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“ ë¦¬ë§Œí‰ë©´ ìƒì„± ìƒ˜í”Œ:")
    for i, result in enumerate(final_results[:3], 1):
        if not result['generated'].startswith('ERROR'):
            print(f"   [{i}] {result['prompt']} â†’ {result['generated']}")
    
    print(f"\nâœ¨ ë¦¬ë§Œí‰ë©´ RealityStone ì‹œìŠ¤í…œ ì™„ë£Œ!")
    
    return {
        'compression_ratio': compression_ratio,
        'memory_saved': memory_saved,
        'quality_retention': quality_retention,
        'quality_improvement': quality_improvement,
        'speed_improvement': speed_improvement,
        'final_grade': grade,
        'riemann_success': quality_improvement > 0.1 and memory_saved > 50
    }

if __name__ == "__main__":
    main_riemann() 