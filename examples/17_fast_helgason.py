"""
Reality Stone ë¹ ë¥¸ í—¬ê°€ì† ì••ì¶•
ì†ë„ ìµœì í™”ëœ ì‹¤ìš©ì ì¸ ë²„ì „

í•µì‹¬: ë‹¨ìˆœí•œ í‘¸ì•µì¹´ë ˆ + ë²¡í„°í™” í‘¸ë¦¬ì— + ë¹ ë¥¸ ì••ì¶•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
import math
import warnings
warnings.filterwarnings("ignore")


class FastPoincareFourier:
    """ë¹ ë¥¸ í‘¸ì•µì¹´ë ˆ í‘¸ë¦¬ì— ì••ì¶•"""
    
    def __init__(self, num_coeffs=16):
        self.num_coeffs = num_coeffs
        print(f"âš¡ ë¹ ë¥¸ í‘¸ì•µì¹´ë ˆ í‘¸ë¦¬ì— (ê³„ìˆ˜: {num_coeffs}ê°œ)")
    
    def compress_matrix(self, matrix):
        """í–‰ë ¬ì„ ë¹ ë¥´ê²Œ ì••ì¶•"""
        # 1. í‘¸ì•µì¹´ë ˆ ì •ê·œí™” (tanhë¡œ [-0.95, 0.95] ë§¤í•‘)
        matrix_flat = matrix.flatten()
        max_val = torch.max(torch.abs(matrix_flat))
        
        if max_val > 0:
            normalized = matrix_flat / max_val
            poincare_vals = torch.tanh(normalized) * 0.95
        else:
            poincare_vals = matrix_flat
            max_val = torch.tensor(1.0, device=matrix.device)
        
        # 2. ë¹ ë¥¸ FFT (PyTorch ë‚´ì¥ ì‚¬ìš©)
        # ì‹¤ìˆ˜ë¥¼ ë³µì†Œìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ FFT ì ìš©
        if len(poincare_vals) % 2 == 1:
            poincare_vals = torch.cat([poincare_vals, torch.zeros(1, device=matrix.device)])
        
        # ë³µì†Œìˆ˜ ë³€í™˜
        real_part = poincare_vals[::2]
        imag_part = poincare_vals[1::2]
        complex_vals = torch.complex(real_part, imag_part)
        
        # FFT
        fft_result = torch.fft.fft(complex_vals)
        
        # 3. ì¤‘ìš”í•œ ê³„ìˆ˜ë§Œ ì„ íƒ (ì—ë„ˆì§€ ê¸°ë°˜)
        magnitudes = torch.abs(fft_result)
        _, top_indices = torch.topk(magnitudes, min(self.num_coeffs, len(magnitudes)))
        
        # ì„ íƒëœ ê³„ìˆ˜ë“¤
        important_coeffs = fft_result[top_indices]
        
        return {
            'coeffs': important_coeffs,
            'indices': top_indices,
            'original_length': len(complex_vals),
            'original_shape': matrix.shape,
            'scale': max_val
        }
    
    def decompress_matrix(self, compressed):
        """ì••ì¶•ëœ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ë³µì›"""
        coeffs = compressed['coeffs']
        indices = compressed['indices']
        original_length = compressed['original_length']
        original_shape = compressed['original_shape']
        scale = compressed['scale']
        
        # FFT ê³„ìˆ˜ ë³µì›
        full_fft = torch.zeros(original_length, dtype=torch.complex64, device=coeffs.device)
        full_fft[indices] = coeffs
        
        # IFFT
        restored_complex = torch.fft.ifft(full_fft)
        
        # ì‹¤ìˆ˜ ë³€í™˜
        real_parts = restored_complex.real
        imag_parts = restored_complex.imag
        restored_flat = torch.stack([real_parts, imag_parts], dim=1).flatten()
        
        # ì›ë˜ í¬ê¸° ë§ì¶¤
        total_size = torch.prod(torch.tensor(original_shape)).item()
        if len(restored_flat) > total_size:
            restored_flat = restored_flat[:total_size]
        elif len(restored_flat) < total_size:
            padding = torch.zeros(total_size - len(restored_flat), device=restored_flat.device)
            restored_flat = torch.cat([restored_flat, padding])
        
        # ìŠ¤ì¼€ì¼ ë³µì› ë° reshape
        restored_matrix = restored_flat.view(original_shape) * scale
        
        return restored_matrix


class FastHelgasonLayer(nn.Module):
    """ë¹ ë¥¸ í—¬ê°€ì† ì••ì¶• ë ˆì´ì–´"""
    
    def __init__(self, mlp_layers, num_coeffs=16):
        super().__init__()
        
        self.num_layers = len(mlp_layers)
        print(f"\nâš¡ ë¹ ë¥¸ í—¬ê°€ì† ë ˆì´ì–´ ({self.num_layers}ê°œ ìœµí•©)")
        
        # ì••ì¶•ê¸° ì´ˆê¸°í™”
        self.compressor = FastPoincareFourier(num_coeffs)
        
        # ê°€ì¤‘ì¹˜ ìœµí•© (ë‹¨ìˆœ í‰ê· )
        c_fc_weights = [mlp.c_fc.weight.data for mlp in mlp_layers]
        c_proj_weights = [mlp.c_proj.weight.data for mlp in mlp_layers]
        
        print("   âš¡ ë¹ ë¥¸ ê°€ì¤‘ì¹˜ ìœµí•©...")
        fused_c_fc = torch.mean(torch.stack(c_fc_weights), dim=0)
        fused_c_proj = torch.mean(torch.stack(c_proj_weights), dim=0)
        
        # ì••ì¶•
        print("   âš¡ c_fc ì••ì¶•...")
        self.c_fc_compressed = self.compressor.compress_matrix(fused_c_fc)
        
        print("   âš¡ c_proj ì••ì¶•...")
        self.c_proj_compressed = self.compressor.compress_matrix(fused_c_proj)
        
        # ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
        if mlp_layers[0].c_fc.bias is not None:
            biases = torch.stack([mlp.c_fc.bias.data for mlp in mlp_layers])
            self.c_fc_bias = nn.Parameter(torch.mean(biases, dim=0))
        else:
            self.register_parameter('c_fc_bias', None)
            
        if mlp_layers[0].c_proj.bias is not None:
            biases = torch.stack([mlp.c_proj.bias.data for mlp in mlp_layers])
            self.c_proj_bias = nn.Parameter(torch.mean(biases, dim=0))
        else:
            self.register_parameter('c_proj_bias', None)
        
        self.activation = nn.GELU()
        
        # í†µê³„
        self._print_stats(c_fc_weights + c_proj_weights)
    
    def _print_stats(self, original_weights):
        """ì••ì¶• í†µê³„"""
        original_params = sum(w.numel() for w in original_weights)
        
        # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° (ë³µì†Œìˆ˜ = ì‹¤ìˆ˜ 2ê°œ)
        compressed_params = (
            len(self.c_fc_compressed['coeffs']) * 2 +  # ë³µì†Œìˆ˜
            len(self.c_proj_compressed['coeffs']) * 2 +
            (self.c_fc_bias.numel() if self.c_fc_bias is not None else 0) +
            (self.c_proj_bias.numel() if self.c_proj_bias is not None else 0)
        )
        
        self.compression_ratio = compressed_params / original_params
        memory_saved = (1 - self.compression_ratio) * 100
        
        print(f"   ğŸ’¾ ì••ì¶• í†µê³„:")
        print(f"   ì›ë³¸: {original_params:,} â†’ ì••ì¶•: {compressed_params:,}")
        print(f"   ì••ì¶•ë¥ : {self.compression_ratio:.3f}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
    
    def forward(self, x):
        """ë¹ ë¥¸ ìˆœì „íŒŒ"""
        # c_fc ë³µì› ë° ì ìš©
        c_fc_weight = self.compressor.decompress_matrix(self.c_fc_compressed)
        h = F.linear(x, c_fc_weight.T, self.c_fc_bias)
        h = self.activation(h)
        
        # c_proj ë³µì› ë° ì ìš©
        c_proj_weight = self.compressor.decompress_matrix(self.c_proj_compressed)
        output = F.linear(h, c_proj_weight.T, self.c_proj_bias)
        
        return output


def apply_fast_helgason_compression(model, num_coeffs=16):
    """ë¹ ë¥¸ í—¬ê°€ì† ì••ì¶• ì ìš©"""
    
    print(f"\nâš¡ ë¹ ë¥¸ í—¬ê°€ì† ì••ì¶•")
    print("=" * 40)
    
    total_layers = len(model.transformer.h)
    original_params = sum(p.numel() for p in model.parameters())
    
    # ë§ˆì§€ë§‰ 2ê°œ ë ˆì´ì–´ë§Œ ìœµí•© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    if total_layers >= 2:
        fusion_groups = [[total_layers - 2, total_layers - 1]]
    else:
        print("   âš ï¸ ë ˆì´ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return model, 1.0
    
    print(f"   ì›ë³¸ ë ˆì´ì–´: {total_layers}ê°œ")
    print(f"   ìœµí•© ê·¸ë£¹: {fusion_groups}")
    
    # ìœµí•© ì ìš©
    for group in fusion_groups:
        print(f"\nğŸ“¦ ê·¸ë£¹ {group} ì••ì¶•...")
        
        # MLP ìˆ˜ì§‘
        mlp_layers = [model.transformer.h[i].mlp for i in group]
        
        # ë¹ ë¥¸ ì••ì¶• ë ˆì´ì–´ ìƒì„±
        compressed_layer = FastHelgasonLayer(mlp_layers, num_coeffs)
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ì— ë°°ì¹˜
        model.transformer.h[group[0]].mlp = compressed_layer
    
    # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì œê±°
    del model.transformer.h[-1]
    
    # ìµœì¢… í†µê³„
    final_params = sum(p.numel() for p in model.parameters())
    total_compression = final_params / original_params
    memory_saved = (1 - total_compression) * 100
    
    print(f"\nğŸ“Š ì „ì²´ ì••ì¶• ê²°ê³¼:")
    print(f"   ë ˆì´ì–´: {total_layers} â†’ {len(model.transformer.h)}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {final_params:,}")
    print(f"   ì••ì¶•ë¥ : {total_compression:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
    
    return model, total_compression


def quick_test(model, tokenizer, test_name=""):
    """ë¹ ë¥¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
    
    print(f"ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ {test_name}")
    
    prompts = ["í•œêµ­ì˜ ìˆ˜ë„ëŠ”", "ì•ˆë…•í•˜ì„¸ìš”"]
    scores = []
    
    for prompt in prompts:
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 5,  # ì§§ê²Œ
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ê°„ë‹¨í•œ ì²´í¬
            score = 1 if len(generated) > len(prompt) + 1 else 0
            scores.append(score)
            
            status = "âœ…" if score else "âŒ"
            print(f"   '{prompt}' â†’ '{generated}' {status}")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì—ëŸ¬: {str(e)[:30]}... âŒ")
            scores.append(0)
    
    quality = sum(scores) / len(scores) if scores else 0
    print(f"   í’ˆì§ˆ: {quality:.1%}")
    
    return quality


def main():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    
    print("âš¡ ë¹ ë¥¸ í—¬ê°€ì† í‘¸ë¦¬ì— ì••ì¶• í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ì›ë³¸ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“‹ ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    original_quality = quick_test(model, tokenizer, "(ì›ë³¸)")
    
    # ë¹ ë¥¸ ì••ì¶•
    try:
        print(f"\nâš¡ ë¹ ë¥¸ í—¬ê°€ì† ì••ì¶• ì‹œì‘...")
        compressed_model = copy.deepcopy(model)
        compressed_model, compression_ratio = apply_fast_helgason_compression(compressed_model, num_coeffs=12)
        
        # ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ“‹ ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        compressed_quality = quick_test(compressed_model, tokenizer, "(ì••ì¶•)")
        
        # ê²°ê³¼
        quality_retention = compressed_quality / original_quality if original_quality > 0 else 0
        memory_saved = (1 - compression_ratio) * 100
        
        print(f"\nğŸ† ë¹ ë¥¸ ê²°ê³¼:")
        print(f"   ì›ë³¸ í’ˆì§ˆ: {original_quality:.1%}")
        print(f"   ì••ì¶• í’ˆì§ˆ: {compressed_quality:.1%}")
        print(f"   í’ˆì§ˆ ë³´ì¡´: {quality_retention:.1%}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
        
        if memory_saved >= 3 and quality_retention >= 0.5:
            print(f"\nğŸ‰ ë¹ ë¥¸ ì„±ê³µ! ì‹¤ìš©ì ì¸ ì••ì¶•")
            print(f"   âœ… ì†ë„ + íš¨ìœ¨ì„± ë‹¬ì„±")
        elif memory_saved >= 3:
            print(f"\nâš¡ ì••ì¶• ì„±ê³µ! ì†ë„ ìš°ì„ ")
        else:
            print(f"\nğŸ’ª ë” ë‚˜ì€ ì„¤ì • í•„ìš”")
        
        print(f"\nâš¡ ë¹ ë¥¸ ê¸°ìˆ :")
        print(f"   âœ… ë²¡í„°í™” í‘¸ì•µì¹´ë ˆ ë§¤í•‘")
        print(f"   âœ… PyTorch FFT í™œìš©")
        print(f"   âœ… TopK ê³„ìˆ˜ ì„ íƒ")
        print(f"   âœ… ìµœì í™”ëœ ë³µì›")
        print(f"   âœ… ë¹ ë¥¸ ì‹¤í–‰ ì†ë„")
        
        print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„±: ìš©ëŸ‰ê³¼ ì†ë„ íš¨ê³¼!")
        
    except Exception as e:
        print(f"âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main() 