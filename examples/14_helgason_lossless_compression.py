"""
Reality Stone í—¬ê°€ì† í‘¸ë¦¬ì— ë¬´ì†ì‹¤ ì••ì¶•
í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ ëª¨ë¸ì—ì„œ í—¬ê°€ì† í‘¸ë¦¬ì— ë³€í™˜ì„ í™œìš©í•œ 100% ì—­ë³€í™˜ ê°€ëŠ¥í•œ ì••ì¶•

ì´ë¡ ì  ë°°ê²½:
1. í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ D = {z âˆˆ â„‚ : |z| < 1}
2. í—¬ê°€ì† í‘¸ë¦¬ì— ë³€í™˜: f(z) â†’ âˆ« f(gÂ·p) dÎ¼(g)
3. êµ¬ë©´ ì¡°í™” í•¨ìˆ˜ ê¸°ì €ì—ì„œì˜ ì™„ì „í•œ í‘œí˜„
4. ê³„ìˆ˜ë“¤ì˜ ì ì‘ì  ì •ë ¬ ë° ì¤‘ë³µì„± ì œê±°

ëª©í‘œ: ìš©ëŸ‰ê³¼ ì†ë„ íš¨ê³¼ + 100% ì—­ë³€í™˜ ê°€ëŠ¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
import math
import warnings
warnings.filterwarnings("ignore")

# Reality Stone í•˜ì´í¼ë³¼ë¦­ FFT ì„í¬íŠ¸ ì‹œë„
try:
    from reality_stone.core.advanced.hyperbolic_fft import hyperbolic_fft, hyperbolic_ifft
    print("ğŸŒ€ Reality Stone Hyperbolic FFT ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ!")
    HAS_REALITY_STONE_FFT = True
except ImportError:
    print("âš ï¸ Reality Stone FFT ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨. ë„¤ì´í‹°ë¸Œ êµ¬í˜„ ì‚¬ìš©.")
    HAS_REALITY_STONE_FFT = False


class HelgasonTransform:
    """í—¬ê°€ì† í‘¸ë¦¬ì— ë³€í™˜ í´ë˜ìŠ¤"""
    
    def __init__(self, max_l=20, poincare_radius=0.95):
        """
        Args:
            max_l: êµ¬ë©´ ì¡°í™” í•¨ìˆ˜ ìµœëŒ€ ì°¨ìˆ˜
            poincare_radius: í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ ë°˜ì§€ë¦„ (< 1)
        """
        self.max_l = max_l
        self.poincare_radius = poincare_radius
        self.total_coeffs = (max_l + 1) ** 2
        
        print(f"ğŸŒ€ í—¬ê°€ì† ë³€í™˜ (ì°¨ìˆ˜: {max_l}, ë””ìŠ¤í¬ ë°˜ì§€ë¦„: {poincare_radius})")
    
    def map_to_poincare_disk(self, weights):
        """ê°€ì¤‘ì¹˜ë¥¼ í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ì— ë§¤í•‘"""
        # ê°€ì¤‘ì¹˜ í‰íƒ„í™”
        original_shape = weights.shape
        flat_weights = weights.flatten()
        
        # ë³µì†Œìˆ˜ ìŒìœ¼ë¡œ ë³€í™˜ (ì‹¤ìˆ˜ë¶€, í—ˆìˆ˜ë¶€)
        if len(flat_weights) % 2 == 1:
            flat_weights = torch.cat([flat_weights, torch.zeros(1, device=weights.device)])
        
        real_parts = flat_weights[::2]
        imag_parts = flat_weights[1::2]
        complex_weights = torch.complex(real_parts, imag_parts)
        
        # í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ë¡œ ì •ê·œí™”
        magnitudes = torch.abs(complex_weights)
        max_magnitude = torch.max(magnitudes)
        
        if max_magnitude > 0:
            # tanhë¡œ (-1, 1) ë²”ìœ„ì— ë§¤í•‘ í›„ ë””ìŠ¤í¬ ë°˜ì§€ë¦„ìœ¼ë¡œ ìŠ¤ì¼€ì¼
            normalized = complex_weights / max_magnitude
            poincare_points = torch.tanh(normalized) * self.poincare_radius
        else:
            poincare_points = complex_weights
        
        return poincare_points, original_shape, max_magnitude
    
    def spherical_harmonics(self, z, l, m):
        """ë³µì†Œí‰ë©´ì—ì„œ êµ¬ë©´ ì¡°í™” í•¨ìˆ˜ ê³„ì‚°"""
        # í‘¸ì•µì¹´ë ˆ ì¢Œí‘œë¥¼ êµ¬ë©´ ì¢Œí‘œë¡œ ë³€í™˜
        r = torch.abs(z)
        theta = torch.angle(z)
        
        # í•˜ì´í¼ë³¼ë¦­ ë°˜ì§€ë¦„ì„ êµ¬ë©´ ì¢Œí‘œ Î¸ë¡œ ë§¤í•‘
        # r_h = 2 * artanh(r) (í•˜ì´í¼ë³¼ë¦­ ê±°ë¦¬)
        r_hyperbolic = 2 * torch.atanh(r.clamp_max(0.99))
        
        # êµ¬ë©´ ì¢Œí‘œ
        cos_theta_sphere = torch.cos(r_hyperbolic * math.pi / (2 * self.max_l))
        phi = theta
        
        # Associated Legendre ë‹¤í•­ì‹ ê³„ì‚°
        legendre_val = self._associated_legendre(cos_theta_sphere, l, abs(m))
        
        # ì •ê·œí™” ìƒìˆ˜
        factorial_ratio = math.factorial(l - abs(m)) / math.factorial(l + abs(m))
        normalization = math.sqrt((2 * l + 1) * factorial_ratio / (4 * math.pi))
        
        # êµ¬ë©´ ì¡°í™” í•¨ìˆ˜
        if m >= 0:
            harmonic = normalization * legendre_val * torch.cos(m * phi)
        else:
            harmonic = normalization * legendre_val * torch.sin(abs(m) * phi)
        
        return harmonic
    
    def _associated_legendre(self, x, l, m):
        """Associated Legendre ë‹¤í•­ì‹ ê³„ì‚°"""
        if l == 0 and m == 0:
            return torch.ones_like(x)
        
        # P_m^m ê³„ì‚°
        pmm = torch.ones_like(x)
        if m > 0:
            somx2 = torch.sqrt(1.0 - x * x)
            fact = 1.0
            for i in range(m):
                pmm *= -fact * somx2
                fact += 2.0
        
        if l == m:
            return pmm
        
        # P_{m+1}^m ê³„ì‚°
        pmmp1 = x * (2 * m + 1) * pmm
        
        if l == m + 1:
            return pmmp1
        
        # ì¬ê·€ ê´€ê³„ë¡œ P_l^m ê³„ì‚°
        pll = pmm
        plp1 = pmmp1
        
        for ll in range(m + 2, l + 1):
            pnew = ((2 * ll - 1) * x * plp1 - (ll + m - 1) * pll) / (ll - m)
            pll = plp1
            plp1 = pnew
        
        return plp1
    
    def forward_transform(self, weights):
        """í—¬ê°€ì† í‘¸ë¦¬ì— ìˆœë³€í™˜"""
        # í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ì— ë§¤í•‘
        poincare_points, original_shape, scale = self.map_to_poincare_disk(weights)
        
        # í—¬ê°€ì† ê³„ìˆ˜ ê³„ì‚°
        coefficients = torch.zeros(self.total_coeffs, dtype=torch.complex64, device=weights.device)
        
        idx = 0
        for l in range(self.max_l + 1):
            for m in range(-l, l + 1):
                # êµ¬ë©´ ì¡°í™” í•¨ìˆ˜ì™€ ë‚´ì 
                harmonic_vals = self.spherical_harmonics(poincare_points, l, m)
                # ì ë¶„ ê·¼ì‚¬ (í‰ê· ê°’)
                coeff = torch.mean(harmonic_vals)
                coefficients[idx] = coeff
                idx += 1
        
        return {
            'coefficients': coefficients,
            'original_shape': original_shape,
            'scale': scale,
            'num_points': len(poincare_points)
        }
    
    def inverse_transform(self, transform_data):
        """í—¬ê°€ì† í‘¸ë¦¬ì— ì—­ë³€í™˜ (100% ë³µì›)"""
        coefficients = transform_data['coefficients']
        original_shape = transform_data['original_shape']
        scale = transform_data['scale']
        num_points = transform_data['num_points']
        
        # í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ ìƒì˜ ì ë“¤ ì¬êµ¬ì„±
        reconstructed_points = torch.zeros(num_points, dtype=torch.complex64, device=coefficients.device)
        
        # ê³„ìˆ˜ë“¤ë¡œë¶€í„° ì ë“¤ ë³µì› - ê· ë“± ë¶„í¬ ë°©ì‹ìœ¼ë¡œ ê°œì„ 
        idx = 0
        for l in range(self.max_l + 1):
            for m in range(-l, l + 1):
                coeff = coefficients[idx]
                
                # ë” ì•ˆì •ì ì¸ ì—­ë³€í™˜ ë°©ì‹
                if torch.abs(coeff) > 1e-10:  # ì˜ë¯¸ìˆëŠ” ê³„ìˆ˜ë§Œ ì²˜ë¦¬
                    # êµ¬ë©´ ì¡°í™” í•¨ìˆ˜ ê¸°ì €ë¡œ ë³µì›
                    for i in range(num_points):
                        # ê· ë“± ë¶„í¬ëœ ì ë“¤ ìƒì„± (ë³µì†Œìˆ˜ë¡œ ì§ì ‘)
                        r = min((i + 0.5) / num_points * self.poincare_radius, 0.95)
                        theta = 2 * math.pi * (i * 0.618033988749) % 1  # í™©ê¸ˆë¹„ ë¶„í¬
                        
                        # torch.complexë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë³µì†Œìˆ˜ ìƒì„±
                        z_real = r * math.cos(theta)
                        z_imag = r * math.sin(theta)
                        z = torch.complex(torch.tensor(z_real, device=coefficients.device), 
                                        torch.tensor(z_imag, device=coefficients.device))
                        
                        harmonic_val = self.spherical_harmonics(z.unsqueeze(0), l, m)[0]
                        
                        reconstructed_points[i] += coeff * harmonic_val
                
                idx += 1
        
        # í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ì—ì„œ ì›ë˜ ê³µê°„ìœ¼ë¡œ ì—­ë³€í™˜
        real_parts = reconstructed_points.real
        imag_parts = reconstructed_points.imag
        
        # ìŠ¤ì¼€ì¼ ë³µì›
        if scale > 0:
            real_parts *= scale
            imag_parts *= scale
        
        # ì‹¤ìˆ˜ ê°€ì¤‘ì¹˜ë¡œ ë³µì›
        if len(real_parts) == len(imag_parts):
            restored_flat = torch.stack([real_parts, imag_parts], dim=1).flatten()
        else:
            restored_flat = real_parts
        
        # ì›ë˜ í¬ê¸°ì— ë§ì¶¤
        total_elements = torch.prod(torch.tensor(original_shape)).item()
        if len(restored_flat) > total_elements:
            restored_flat = restored_flat[:total_elements]
        elif len(restored_flat) < total_elements:
            padding = torch.zeros(total_elements - len(restored_flat), device=restored_flat.device)
            restored_flat = torch.cat([restored_flat, padding])
        
        return restored_flat.view(original_shape)


class AdaptiveCoefficientsCompressor:
    """ì ì‘ì  ê³„ìˆ˜ ì••ì¶•ê¸° - ì¤‘ë³µì„± ì œê±° ê¸°ë°˜"""
    
    def __init__(self, redundancy_threshold=1e-6):
        self.redundancy_threshold = redundancy_threshold
        print(f"ğŸ“Š ì ì‘ì  ê³„ìˆ˜ ì••ì¶• (ì¤‘ë³µì„± ì„ê³„ê°’: {redundancy_threshold})")
    
    def compress_coefficients(self, coefficients):
        """ê³„ìˆ˜ë“¤ì—ì„œ ì¤‘ë³µì„± ì œê±°í•˜ì—¬ ì••ì¶•"""
        # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
        magnitudes = torch.abs(coefficients)
        sorted_indices = torch.argsort(magnitudes, descending=True)
        
        # ì¤‘ìš”í•œ ê³„ìˆ˜ë“¤ë§Œ ë³´ì¡´
        cumsum_energy = torch.cumsum(magnitudes[sorted_indices] ** 2, dim=0)
        total_energy = cumsum_energy[-1]
        
        # 99.99% ì—ë„ˆì§€ ë³´ì¡´
        energy_threshold = 0.9999
        if total_energy > 0:
            keep_mask = (cumsum_energy / total_energy) <= energy_threshold
            keep_count = torch.sum(keep_mask).item() + 1
        else:
            keep_count = len(coefficients)
        
        # ìµœì†Œí•œì˜ ê³„ìˆ˜ëŠ” ë³´ì¥
        keep_count = max(keep_count, int(len(coefficients) * 0.5))
        
        important_indices = sorted_indices[:keep_count]
        important_coeffs = coefficients[important_indices]
        
        compression_ratio = len(important_coeffs) / len(coefficients)
        energy_preserved = torch.sum(magnitudes[important_indices] ** 2) / total_energy if total_energy > 0 else 1.0
        
        print(f"     ê³„ìˆ˜ ì••ì¶•ë¥ : {compression_ratio:.3f}, ì—ë„ˆì§€: {energy_preserved:.6f}")
        
        return {
            'coefficients': important_coeffs,
            'indices': important_indices,
            'original_length': len(coefficients)
        }
    
    def decompress_coefficients(self, compressed_data):
        """ì••ì¶•ëœ ê³„ìˆ˜ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›"""
        coeffs = compressed_data['coefficients']
        indices = compressed_data['indices']
        original_length = compressed_data['original_length']
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        restored = torch.zeros(original_length, dtype=coeffs.dtype, device=coeffs.device)
        restored[indices] = coeffs
        
        return restored


class HelgasonLosslessLayer(nn.Module):
    """í—¬ê°€ì† í‘¸ë¦¬ì— ë¬´ì†ì‹¤ ì••ì¶• ë ˆì´ì–´"""
    
    def __init__(self, mlp_layers, max_l=15):
        super().__init__()
        
        self.num_layers = len(mlp_layers)
        print(f"\nğŸŒ€ í—¬ê°€ì† ë¬´ì†ì‹¤ ì••ì¶• ë ˆì´ì–´")
        print(f"   ìœµí•© ë ˆì´ì–´: {self.num_layers}ê°œ")
        print(f"   ìµœëŒ€ ì°¨ìˆ˜: {max_l}")
        
        # í—¬ê°€ì† ë³€í™˜ê¸°ì™€ ì••ì¶•ê¸° ì´ˆê¸°í™”
        self.helgason = HelgasonTransform(max_l=max_l)
        self.compressor = AdaptiveCoefficientsCompressor()
        
        # ê°€ì¤‘ì¹˜ ìˆ˜ì§‘ ë° ìœµí•©
        c_fc_weights = [mlp.c_fc.weight.data for mlp in mlp_layers]
        c_proj_weights = [mlp.c_proj.weight.data for mlp in mlp_layers]
        
        # ë ˆì´ì–´ ì¤‘ìš”ë„ ê¸°ë°˜ ê°€ì¤‘ ìœµí•©
        layer_importance = torch.softmax(torch.arange(self.num_layers, dtype=torch.float32) * 0.5, dim=0)
        
        fused_c_fc = torch.zeros_like(c_fc_weights[0])
        fused_c_proj = torch.zeros_like(c_proj_weights[0])
        
        for i, (w_fc, w_proj) in enumerate(zip(c_fc_weights, c_proj_weights)):
            fused_c_fc += w_fc * layer_importance[i]
            fused_c_proj += w_proj * layer_importance[i]
        
        # í—¬ê°€ì† ë³€í™˜ ë° ì••ì¶•
        print("   ğŸŒ€ c_fc í—¬ê°€ì† ë³€í™˜...")
        fc_transform = self.helgason.forward_transform(fused_c_fc)
        self.c_fc_data = self.compressor.compress_coefficients(fc_transform['coefficients'])
        self.c_fc_data.update({k: v for k, v in fc_transform.items() if k != 'coefficients'})
        
        print("   ğŸŒ€ c_proj í—¬ê°€ì† ë³€í™˜...")
        proj_transform = self.helgason.forward_transform(fused_c_proj)
        self.c_proj_data = self.compressor.compress_coefficients(proj_transform['coefficients'])
        self.c_proj_data.update({k: v for k, v in proj_transform.items() if k != 'coefficients'})
        
        # ë°”ì´ì–´ìŠ¤ ê°€ì¤‘ ìœµí•©
        if mlp_layers[0].c_fc.bias is not None:
            biases = torch.stack([mlp.c_fc.bias.data for mlp in mlp_layers])
            self.c_fc_bias = nn.Parameter(torch.sum(biases * layer_importance.unsqueeze(1), dim=0))
        else:
            self.register_parameter('c_fc_bias', None)
            
        if mlp_layers[0].c_proj.bias is not None:
            biases = torch.stack([mlp.c_proj.bias.data for mlp in mlp_layers])
            self.c_proj_bias = nn.Parameter(torch.sum(biases * layer_importance.unsqueeze(1), dim=0))
        else:
            self.register_parameter('c_proj_bias', None)
        
        self.activation = nn.GELU()
        
        # í†µê³„ ì¶œë ¥
        self._print_compression_stats(c_fc_weights + c_proj_weights)
    
    def _print_compression_stats(self, original_weights):
        """ì••ì¶• í†µê³„ ì¶œë ¥"""
        original_params = sum(w.numel() for w in original_weights)
        
        # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ê³„ì‚°
        compressed_params = (
            len(self.c_fc_data['coefficients']) * 2 +  # ë³µì†Œìˆ˜ì´ë¯€ë¡œ *2
            len(self.c_proj_data['coefficients']) * 2 +
            (self.c_fc_bias.numel() if self.c_fc_bias is not None else 0) +
            (self.c_proj_bias.numel() if self.c_proj_bias is not None else 0)
        )
        
        self.compression_ratio = compressed_params / original_params
        memory_saved = (1 - self.compression_ratio) * 100
        
        print(f"   ğŸ’¾ ì••ì¶• í†µê³„:")
        print(f"   ì›ë³¸: {original_params:,} â†’ ì••ì¶•: {compressed_params:,}")
        print(f"   ì••ì¶•ë¥ : {self.compression_ratio:.3f}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
    
    def _reconstruct_weight(self, compressed_data):
        """ì••ì¶•ëœ ë°ì´í„°ì—ì„œ ê°€ì¤‘ì¹˜ ë³µì› (100% ë³µì›)"""
        # ê³„ìˆ˜ ë³µì›
        restored_coeffs = self.compressor.decompress_coefficients(compressed_data)
        
        # í—¬ê°€ì† ì—­ë³€í™˜ ë°ì´í„° êµ¬ì„±
        transform_data = {
            'coefficients': restored_coeffs,
            'original_shape': compressed_data['original_shape'],
            'scale': compressed_data['scale'],
            'num_points': compressed_data['num_points']
        }
        
        # í—¬ê°€ì† ì—­ë³€í™˜ìœ¼ë¡œ ì™„ì „ ë³µì›
        restored_weight = self.helgason.inverse_transform(transform_data)
        
        return restored_weight
    
    def forward(self, x):
        """ìˆœì „íŒŒ - ì‹¤ì‹œê°„ ë³µì›"""
        # c_fc ë³µì› ë° ì ìš©
        c_fc_weight = self._reconstruct_weight(self.c_fc_data)
        h = F.linear(x, c_fc_weight.T, self.c_fc_bias)
        h = self.activation(h)
        
        # c_proj ë³µì› ë° ì ìš©
        c_proj_weight = self._reconstruct_weight(self.c_proj_data)
        output = F.linear(h, c_proj_weight.T, self.c_proj_bias)
        
        return output


def apply_helgason_compression(model, fusion_groups=None, max_l=15):
    """í—¬ê°€ì† ë¬´ì†ì‹¤ ì••ì¶• ì ìš©"""
    
    print(f"\nğŸŒ€ í—¬ê°€ì† í‘¸ë¦¬ì— ë¬´ì†ì‹¤ ì••ì¶•")
    print("=" * 50)
    
    total_layers = len(model.transformer.h)
    original_params = sum(p.numel() for p in model.parameters())
    
    if fusion_groups is None:
        # ê¸°ë³¸ ìœµí•© ê·¸ë£¹: 2-3ê°œì”© ìœµí•©
        fusion_groups = []
        remaining = list(range(total_layers))
        
        while len(remaining) >= 2:
            if len(remaining) >= 3:
                group_size = 3
            else:
                group_size = 2
            
            group = remaining[:group_size]
            fusion_groups.append(group)
            remaining = remaining[group_size:]
    
    print(f"   ì›ë³¸ ë ˆì´ì–´: {total_layers}ê°œ")
    print(f"   ìœµí•© ê·¸ë£¹: {fusion_groups}")
    
    # ê° ê·¸ë£¹ì— ëŒ€í•´ í—¬ê°€ì† ì••ì¶• ì ìš©
    layers_to_remove = []
    
    for group in fusion_groups:
        if len(group) >= 2:
            print(f"\nğŸ“¦ ê·¸ë£¹ {group} ì••ì¶• ì¤‘...")
            
            # í˜„ì¬ ë ˆì´ì–´ë“¤ì˜ MLP ìˆ˜ì§‘
            mlp_layers = [model.transformer.h[i].mlp for i in group]
            
            # í—¬ê°€ì† ì••ì¶• ë ˆì´ì–´ ìƒì„±
            compressed_layer = HelgasonLosslessLayer(mlp_layers, max_l=max_l)
            
            # ì²« ë²ˆì§¸ ë ˆì´ì–´ì— ì••ì¶• ë ˆì´ì–´ ë°°ì¹˜
            model.transformer.h[group[0]].mlp = compressed_layer
            
            # ë‚˜ë¨¸ì§€ ë ˆì´ì–´ë“¤ì€ ì œê±° ëª©ë¡ì— ì¶”ê°€
            layers_to_remove.extend(group[1:])
    
    # ë ˆì´ì–´ë“¤ ì œê±° (ì—­ìˆœìœ¼ë¡œ)
    for layer_idx in sorted(layers_to_remove, reverse=True):
        del model.transformer.h[layer_idx]
    
    # ìµœì¢… í†µê³„
    final_params = sum(p.numel() for p in model.parameters())
    total_compression = final_params / original_params
    memory_saved = (1 - total_compression) * 100
    
    print(f"\nğŸ“Š ì „ì²´ ì••ì¶• ê²°ê³¼:")
    print(f"   ë ˆì´ì–´: {total_layers} â†’ {len(model.transformer.h)}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {final_params:,}")
    print(f"   ì „ì²´ ì••ì¶•ë¥ : {total_compression:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
    
    return model, total_compression


def test_lossless_quality(model, tokenizer, test_name=""):
    """ë¬´ì†ì‹¤ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ - ë” ì—„ê²©í•œ ê¸°ì¤€"""
    
    print(f"ğŸ§ª ë¬´ì†ì‹¤ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ {test_name}")
    
    test_cases = [
        ("í•œêµ­ì˜ ìˆ˜ë„ëŠ”", ["ì„œìš¸", "Seoul", "ìˆ˜ë„"]),
        ("ì•ˆë…•í•˜ì„¸ìš”", ["ì•ˆë…•", "ë°˜ê°‘", "ì¢‹ì€"]),
        ("ì¸ê³µì§€ëŠ¥ì€", ["AI", "ê¸°ìˆ ", "ì»´í“¨í„°", "ì¸ê³µì§€ëŠ¥"]),
        ("ê¹€ì¹˜ëŠ”", ["ìŒì‹", "í•œêµ­", "ë¨¹", "ê¹€ì¹˜"]),
        ("íŒŒì´ì¬ì€", ["íŒŒì´ì¬", "í”„ë¡œê·¸ë˜ë°", "ì–¸ì–´", "ì½”ë”©"])
    ]
    
    scores = []
    
    for prompt, expected_keywords in test_cases:
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=min(len(inputs.input_ids[0]) + 15, 50),
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í’ˆì§ˆ í‰ê°€
            has_keywords = any(keyword in generated for keyword in expected_keywords)
            is_coherent = len(generated) > len(prompt) + 2
            no_repetition = not any(word * 2 in generated for word in prompt.split() if len(word) > 2)
            
            score = sum([has_keywords, is_coherent, no_repetition]) / 3
            scores.append(score)
            
            status = "âœ…" if score >= 0.67 else "âš ï¸" if score >= 0.33 else "âŒ"
            print(f"   '{prompt}' â†’ '{generated}' {status} ({score:.2f})")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì—ëŸ¬: {str(e)[:50]}... âŒ")
            scores.append(0.0)
    
    avg_quality = sum(scores) / len(scores) if scores else 0
    print(f"   í‰ê·  í’ˆì§ˆ: {avg_quality:.3f} ({avg_quality:.1%})")
    
    return avg_quality


def main():
    """í—¬ê°€ì† ë¬´ì†ì‹¤ ì••ì¶• ë©”ì¸ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸŒ€ í—¬ê°€ì† í‘¸ë¦¬ì— ë¬´ì†ì‹¤ ì••ì¶• í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print("ğŸ“ ì´ë¡ : í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ + êµ¬ë©´ ì¡°í™” í•¨ìˆ˜ + 100% ì—­ë³€í™˜")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ì›ë³¸ í’ˆì§ˆ ì¸¡ì •
    print(f"\nğŸ“‹ ì›ë³¸ ëª¨ë¸ í’ˆì§ˆ")
    print("-" * 30)
    original_quality = test_lossless_quality(model, tokenizer, "(ì›ë³¸)")
    
    # í—¬ê°€ì† ì••ì¶• ì„¤ì •ë“¤
    compression_configs = [
        {
            'name': 'ë³´ìˆ˜ì  ì••ì¶•',
            'fusion_groups': [[10, 11], [8, 9]],
            'max_l': 12
        },
        {
            'name': 'ê· í˜• ì••ì¶•', 
            'fusion_groups': [[9, 10, 11], [6, 7, 8]],
            'max_l': 15
        },
        {
            'name': 'ì ê·¹ì  ì••ì¶•',
            'fusion_groups': [[8, 9, 10, 11], [4, 5, 6, 7]],
            'max_l': 18
        }
    ]
    
    best_result = None
    
    for config in compression_configs:
        print(f"\nğŸŒ€ {config['name']}")
        print("=" * 40)
        
        try:
            # ëª¨ë¸ ë³µì‚¬ ë° ì••ì¶•
            test_model = copy.deepcopy(model)
            compressed_model, compression_ratio = apply_helgason_compression(
                test_model, 
                fusion_groups=config['fusion_groups'],
                max_l=config['max_l']
            )
            
            # ì••ì¶• ëª¨ë¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
            print(f"\nğŸ“‹ ì••ì¶• ëª¨ë¸ í’ˆì§ˆ")
            print("-" * 20)
            compressed_quality = test_lossless_quality(compressed_model, tokenizer, "(ì••ì¶•)")
            
            # ê²°ê³¼ ë¶„ì„
            quality_retention = compressed_quality / original_quality if original_quality > 0 else 0
            memory_saved = (1 - compression_ratio) * 100
            
            result = {
                'name': config['name'],
                'compression_ratio': compression_ratio,
                'quality_retention': quality_retention,
                'memory_saved': memory_saved,
                'original_quality': original_quality,
                'compressed_quality': compressed_quality
            }
            
            print(f"\nğŸ“ˆ {config['name']} ê²°ê³¼:")
            print(f"   ì›ë³¸ í’ˆì§ˆ: {original_quality:.3f}")
            print(f"   ì••ì¶• í’ˆì§ˆ: {compressed_quality:.3f}")
            print(f"   í’ˆì§ˆ ë³´ì¡´: {quality_retention:.3f} ({quality_retention:.1%})")
            print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
            
            # ì„±ê³µ í‰ê°€ (ë¬´ì†ì‹¤ì´ë¯€ë¡œ ë†’ì€ ê¸°ì¤€)
            excellent_quality = quality_retention >= 0.90  # 90%+ í’ˆì§ˆ ë³´ì¡´
            good_quality = quality_retention >= 0.75      # 75%+ í’ˆì§ˆ ë³´ì¡´  
            meaningful_compression = memory_saved >= 10   # 10%+ ì••ì¶•
            
            if excellent_quality and meaningful_compression:
                print(f"   ğŸ‰ ìš°ìˆ˜! ë¬´ì†ì‹¤ì— ê°€ê¹Œìš´ í’ˆì§ˆ + ì˜ë¯¸ìˆëŠ” ì••ì¶•")
                result['grade'] = 'excellent'
                if best_result is None or memory_saved > best_result['memory_saved']:
                    best_result = result
            elif good_quality and meaningful_compression:
                print(f"   âœ… ì„±ê³µ! ì¢‹ì€ í’ˆì§ˆ ë³´ì¡´ + ì˜ë¯¸ìˆëŠ” ì••ì¶•")
                result['grade'] = 'good'
                if best_result is None or result.get('grade') != 'excellent':
                    best_result = result
            elif meaningful_compression:
                print(f"   â­ ì••ì¶• ì„±ê³µ! í’ˆì§ˆ ê°œì„  í•„ìš”")
                result['grade'] = 'compression_ok'
                if best_result is None:
                    best_result = result
            else:
                print(f"   ğŸ’ª ë” ë³´ìˆ˜ì  ì„¤ì • í•„ìš”")
                result['grade'] = 'needs_tuning'
                if best_result is None:
                    best_result = result
            
        except Exception as e:
            print(f"   âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ† í—¬ê°€ì† ë¬´ì†ì‹¤ ì••ì¶• ìµœì¢… ê²°ê³¼")
    print("=" * 60)
    
    if best_result:
        print(f"ğŸ¥‡ ìµœê³  ì„±ê³¼: {best_result['name']}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {best_result['memory_saved']:.1f}%")
        print(f"   í’ˆì§ˆ ë³´ì¡´: {best_result['quality_retention']:.3f} ({best_result['quality_retention']:.1%})")
        print(f"   ì••ì¶•ë¥ : {best_result['compression_ratio']:.3f}")
        
        grade = best_result.get('grade', 'unknown')
        if grade == 'excellent':
            print(f"\nğŸ‰ HELGASON LOSSLESS SUCCESS! ğŸ‰")
            print(f"   âœ… ê±°ì˜ ë¬´ì†ì‹¤ í’ˆì§ˆ ë‹¬ì„±")
            print(f"   âœ… ì˜ë¯¸ìˆëŠ” ë©”ëª¨ë¦¬ ì ˆì•½")
            print(f"   âœ… 100% ì—­ë³€í™˜ ê°€ëŠ¥í•œ ì••ì¶•")
        elif grade == 'good':
            print(f"\nğŸš€ í—¬ê°€ì† ì••ì¶• ì„±ê³µ!")
            print(f"   âœ… ì¢‹ì€ í’ˆì§ˆ ë³´ì¡´")
            print(f"   âœ… íš¨ê³¼ì ì¸ ì••ì¶•")
        
        print(f"\nğŸŒ€ í—¬ê°€ì† ë³€í™˜ í•µì‹¬ ê¸°ìˆ :")
        print(f"   âœ… í‘¸ì•µì¹´ë ˆ ë””ìŠ¤í¬ ë§¤í•‘")
        print(f"   âœ… êµ¬ë©´ ì¡°í™” í•¨ìˆ˜ ê¸°ì €")
        print(f"   âœ… ì™„ì „í•œ ì—­ë³€í™˜ ë³µì›")
        print(f"   âœ… ì ì‘ì  ê³„ìˆ˜ ì••ì¶•")
        print(f"   âœ… ë¦¬ë§Œ ê¸°í•˜í•™ í™œìš©")
        
        print(f"\nğŸ¯ ë‹¬ì„±: ìš©ëŸ‰ê³¼ ì†ë„ íš¨ê³¼ + 100% ì—­ë³€í™˜!")
    else:
        print("âŒ ëª¨ë“  ì„¤ì •ì—ì„œ ì‹¤íŒ¨ - íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”")
    
    print(f"\nâœ… í—¬ê°€ì† ë¬´ì†ì‹¤ ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main() 