#!/usr/bin/env python3
"""
ë¹ ë¥¸ ê·¹í•œ ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸
"""

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import re
from collections import Counter

def generate_with_anti_repetition(model, tokenizer, prompt, max_length=25):
    """ê·¹í•œ ë°˜ë³µ ë°©ì§€ ìƒì„± (í•œêµ­ì–´ ì´ˆíŠ¹í™”)"""
    
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_length=max_length,
            do_sample=True,
            temperature=0.6,          # ë³´ìˆ˜ì  ì˜¨ë„
            top_p=0.8,               # ì œí•œì  í™•ë¥  
            top_k=30,                # ì œí•œì  ì„ íƒ
            repetition_penalty=1.8,   # ë°˜ë³µ í˜ë„í‹° ê·¹ëŒ€í™”
            no_repeat_ngram_size=5,   # n-gram í¬ê¸° í™•ëŒ€
            pad_token_id=tokenizer.eos_token_id,
            # beam search ê´€ë ¨ ì„¤ì •ë“¤ ì œê±° (ì¶©ëŒ í•´ê²°)
            min_length=len(inputs.input_ids[0]) + 2,  # ìµœì†Œ ê¸¸ì´ ë³´ì¥
        )
    
    return tokenizer.decode(output[0], skip_special_tokens=True)

def advanced_quality_evaluation(generated_text, prompt):
    """ì—„ê²©í•œ í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    generated_only = generated_text[len(prompt):].strip()
    if len(generated_only) < 2:
        return 0.0
    
    score = 0.0
    max_score = 7.0
    
    # 1. ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬ (0-2ì )
    repetition_penalty = calculate_repetition_penalty(generated_only)
    repetition_score = max(0, 2.0 - repetition_penalty * 4)
    score += repetition_score
    
    # 2. í•œêµ­ì–´ ë¬¸ë²• êµ¬ì¡° (0-2ì )
    grammar_score = evaluate_korean_grammar(generated_only)
    score += grammar_score
    
    # 3. ì˜ë¯¸ ì—°ê´€ì„± (0-1.5ì )
    semantic_score = calculate_semantic_relevance(prompt, generated_only)
    score += semantic_score * 1.5
    
    # 4. í…ìŠ¤íŠ¸ ìì—°ìŠ¤ëŸ¬ì›€ (0-1ì )
    naturalness_score = evaluate_naturalness(generated_only)
    score += naturalness_score
    
    # 5. íŠ¹ìˆ˜ë¬¸ì/ì˜¤ë¥˜ íŒ¨í„´ í˜ë„í‹° (0-0.5ì )
    error_penalty = calculate_error_penalty(generated_only)
    score += max(0, 0.5 - error_penalty)
    
    return min(score / max_score * 3.0, 3.0)

def calculate_repetition_penalty(text):
    """ë°˜ë³µ íŒ¨í„´ í˜ë„í‹° ê³„ì‚°"""
    char_repeats = len(re.findall(r'(.)\1{2,}', text))
    words = text.split()
    if len(words) > 1:
        word_counts = Counter(words)
        repeated_words = sum(1 for count in word_counts.values() if count > 2)
    else:
        repeated_words = 0
    punct_repeats = len(re.findall(r'[.!?]{3,}|[~]{2,}|[/]{2,}', text))
    total_penalty = min(1.0, (char_repeats + repeated_words + punct_repeats * 2) / 10)
    return total_penalty

def evaluate_korean_grammar(text):
    """í•œêµ­ì–´ ë¬¸ë²• êµ¬ì¡° í‰ê°€"""
    score = 0.0
    korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'í•´ìš”', 'ì–´ìš”', 'ì•„ìš”', 'ë„¤ìš”', 'ì£ ', 'ìŠµë‹ˆë‹¤', 'ê² ìŠµë‹ˆë‹¤']
    has_proper_ending = any(text.endswith(ending) for ending in korean_endings)
    if has_proper_ending:
        score += 1.0
    elif any(ending in text for ending in korean_endings):
        score += 0.5
    
    sentences = [s.strip() for s in re.split('[.!?]', text) if s.strip()]
    if sentences:
        complete_sentences = sum(1 for s in sentences if len(s.split()) >= 2)
        if complete_sentences > 0:
            score += 0.8
        else:
            score += 0.3
    
    particles = ['ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ì˜']
    has_particles = any(p in text for p in particles)
    if has_particles:
        score += 0.2
    
    return min(score, 2.0)

def evaluate_naturalness(text):
    """í…ìŠ¤íŠ¸ ìì—°ìŠ¤ëŸ¬ì›€ í‰ê°€"""
    score = 1.0
    weird_patterns = [
        r'[.]{3,}', r'[!]{2,}', r'[?]{2,}', r'[/]{2,}', 
        r'[~]{3,}', r'[:]{2,}', r'[0-9]{5,}'
    ]
    
    for pattern in weird_patterns:
        if re.search(pattern, text):
            score -= 0.3
    
    words = text.split()
    if words:
        avg_word_length = sum(len(w) for w in words) / len(words)
        if avg_word_length > 10:
            score -= 0.3
    
    return max(0, score)

def calculate_error_penalty(text):
    """ì˜¤ë¥˜ íŒ¨í„´ í˜ë„í‹° ê³„ì‚°"""
    penalty = 0.0
    severe_errors = [
        r'[ê°€-í£]+[/]+[ê°€-í£]+', r'[:-]+[/]+',
        r'[&+-]{2,}', r'[()\[\]]{3,}'
    ]
    
    for pattern in severe_errors:
        matches = len(re.findall(pattern, text))
        penalty += matches * 0.5
    
    return penalty

def calculate_semantic_relevance(prompt, generated):
    """ì˜ë¯¸ì  ì—°ê´€ì„± ê³„ì‚°"""
    keyword_mapping = {
        'ì•ˆë…•': ['ì•ˆë…•', 'ë°˜ê°‘', 'ì¢‹', 'ê°ì‚¬'],
        'ë‚ ì”¨': ['ë‚ ì”¨', 'ë§‘', 'íë¦¼', 'ë¹„', 'ëˆˆ', 'ë”°ëœ»', 'ì¶¥', 'ì¢‹'],
        'ìˆ˜ë„': ['ì„œìš¸', 'ë„ì‹œ', 'í•œêµ­', 'ìˆ˜ë„'],
        'ì¸ê³µì§€ëŠ¥': ['AI', 'ê¸°ìˆ ', 'ì»´í“¨í„°', 'ë¡œë´‡', 'ì§€ëŠ¥', 'í•™ìŠµ'],
        'ìŒì‹': ['ìŒì‹', 'ë§›', 'ë¨¹', 'ìš”ë¦¬', 'ì‹ì‚¬'],
    }
    
    relevance = 0.0
    for key, keywords in keyword_mapping.items():
        if key in prompt:
            matches = sum(1 for kw in keywords if kw in generated)
            relevance = max(relevance, min(1.0, matches / 2))
    
    return relevance

def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸš€ ê·¹í•œ ì••ì¶• ëª¨ë¸ ë¹ ë¥¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    model_name = "skt/kogpt2-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”", 
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì´ë€",
        "ë§›ìˆëŠ” ìŒì‹ì€"
    ]
    
    results = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n[{i}/5] '{prompt}'")
        
        try:
            t0 = time.time()
            generated_text = generate_with_anti_repetition(model, tokenizer, prompt, max_length=25)
            elapsed = time.time() - t0
            
            print(f"  ìƒì„±: {generated_text}")
            print(f"  ì‹œê°„: {elapsed:.3f}ì´ˆ")
            
            quality_score = advanced_quality_evaluation(generated_text, prompt)
            print(f"  í’ˆì§ˆ: {quality_score:.2f}/3.0")
            
            results.append({
                'prompt': prompt,
                'generated': generated_text,
                'time': elapsed,
                'quality': quality_score
            })
            
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬: {e}")
            results.append({
                'prompt': prompt,
                'generated': f"ERROR: {e}",
                'time': 0,
                'quality': 0
            })
    
    # í†µê³„
    avg_time = sum(r['time'] for r in results) / len(results) if results else 0
    avg_quality = sum(r['quality'] for r in results) / len(results) if results else 0
    
    print(f"\nğŸ“Š ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í†µê³„:")
    print(f"  í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ")
    print(f"  í‰ê·  í’ˆì§ˆ: {avg_quality:.2f}/3.0")
    
    # ëª¨ë¸ ì •ë³´
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“‹ ëª¨ë¸ ì •ë³´:")
    print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,} ({total_params/1e6:.1f}M)")
    print(f"  ëª¨ë¸ íƒ€ì…: ì›ë³¸ ëª¨ë¸ (ì••ì¶• í…ŒìŠ¤íŠ¸ìš©)")
    
    if avg_quality >= 2.0:
        print("âœ… ìƒì„± ê¸°ëŠ¥ ì •ìƒ ì‘ë™!")
    elif avg_quality >= 1.0:
        print("ğŸ”§ ìƒì„± ê¸°ëŠ¥ ë¶€ë¶„ ì‘ë™")
    else:
        print("âŒ ìƒì„± ê¸°ëŠ¥ ë¬¸ì œ ìˆìŒ")

if __name__ == "__main__":
    quick_test()