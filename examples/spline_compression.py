import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, List, Dict, Any
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from transformers.modeling_utils import Conv1D
from datasets import load_dataset
HF_AVAILABLE = True
RS_AVAILABLE = True

try:
    import reality_stone as rs
    print("âœ… reality_stone ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    RS_AVAILABLE = False
    raise Exception("reality_stone ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

def catmull_rom_interpolation(control_points, t_values):
    """ë²¡í„°í™”ëœ Catmull-Rom ìŠ¤í”Œë¼ì¸ ë³´ê°„"""
    k = control_points.shape[0] - 1
    m = t_values.shape[0]
    
    t_scaled = t_values * k
    j = torch.floor(t_scaled).long()
    t_local = t_scaled - j.float()
    
    # ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í´ë¨í•‘
    j = torch.clamp(j, 1, k - 2)
    
    p0 = control_points[j - 1]
    p1 = control_points[j]
    p2 = control_points[j + 1]
    p3 = control_points[j + 2]
    
    t_local = t_local.unsqueeze(1) # (m, 1)
    
    # Catmull-Rom ê³µì‹
    return 0.5 * (
        (2 * p1) +
        (-p0 + p2) * t_local +
        (2 * p0 - 5 * p1 + 4 * p2 - p3) * t_local.pow(2) +
        (-p0 + 3 * p1 - 3 * p2 + p3) * t_local.pow(3)
    )

def geodesic_spline_with_reality_stone(control_points, t_values, use_reality_stone=True):
    """ë²¡í„°í™”ëœ ì§€ì˜¤ë°ì‹ ìŠ¤í”Œë¼ì¸ ë³´ê°„ (reality_stone ì‚¬ìš©)"""
    if not RS_AVAILABLE or not use_reality_stone:
        return catmull_rom_interpolation(control_points, t_values)
    
    try:
        k = control_points.shape[0] - 1
        m = t_values.shape[0]
        
        t_scaled = t_values * k
        j = torch.floor(t_scaled).long()
        t_local = (t_scaled - j.float()).unsqueeze(-1) # (m, 1)

        j = torch.clamp(j, 1, k - 2)
        
        # ì œì–´ì  ì„ íƒ (m, in_features)
        p0 = control_points[j - 1]
        p1 = control_points[j]
        p2 = control_points[j + 1]
        p3 = control_points[j + 2]

        # ì°¸ê³ : ì‹¤ì œ reality_stoneì˜ í•¨ìˆ˜ëª…ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆë¥¼ ë“¤ì–´ lorentz_exp_map, poincare_exp_map ë“±ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” lorentz_exp_map/log_mapì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        exp_map_func = getattr(rs, 'lorentz_exp_map', None)
        log_map_func = getattr(rs, 'lorentz_log_map', None)

        if exp_map_func and log_map_func:
            # ì ‘ì„  ë²¡í„° ê³„ì‚° (m, in_features)
            v1 = log_map_func(p1, p2)
            v0 = log_map_func(p1, p0)
            v2 = log_map_func(p1, p3)
            
            # Hermite ê¸°ë°˜ ì ‘ì„  ë²¡í„°
            tangent_p1 = 0.5 * (v1 - v0)
            tangent_p2 = 0.5 * (v2 - v1)
            
            # Hermite ê³„ìˆ˜ (m, 1)
            h00 = 2 * t_local**3 - 3 * t_local**2 + 1
            h10 = t_local**3 - 2 * t_local**2 + t_local
            h01 = -2 * t_local**3 + 3 * t_local**2
            h11 = t_local**3 - t_local**2
            
            # ì ‘ì„  ê³µê°„ì—ì„œ ë³´ê°„
            tangent_interp = h10 * tangent_p1 + h01 * v1 + h11 * tangent_p2
            
            # ì§€ìˆ˜ ë§µìœ¼ë¡œ ë‹¤ì‹œ ë§¤ë‹ˆí´ë“œë¡œ
            result = exp_map_func(p1, tangent_interp)
            return result
        else:
            print("reality_stoneì—ì„œ 'lorentz_exp_map'/'lorentz_log_map'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Fallbackí•©ë‹ˆë‹¤.")
            return catmull_rom_interpolation(control_points, t_values)
            
    except Exception as e:
        import traceback
        print(f"Reality Stone ì§€ì˜¤ë°ì‹ ë³´ê°„ ì‹¤íŒ¨: {e}. ì¼ë°˜ ìŠ¤í”Œë¼ì¸ ì‚¬ìš©.")
        traceback.print_exc()
        return catmull_rom_interpolation(control_points, t_values)

class SplineLinearTransform(nn.Module):
    """
    ìŠ¤í”Œë¼ì¸ ê¸°ë°˜ ì„ í˜• ë³€í™˜ ë ˆì´ì–´ (reality_stone í†µí•©)
    ì´ë¡ : m x n ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ëª¨ë“  mê°œ í–‰ì´ í•˜ë‚˜ì˜ ìŠ¤í”Œë¼ì¸ ìœ„ì— ë†“ì„
    ì••ì¶•ë¥ : (k+1) * n / (m * n) = (k+1) / m
    """
    def __init__(self, in_features: int, out_features: int, 
                 k: int = 50,  # ì œì–´ì  ê°œìˆ˜ - 1
                 use_geodesic: bool = True,
                 bias: bool = True,
                 initial_weight_data: Optional[torch.Tensor] = None,
                 initial_bias_data: Optional[torch.Tensor] = None,
                 module_name_for_debug: str = ""):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.k = k
        self.use_geodesic = use_geodesic and RS_AVAILABLE
        self.module_name_for_debug = module_name_for_debug
        
        # ì œì–´ì  ì´ˆê¸°í™” - ì´ë¡ ì— ë§ê²Œ (k+1, in_features) í˜•íƒœ
        if initial_weight_data is not None:
            # ê¸°ì¡´ ê°€ì¤‘ì¹˜ì—ì„œ ì œì–´ì  í”¼íŒ…
            if initial_weight_data.shape != (out_features, in_features):
                if initial_weight_data.shape == (in_features, out_features):
                    initial_weight_data = initial_weight_data.transpose(0, 1)
                else:
                    raise ValueError(f"ê°€ì¤‘ì¹˜ í˜•íƒœ ë¶ˆì¼ì¹˜: {initial_weight_data.shape}")
            
            self.control_points = nn.Parameter(self._fit_control_points_to_weight(initial_weight_data))
        else:
            # ëœë¤ ì´ˆê¸°í™” - (k+1, in_features) í˜•íƒœ
            self.control_points = nn.Parameter(
                torch.randn(k + 1, in_features) * 0.02
            )
        
        # Bias ì´ˆê¸°í™”
        if bias:
            if initial_bias_data is not None:
                if initial_bias_data.shape != (out_features,):
                    raise ValueError(f"Bias í˜•íƒœ ë¶ˆì¼ì¹˜: {initial_bias_data.shape}")
                self.bias = nn.Parameter(initial_bias_data.clone())
            else:
                self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)
        
        # ìºì‹œëœ ê°€ì¤‘ì¹˜ë¥¼ ìœ„í•œ ë²„í¼. persistent=Falseë¡œ state_dictì— ì €ì¥ë˜ì§€ ì•Šë„ë¡ í•¨
        self.register_buffer('cached_weight', torch.empty(0), persistent=False)
        
        if self.module_name_for_debug and initial_weight_data is not None:
            with torch.no_grad():
                # .to() í˜¸ì¶œì„ ìœ„í•´ ë””ë°”ì´ìŠ¤ë¥¼ ë§ì¶°ì¤Œ
                self.control_points = self.control_points.to(initial_weight_data.device)
                if self.bias is not None:
                    self.bias = self.bias.to(initial_weight_data.device)
                reconstructed = self._decompress_weight()
                mse = F.mse_loss(initial_weight_data, reconstructed)
                print(f"    DEBUG [{self.module_name_for_debug}] ìŠ¤í”Œë¼ì¸ í”¼íŒ… MSE: {mse.item():.6f}")

    def _fit_control_points_to_weight(self, target_weight: torch.Tensor) -> nn.Parameter:
        out_features, in_features = target_weight.shape
        # ì œì–´ì ì„ target_weightì™€ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ì— ìƒì„±
        temp_control_points = nn.Parameter(
            torch.randn(self.k + 1, in_features, device=target_weight.device) * 0.02
        )
        
        optimizer = torch.optim.AdamW([temp_control_points], lr=1e-2, weight_decay=1e-4)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)
        
        progress_bar = tqdm(range(1000), desc=f"í”¼íŒ… {self.module_name_for_debug}", leave=False)
        
        for step in progress_bar:
            optimizer.zero_grad()
            reconstructed = self._interpolate_from_control_points(temp_control_points)
            loss = F.mse_loss(reconstructed, target_weight)
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            if step % 100 == 0 or step == 999:
            progress_bar.set_postfix({'Loss': f'{loss.item():.6f}'})
        
        progress_bar.close()
        return temp_control_points.detach()

    def _interpolate_from_control_points(self, control_points: torch.Tensor) -> torch.Tensor:
        m = self.out_features
        t_values = torch.linspace(0, 1, m, device=control_points.device)
            
            if self.use_geodesic:
            return geodesic_spline_with_reality_stone(control_points, t_values)
            else:
            return catmull_rom_interpolation(control_points, t_values)

    def _decompress_weight(self) -> torch.Tensor:
        # í‰ê°€ ëª¨ë“œì´ê³  ìºì‹œê°€ ìœ íš¨í•˜ë©´ ìºì‹œëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
        if not self.training and hasattr(self, 'cached_weight') and self.cached_weight.numel() > 0:
            return self.cached_weight

        weight = self._interpolate_from_control_points(self.control_points)
        
        # í‰ê°€ ëª¨ë“œì—ì„œëŠ” ê°€ì¤‘ì¹˜ë¥¼ ìºì‹œ
        if not self.training:
            self.cached_weight = weight.detach()
        
        return weight

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass"""
        weight = self._decompress_weight()
        # Conv1D ê°€ì¤‘ì¹˜ í˜•ì‹(out, in, 1)ì— ë§ì¶° unsqueeze í•˜ê³ ,
        # F.linear ëŒ€ì‹  F.conv1dë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ,
        # ì—¬ê¸°ì„œëŠ” SplineLinearTransformì´ Linearì™€ Conv1Dë¥¼ ëª¨ë‘ ëŒ€ì²´í•˜ë¯€ë¡œ
        # F.linearë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜ì ì„.
        # Conv1Dì˜ weightëŠ” (in_channels, out_channels, width)ì§€ë§Œ
        # transformersì˜ Conv1DëŠ” (nf, nx) -> (in_features, out_features) í˜•íƒœì´ë¯€ë¡œ
        # transposeê°€ í•„ìš”í–ˆì—ˆìŒ. SplineLinearTransformì€ (out_features, in_features)ë¥¼ ìƒì„±í•˜ë¯€ë¡œ
        # F.linearì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë¨.
        return F.linear(x, weight, self.bias)

    def train(self, mode: bool = True):
        # train ëª¨ë“œë¡œ ë³€ê²½ ì‹œ ìºì‹œë¥¼ ë¹„ì›Œ ë‹¤ìŒ evalë•Œ ìƒˆë¡œ ìƒì„±í•˜ë„ë¡ í•¨
        if mode:
            self.cached_weight = torch.empty(0, device=self.control_points.device)
        return super().train(mode)

def convert_linear_to_spline(
    model: nn.Module,
    k: int = 50,
    use_geodesic: bool = True
) -> (nn.Module, Dict[str, Any]):
    """
    ëª¨ë¸ì˜ ëª¨ë“  nn.Linearì™€ Conv1D ë ˆì´ì–´ë¥¼ SplineLinearTransformìœ¼ë¡œ ì¬ê·€ ì—†ì´ êµì²´í•©ë‹ˆë‹¤.
    """
    stats_accumulator = {
            'total_original_params': 0,
            'total_compressed_params': 0,
            'num_layers_converted': 0,
            'conversion_details': []
        }

    modules_to_replace = []
    for name, module in model.named_modules():
        is_linear = isinstance(module, nn.Linear)
        is_conv1d = HF_AVAILABLE and Conv1D is not None and isinstance(module, Conv1D)
        
        # í•˜ìœ„ SplineLinearTransform ë ˆì´ì–´ëŠ” ê±´ë„ˆëœ€
        if any(isinstance(parent, SplineLinearTransform) for parent in name.split('.')):
            continue
            
        if is_linear or is_conv1d:
            modules_to_replace.append((name, module))

    for name, module in modules_to_replace:
        current_full_path = name
        
        # ë¶€ëª¨ ëª¨ë“ˆê³¼ í˜„ì¬ ëª¨ë“ˆì˜ ì´ë¦„ì„ ì°¾ìŒ
        path_tokens = name.split('.')
        parent_module = model
        if len(path_tokens) > 1:
            parent_module = model.get_submodule('.'.join(path_tokens[:-1]))
        child_name = path_tokens[-1]

        # íŒŒë¼ë¯¸í„° ë° ê°€ì¤‘ì¹˜ ì¶”ì¶œ
        if isinstance(module, nn.Linear):
            original_params = module.weight.numel() + (module.bias.numel() if module.bias is not None else 0)
            initial_weight_data = module.weight.data
            in_features, out_features = module.in_features, module.out_features
            shape_info = (out_features, in_features)
            layer_type_info = "Linear"
        else: # Conv1D
            original_params = module.weight.numel() + (module.bias.numel() if module.bias is not None else 0)
            # transformers Conv1D ê°€ì¤‘ì¹˜ëŠ” (in_features, out_features) í˜•íƒœì´ë¯€ë¡œ,
            # (out_features, in_features)ë¡œ transpose í•„ìš”
            initial_weight_data = module.weight.data.transpose(0, 1)
            in_features, out_features = module.weight.shape[0], module.weight.shape[1]
            shape_info = (out_features, in_features)
            layer_type_info = "Conv1D"

        # Spline ë ˆì´ì–´ ìƒì„±
            spline_layer = SplineLinearTransform(
            in_features=in_features,
            out_features=out_features,
                k=k,
                use_geodesic=use_geodesic,
                bias=module.bias is not None,
            initial_weight_data=initial_weight_data,
                initial_bias_data=module.bias.data if module.bias is not None else None,
                module_name_for_debug=current_full_path
            )
        
        # ëª¨ë“ˆ êµì²´
        setattr(parent_module, child_name, spline_layer)

        # í†µê³„ ì—…ë°ì´íŠ¸
            compressed_params = spline_layer.control_points.numel()
            if spline_layer.bias is not None:
                compressed_params += spline_layer.bias.numel()
        stats_accumulator['total_original_params'] += original_params
        stats_accumulator['total_compressed_params'] += compressed_params
        stats_accumulator['num_layers_converted'] += 1
            
        compression_ratio = compressed_params / original_params if original_params > 0 else 0
        stats_accumulator['conversion_details'].append({
                'layer_name': current_full_path,
                'original_params': original_params,
                'compressed_params': compressed_params,
                'compression_ratio': compression_ratio,
            'shape': shape_info
            })
        
        print(f"  âœ… {current_full_path} ({layer_type_info}): {shape_info[0]}Ã—{shape_info[1]} â†’ {k+1} ì œì–´ì  (ì••ì¶•ë¥ : {compression_ratio:.3f})")

        print(f"\nğŸ“Š ìŠ¤í”Œë¼ì¸ ì••ì¶• ì™„ë£Œ:")
    print(f"  ë³€í™˜ëœ ë ˆì´ì–´ ìˆ˜: {stats_accumulator['num_layers_converted']}")
    print(f"  ì›ë³¸ íŒŒë¼ë¯¸í„°: {stats_accumulator['total_original_params']:,}")
    print(f"  ì••ì¶• íŒŒë¼ë¯¸í„°: {stats_accumulator['total_compressed_params']:,}")
    if stats_accumulator['total_original_params'] > 0:
        overall_compression = stats_accumulator['total_compressed_params'] / stats_accumulator['total_original_params']
        print(f"  ì „ì²´ ì••ì¶•ë¥ : {overall_compression:.3f} ({overall_compression*100:.1f}%)")
        print(f"  ë©”ëª¨ë¦¬ ì ˆì•½: {(1-overall_compression)*100:.1f}%")
    
    return model, stats_accumulator

def get_model_size_mb(model, count_buffers=True):
    param_size = 0
    buffer_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    if count_buffers:
        for buffer in model.buffers():
            if buffer is not None:
                buffer_size += buffer.nelement() * buffer.element_size()
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb

@torch.no_grad()
def evaluate_perplexity(model, tokenizer, device, num_samples=100):
    """ëª¨ë¸ì˜ Perplexityë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
    model.eval()
    try:
        # í‰ê°€ìš© ë°ì´í„°ì…‹ ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¹ ë¥´ê²Œ)
        dataset = load_dataset("wikipedia", "20220301.ko", split="train", streaming=True)
        dataset = dataset.take(num_samples)
        texts = [example['text'] for example in dataset if len(example['text']) > 50]
    except Exception as e:
        print(f"Perplexity í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return float('inf')

    total_loss = 0
    total_tokens = 0
    
    print(f"\nğŸ¤” Perplexity í‰ê°€ ì¤‘ ({len(texts)}ê°œ ìƒ˜í”Œ)...")
    for text in tqdm(texts, desc="Perplexity ê³„ì‚°", leave=False):
        inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True).to(device)
        input_ids = inputs.input_ids
        
        if input_ids.size(1) < 2:
            continue

        outputs = model(input_ids, labels=input_ids)
        loss = outputs.loss * (input_ids.size(1) - 1) # í† í° ìˆ˜ ë§Œí¼ loss ê³±í•˜ê¸°
        
        total_loss += loss.item()
        total_tokens += (input_ids.size(1) - 1)

    if total_tokens == 0:
        return float('inf')
        
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss)).item()
    print(f"  - í‰ê·  Loss: {avg_loss:.4f}")
    print(f"  - Perplexity: {perplexity:.4f}")
    return perplexity

def demo_spline_compression():
    if not HF_AVAILABLE:
        print("\nâš ï¸ ì´ ë°ëª¨ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ 'transformers'ì™€ 'tokenizers' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("pip install transformers tokenizers")
        return
    print(f"Reality Stone ì‚¬ìš© ê°€ëŠ¥: {RS_AVAILABLE}")
    model_name = "skt/kogpt2-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    original_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    original_model.eval()
    original_params = sum(p.numel() for p in original_model.parameters())
    original_size_mb = get_model_size_mb(original_model)
    
    # ì›ë³¸ ëª¨ë¸ Perplexity í‰ê°€
    original_perplexity = evaluate_perplexity(original_model, tokenizer, device)
    
    print(f"\nğŸ“Š ì›ë³¸ ëª¨ë¸ ({model_name}):")
    print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {original_params:,}")
    print(f"  ëª¨ë¸ í¬ê¸°: {original_size_mb:.2f} MB")
    
    prompt = "ì¸ê³µì§€ëŠ¥ì´ ì„¸ìƒì„ ì§€ë°°í•˜ëŠ” ì‹œëŒ€,"
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    with torch.no_grad():
        original_output_ids = original_model.generate(
            input_ids, max_length=50, num_return_sequences=1,
            pad_token_id=tokenizer.pad_token_id
        )
    original_text = tokenizer.decode(original_output_ids[0], skip_special_tokens=True)
    print("\nğŸ“ ì›ë³¸ ëª¨ë¸ ìƒì„± í…ìŠ¤íŠ¸:")
    print(original_text)
    print(f"  - Perplexity: {original_perplexity:.4f}")
    k_values = [10, 15, 20, 25] 
    results_summary = []

    for k in k_values:
        print(f"\n" + "="*60)
        print(f"ğŸ”§ k={k}ë¡œ ìŠ¤í”Œë¼ì¸ ì••ì¶• + íŒŒì¸íŠœë‹ ì‹¤í—˜")
        test_model = AutoModelForCausalLM.from_pretrained(model_name)
        test_model.load_state_dict(original_model.state_dict())
        test_model.to(device)
        print(f"\n1ï¸âƒ£ ìŠ¤í”Œë¼ì¸ ì••ì¶• ì ìš© ì¤‘ (k={k})...")
        compressed_model, stats = convert_linear_to_spline(
            test_model,
            k=k,
            use_geodesic=RS_AVAILABLE
        )
        compressed_model.eval()
        compressed_perplexity = evaluate_perplexity(compressed_model, tokenizer, device)

        with torch.no_grad():
            try:
                compressed_output_ids = compressed_model.generate(
                    input_ids, max_length=50, num_return_sequences=1,
                    pad_token_id=tokenizer.pad_token_id
                )
                compressed_text_before = tokenizer.decode(compressed_output_ids[0], skip_special_tokens=True)
            except Exception as e:
                compressed_text_before = f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"
        print("\nğŸ“ ì••ì¶• ì§í›„ ìƒì„± í…ìŠ¤íŠ¸:")
        print(compressed_text_before)
        print(f"  - Perplexity: {compressed_perplexity:.4f}")

        print(f"\n2ï¸âƒ£ ì••ì¶• ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘...")
        finetuned_model = finetune_compressed_model(
            compressed_model, 
            tokenizer, 
            device=device,
            num_steps = 5000,  
            learning_rate=5e-5
        )
        finetuned_model.eval()
        finetuned_perplexity = evaluate_perplexity(finetuned_model, tokenizer, device)
        with torch.no_grad():
            try:
                finetuned_output_ids = finetuned_model.generate(
                    input_ids, max_length=50, num_return_sequences=1,
                    pad_token_id=tokenizer.pad_token_id
                )
                finetuned_text = tokenizer.decode(finetuned_output_ids[0], skip_special_tokens=True)
            except Exception as e:
                finetuned_text = f"í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"
        total_compressed_params = sum(p.numel() for p in finetuned_model.parameters())
        compressed_size_mb = get_model_size_mb(finetuned_model, count_buffers=False)
        full_compressed_size_mb = get_model_size_mb(finetuned_model, count_buffers=True)
        compression_ratio = total_compressed_params / original_params
        print(f"\nğŸ“ˆ k={k} ìµœì¢… ê²°ê³¼:")
        print(f"  ì••ì¶• ëª¨ë¸ ì´ íŒŒë¼ë¯¸í„°: {total_compressed_params:,} (ì›ë³¸ì˜ {compression_ratio*100:.2f}%)")
        print(f"    - ë³€í™˜ëœ ë ˆì´ì–´: {stats['total_compressed_params']:,}ê°œ")
        print(f"    - ê³ ì • ë ˆì´ì–´ (ì„ë² ë”© ë“±): {total_compressed_params - stats['total_compressed_params']:,}ê°œ")
        print(f"  ì••ì¶• ëª¨ë¸ í¬ê¸° (ì €ì¥ ì‹œ): {compressed_size_mb:.2f} MB (ì›ë³¸ ëŒ€ë¹„ {100 - (compressed_size_mb / original_size_mb * 100):.1f}% ê°ì†Œ)")
        print(f"  ì••ì¶• ëª¨ë¸ í¬ê¸° (ë²„í¼ í¬í•¨): {full_compressed_size_mb:.2f} MB (ì›ë³¸ ëŒ€ë¹„ {100 - (full_compressed_size_mb / original_size_mb * 100):.1f}% ê°ì†Œ)")
        print("\nğŸ“ íŒŒì¸íŠœë‹ í›„ ìƒì„± í…ìŠ¤íŠ¸:")
        print(finetuned_text)
        print(f"  - Perplexity: {finetuned_perplexity:.4f}")
        print(f"\nğŸ“Š í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¹„êµ (k={k}):")
        print(f"  ì›ë³¸ (PPL: {original_perplexity:.2f}):".ljust(25) + f"{original_text}")
        print(f"  ì••ì¶• ì§í›„ (PPL: {compressed_perplexity:.2f}):".ljust(25) + f"{compressed_text_before}")
        print(f"  íŒŒì¸íŠœë‹ í›„ (PPL: {finetuned_perplexity:.2f}):".ljust(25) + f"{finetuned_text}")
        
        results_summary.append({
            'k': k,
            'compression_ratio': compression_ratio,
            'compressed_ppl': compressed_perplexity,
            'finetuned_ppl': finetuned_perplexity
        })
    
    print("\n" + "="*60)
    print("ğŸ“ˆ ìµœì¢… ìš”ì•½")
    print("="*60)
    print(f"ì›ë³¸ ëª¨ë¸ Perplexity: {original_perplexity:.4f}")
    print("-" * 60)
    print(f"{'k':<5} | {'ì••ì¶•ë¥ ':<10} | {'ì••ì¶• í›„ PPL':<15} | {'íŒŒì¸íŠœë‹ í›„ PPL':<15}")
    print("-" * 60)
    for res in results_summary:
        print(f"{res['k']:<5} | {res['compression_ratio']:.3f}      | {res['compressed_ppl']:<15.2f} | {res['finetuned_ppl']:<15.2f}")
    print("-" * 60)
        
    return finetuned_model

def prepare_korean_dataset(tokenizer, max_length=512, num_samples=10000):
    """í•œêµ­ì–´ ë°ì´í„°ì…‹ ì¤€ë¹„"""
    print("ğŸ“š í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    try:
        dataset = load_dataset("wikipedia", "20220301.ko", split="train", streaming=True)
        texts = []
        for i, example in enumerate(dataset):
            if i >= num_samples:
                break
            text = example["text"]
            if len(text.strip()) > 50:  
                texts.append(text.strip())
        print(f"  ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸: {len(texts)}ê°œ")
        
    except Exception as e:
        print(f"  ìœ„í‚¤í”¼ë””ì•„ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e
    print("  í† í¬ë‚˜ì´ì§• ì¤‘...")
    tokenized = tokenizer(
        texts,
        truncation=True,
        padding=False,
        max_length=max_length,
        return_tensors="pt"
    )
    class SimpleDataset:
        def __init__(self, input_ids):
            self.input_ids = input_ids
        
        def __len__(self):
            return len(self.input_ids)
        
        def __getitem__(self, idx):
            return {"input_ids": self.input_ids[idx]}
    
    dataset = SimpleDataset(tokenized["input_ids"])
    print(f"  ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(dataset)}ê°œ ìƒ˜í”Œ")
    
    return dataset

def finetune_compressed_model(model, tokenizer, device, num_steps=50000, learning_rate=1e-4):
    """ì••ì¶•ëœ ëª¨ë¸ íŒŒì¸íŠœë‹"""
    print(f"ğŸ”§ ì••ì¶• ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œì‘ ({num_steps:,} ìŠ¤í…)")
    # ëª¨ë¸ì„ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    model.to(device)
    model.train()

    train_dataset = prepare_korean_dataset(tokenizer, num_samples=5000)
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
        return_tensors="pt"
    )
    training_args = TrainingArguments(
        output_dir="./spline_finetuned",
        overwrite_output_dir=True,
        max_steps=num_steps,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=2,
        learning_rate=learning_rate,
        warmup_steps=1000,
        logging_steps=1000,
        save_steps=10000,
        evaluation_strategy="no",
        save_total_limit=2,
        prediction_loss_only=True,
        remove_unused_columns=False,
        dataloader_drop_last=True,
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
    )
    print("  íŒŒì¸íŠœë‹ ì‹œì‘...")
    trainer.train()
    print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    return model

if __name__ == "__main__":
    demo_spline_compression() 