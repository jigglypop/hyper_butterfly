import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import copy
from tqdm import tqdm
import reality_stone as rs
import gc

def precise_memory_measure(device, label=""):
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        gc.collect()
        torch.cuda.synchronize()
        memory_mb = torch.cuda.memory_allocated() / 1024**2
        print(f"{label}: {memory_mb:.1f} MB")
        return memory_mb
    return 0.0

class MinimalPoincareBallLinear(nn.Module):
    """ìµœì†Œí•œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš© PoincarÃ© Ball ë ˆì´ì–´"""
    def __init__(self, in_features: int, out_features: int, curvature: float = 1.0, bias: bool = True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.curvature = curvature
        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)
            
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.weight.device != x.device:
            self.weight.data = self.weight.data.to(x.device)
            if self.bias is not None:
                self.bias.data = self.bias.data.to(x.device)
        
        # ê¸°ë³¸ ì„ í˜• ì—°ì‚°ë§Œ (hyperbolic ì—°ì‚° ì œê±°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
        return F.linear(x, self.weight, self.bias)

def replace_with_minimal_layers(model: nn.Module, curvature: float = 1.0):
    """ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”ë¥¼ ìœ„í•´ ê¸°ë³¸ ë ˆì´ì–´ë¡œë§Œ êµì²´"""
    total_replaced = 0
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
    device = next(model.parameters()).device
    start_memory = precise_memory_measure(device, "êµì²´ ì‹œì‘")
    
    for name, module in model.named_modules():
        if hasattr(module, 'c_attn') and hasattr(module.c_attn, 'weight'):
            old_layer = module.c_attn
            if hasattr(old_layer, 'nf'):  # GPT2Conv1D
                in_features = old_layer.weight.shape[0]
                out_features = old_layer.weight.shape[1]
                new_layer = MinimalPoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data.t())
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            else:  # nn.Linear
                out_features, in_features = old_layer.weight.shape
                new_layer = MinimalPoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data)
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ êµì²´
            del module.c_attn
            torch.cuda.empty_cache()
            module.c_attn = new_layer
            total_replaced += 1
            
        if hasattr(module, 'c_proj') and hasattr(module.c_proj, 'weight'):
            old_layer = module.c_proj
            if hasattr(old_layer, 'nf'):
                in_features = old_layer.weight.shape[0]
                out_features = old_layer.weight.shape[1]
                new_layer = MinimalPoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data.t())
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            else:
                out_features, in_features = old_layer.weight.shape
                new_layer = MinimalPoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data)
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            
            del module.c_proj
            torch.cuda.empty_cache()
            module.c_proj = new_layer
            total_replaced += 1
            
        if hasattr(module, 'c_fc') and hasattr(module.c_fc, 'weight'):
            old_layer = module.c_fc
            if hasattr(old_layer, 'nf'):
                in_features = old_layer.weight.shape[0]
                out_features = old_layer.weight.shape[1]
                new_layer = MinimalPoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data.t())
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            else:
                out_features, in_features = old_layer.weight.shape
                new_layer = MinimalPoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data)
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            
            del module.c_fc
            torch.cuda.empty_cache()
            module.c_fc = new_layer
            total_replaced += 1
    
    end_memory = precise_memory_measure(device, "êµì²´ ì™„ë£Œ")
    print(f"ì´ {total_replaced}ê°œ ë ˆì´ì–´ êµì²´ (ë©”ëª¨ë¦¬ ë³€í™”: {end_memory - start_memory:+.1f} MB)")
    return model

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_name = "skt/kogpt2-base-v2"
    
    print("ì •ë°€ ë©”ëª¨ë¦¬ ì¸¡ì • í…ŒìŠ¤íŠ¸")
    
    # 1ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    initial_memory = precise_memory_measure(device, "ì´ˆê¸° ìƒíƒœ")
    
    # 2ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ
    print("\n=== ëª¨ë¸ ë¡œë“œ ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ===")
    teacher = AutoModelForCausalLM.from_pretrained(model_name)
    after_load_memory = precise_memory_measure(device, "ëª¨ë¸ ë¡œë“œ í›„ (CPU)")
    
    teacher = teacher.to(device)
    after_gpu_memory = precise_memory_measure(device, "GPU ì´ë™ í›„")
    
    # 3ë‹¨ê³„: deepcopy í…ŒìŠ¤íŠ¸
    print("\n=== deepcopy ë©”ëª¨ë¦¬ ì˜í–¥ ===")
    student = copy.deepcopy(teacher)
    after_copy_memory = precise_memory_measure(device, "deepcopy í›„")
    copy_overhead = after_copy_memory - after_gpu_memory
    print(f"deepcopy ì˜¤ë²„í—¤ë“œ: {copy_overhead:.1f} MB ({copy_overhead/after_gpu_memory*100:.1f}%)")
    
    # 4ë‹¨ê³„: ë ˆì´ì–´ êµì²´
    print("\n=== ë ˆì´ì–´ êµì²´ ë©”ëª¨ë¦¬ ì˜í–¥ ===")
    student = replace_with_minimal_layers(student, 1.0)
    after_replace_memory = precise_memory_measure(device, "ë ˆì´ì–´ êµì²´ í›„")
    
    # 5ë‹¨ê³„: ì›ë³¸ ëª¨ë¸ ì‚­ì œ í…ŒìŠ¤íŠ¸
    print("\n=== ëª¨ë¸ ì‚­ì œ ë©”ëª¨ë¦¬ ì˜í–¥ ===")
    del teacher
    torch.cuda.empty_cache()
    gc.collect()
    after_teacher_del_memory = precise_memory_measure(device, "teacher ì‚­ì œ í›„")
    
    # 6ë‹¨ê³„: ìµœì¢… ë©”ëª¨ë¦¬ ë¹„ìœ¨
    print("\n=== ìµœì¢… ë©”ëª¨ë¦¬ ë¶„ì„ ===")
    final_ratio = after_teacher_del_memory / after_gpu_memory
    print(f"ì›ë³¸ ëª¨ë¸ ë©”ëª¨ë¦¬: {after_gpu_memory:.1f} MB")
    print(f"ìµœì¢… student ë©”ëª¨ë¦¬: {after_teacher_del_memory:.1f} MB")
    print(f"ì‹¤ì œ ë©”ëª¨ë¦¬ ë¹„ìœ¨: {final_ratio:.3f}")
    
    if final_ratio < 1.2:
        print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì„±ê³µ!")
    elif final_ratio < 1.5:
        print("ğŸŸ¡ ë¶€ë¶„ì  ë©”ëª¨ë¦¬ ìµœì í™”")
    else:
        print("âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨")
    
    # 7ë‹¨ê³„: ê°„ë‹¨í•œ ë™ì‘ í…ŒìŠ¤íŠ¸
    print("\n=== ë™ì‘ í…ŒìŠ¤íŠ¸ ===")
    test_input = tokenizer("ì•ˆë…•í•˜ì„¸ìš”", return_tensors="pt").to(device)
    with torch.no_grad():
        output = student.generate(**test_input, max_length=20, do_sample=False, pad_token_id=tokenizer.eos_token_id)
    result = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"í…ŒìŠ¤íŠ¸ ì¶œë ¥: {result}")
    
    final_memory = precise_memory_measure(device, "ìµœì¢… ìƒíƒœ")
    print(f"\n=== ë©”ëª¨ë¦¬ ë³€í™” ìš”ì•½ ===")
    print(f"ì´ˆê¸°: {initial_memory:.1f} MB")
    print(f"ì›ë³¸ ë¡œë“œ: {after_gpu_memory:.1f} MB (+{after_gpu_memory-initial_memory:.1f})")
    print(f"ë³µì‚¬ í›„: {after_copy_memory:.1f} MB (+{copy_overhead:.1f})")
    print(f"êµì²´ í›„: {after_replace_memory:.1f} MB (+{after_replace_memory-after_copy_memory:.1f})")
    print(f"ì‚­ì œ í›„: {after_teacher_del_memory:.1f} MB ({final_ratio:.3f}ë°°)")

if __name__ == "__main__":
    main() 