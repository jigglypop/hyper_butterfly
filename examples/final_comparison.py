import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import copy
from tqdm import tqdm
try:
    import reality_stone as rs
    from reality_stone.layers import RBELinear
    RS_AVAILABLE = True
except ImportError:
    RS_AVAILABLE = False
    # Define a placeholder class if reality_stone is not available
    class RBELinear(nn.Module):
        def __init__(self, in_features, out_features, **kwargs):
            super().__init__()
            self.linear = nn.Linear(in_features, out_features)
        def forward(self, x):
            return self.linear(x)
        @classmethod
        def from_linear(cls, linear, **kwargs):
            return cls(linear.in_features, linear.out_features)

# --- í—¬ê°€ì†-RBE í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶•ê¸° ---
class HelgasonRBECompressor:
    """
    í—¬ê°€ì†-í‘¸ë¦¬ì— ë³€í™˜ìœ¼ë¡œ ì£¼ìš” íŠ¹ì§•ì„ ì¡ê³ ,
    ì”ì°¨ë¥¼ RBEë¡œ ì •ë°€í•˜ê²Œ ì••ì¶•í•˜ëŠ” ê·¹í•œ ì••ì¶•ê¸°
    """
    def __init__(self, W: torch.Tensor, compression_ratio=0.1):
        self.shape = W.shape
        self.dtype = W.dtype
        self.device = W.device

        print(f"    ğŸŒ€ í—¬ê°€ì†-ë¹„íŠ¸í•„ë“œ ì••ì¶• ì‹œì‘: {self.shape}, ëª©í‘œì••ì¶•ë¥ ={compression_ratio:.1%}")

        # 1. í—¬ê°€ì†-í‘¸ë¦¬ì— ë³€í™˜ìœ¼ë¡œ ë§¤í¬ë¡œ êµ¬ì¡° ì••ì¶•
        W_fft = torch.fft.fft2(W.float())
        energy = torch.abs(W_fft)**2
        sorted_indices = torch.argsort(energy.flatten(), descending=True)
        
        # ì—ë„ˆì§€ ê¸°ë°˜ìœ¼ë¡œ ì£¼ìš” ì£¼íŒŒìˆ˜ ì„ íƒ (ë§¤í¬ë¡œ ì •ë³´)
        macro_budget = int(W.numel() * compression_ratio * 0.5) # ì˜ˆì‚°ì˜ 50%
        important_indices = sorted_indices[:macro_budget]
        
        freq_mask = torch.zeros_like(energy.flatten(), dtype=torch.bool)
        freq_mask[important_indices] = True
        self.freq_mask = freq_mask.reshape(W_fft.shape)
        
        self.important_freqs = torch.where(self.freq_mask, W_fft, torch.zeros_like(W_fft))

        # 2. ì”ì°¨ ê³„ì‚° (ë§ˆì´í¬ë¡œ ì •ë³´)
        macro_reconstructed = torch.fft.ifft2(self.important_freqs).real.to(self.dtype)
        residual = W - macro_reconstructed
        print(f"       - 1ë‹¨ê³„(í—¬ê°€ì†): ì”ì°¨ì—ë„ˆì§€ = {torch.norm(residual) / torch.norm(W):.2%}")

        # 3. ì”ì°¨ë¥¼ Bitfieldë¡œ 2ì°¨ ì••ì¶•
        if RS_AVAILABLE:
            # BitfieldLinear.from_linearëŠ” nn.Linear ê°ì²´ë¥¼ ì¸ìë¡œ ë°›ìŒ
            residual_linear_layer = nn.Linear(self.shape[1], self.shape[0], bias=False)
            residual_linear_layer.weight.data = residual
            residual_linear_layer.to(self.device)
            self.residual_bitfield = BitfieldLinear.from_linear(residual_linear_layer, r_max=0.5)
        else: # Fallback
            self.residual_bitfield = residual

    def reconstruct(self) -> torch.Tensor:
        """ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ë³µì› (ë””ë²„ê¹…ìš©, í˜„ì¬ decompress ë¯¸ì§€ì›)"""
        macro_reconstructed = torch.fft.ifft2(self.important_freqs).real.to(self.dtype)
        
        if RS_AVAILABLE:
            # Bitfield ë³µì›ì€ í˜„ì¬ ë¶ˆê°€. 
            # decompress() ë©”ì†Œë“œë¥¼ BitfieldLinearì— ì¶”ê°€í•´ì•¼ ì™„ì „í•œ ë³µì›ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
            print("âš ï¸ Bitfield decompressëŠ” ë¯¸êµ¬í˜„ ìƒíƒœì…ë‹ˆë‹¤. ë§¤í¬ë¡œ ë¶€ë¶„ë§Œ ë³µì›í•©ë‹ˆë‹¤.")
            residual_reconstructed = torch.zeros_like(macro_reconstructed)
        else:
            residual_reconstructed = self.residual_bitfield

        return macro_reconstructed + residual_reconstructed

    def apply(self, x: torch.Tensor) -> torch.Tensor:
        """ì••ì¶•ëœ ì—°ì‚° ì ìš©"""
        # 1. Macro (FFT) ë¶€ë¶„ ì ìš©
        macro_reconstructed = torch.fft.ifft2(self.important_freqs).real.to(x.dtype)
        macro_output = F.linear(x, macro_reconstructed)

        # 2. Micro (Bitfield) ë¶€ë¶„ ì ìš©
        if RS_AVAILABLE:
            residual_output = self.residual_bitfield(x)
        else:
            residual_output = F.linear(x, self.residual_bitfield)

        return macro_output + residual_output

class HybridCompressedLinear(nn.Module):
    """í—¬ê°€ì†-ë¹„íŠ¸í•„ë“œ ì••ì¶•ì„ ì ìš©í•œ ìµœì¢… ì„ í˜• ë ˆì´ì–´"""
    def __init__(self, linear_layer: nn.Linear, compression_ratio=0.1, is_attn=False):
        super().__init__()
        
        self.is_attn = is_attn
        self.in_features = linear_layer.in_features
        self.out_features = linear_layer.out_features
        
        if self.is_attn:
            # c_attn ê°€ì¤‘ì¹˜ëŠ” [out*3, in] ëª¨ì–‘
            weights = linear_layer.weight.data
            w_q, w_k, w_v = torch.chunk(weights, 3, dim=0)
            self.compressor_q = HelgasonBitfieldCompressor(w_q, compression_ratio)
            self.compressor_k = HelgasonBitfieldCompressor(w_k, compression_ratio)
            self.compressor_v = HelgasonBitfieldCompressor(w_v, compression_ratio)
        else:
            self.compressor = HelgasonBitfieldCompressor(linear_layer.weight.data, compression_ratio)
        
        if linear_layer.bias is not None:
            self.bias = nn.Parameter(linear_layer.bias.data.clone())
        else:
            self.register_parameter('bias', None)

    def forward(self, x):
        if self.is_attn:
            # Q, K, V ê°ê°ì— ëŒ€í•´ ì••ì¶•ëœ ì—°ì‚° ìˆ˜í–‰ í›„ ê²°í•©
            q = self.compressor_q.apply(x)
            k = self.compressor_k.apply(x)
            v = self.compressor_v.apply(x)
            # transformersì˜ c_attn ì¶œë ¥ê³¼ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ í•©ì¹¨
            output = torch.cat([q, k, v], dim=-1)
        else:
            output = self.compressor.apply(x)
            
        if self.bias is not None:
            output += self.bias
        return output

def apply_hybrid_compression_to_model(model, compression_ratio=0.1):
    """ëª¨ë¸ì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• ë ˆì´ì–´ë¡œ êµì²´"""
    print(f"\nğŸŒ€ ëª¨ë¸ ì „ì²´ í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• ì‹œì‘ (ëª©í‘œ: {compression_ratio:.1%})")
    
    layers_to_replace = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) or type(module).__name__ == 'Conv1D':
            layers_to_replace.append(name)
            
    for name in tqdm(layers_to_replace, desc="ì••ì¶• ì§„í–‰"):
        module = model.get_submodule(name)
        if '.' not in name:
            continue

        parent_name, child_name = name.rsplit('.', 1)
        parent_module = model.get_submodule(parent_name)

        is_attn = 'c_attn' in name
        
        # ê°€ì¤‘ì¹˜ ëª¨ì–‘ì„ [out_features, in_features]ë¡œ í†µì¼
        if type(module).__name__ == 'Conv1D':
            # Conv1Dì˜ ê°€ì¤‘ì¹˜ëŠ” [in, out] -> t() -> [out, in]
            out_features, in_features = module.weight.shape
            linear_equiv = nn.Linear(in_features, out_features, bias=(module.bias is not None))
            linear_equiv.weight.data = module.weight.data.t()
            if module.bias is not None:
                linear_equiv.bias.data = module.bias.data
        else: # nn.Linear
            linear_equiv = module
            
        new_layer = HybridCompressedLinear(linear_equiv, compression_ratio, is_attn=is_attn)
        setattr(parent_module, child_name, new_layer)
        
    return model

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_name = "skt/kogpt2-base-v2"
    
    if not RS_AVAILABLE:
        print("="*60)
        print("âš ï¸ ê²½ê³ : RealityStone ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ì••ì¶• ê¸°ëŠ¥ ì—†ì´ í´ë°± ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        print("="*60)

    print("RealityStone í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• ìµœì¢… ë¹„êµ í…ŒìŠ¤íŠ¸")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # use_safetensors=Trueë¥¼ ì¶”ê°€í•˜ì—¬ ë³´ì•ˆ ë° ë¡œë”© ë¬¸ì œ í•´ê²°
    teacher = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True).to(device)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    prompts = ["ì•ˆë…•í•˜ì„¸ìš”", "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”", "í•œêµ­ì˜ ìˆ˜ë„ëŠ”", "ì¸ê³µì§€ëŠ¥ì´ë€", "ë§›ìˆëŠ” ìŒì‹ì€"]
    
    # ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    teacher_copy = copy.deepcopy(teacher)
    orig_results, orig_time = test_model(teacher_copy, tokenizer, device, prompts, "ì›ë³¸")
    del teacher_copy

    # í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸
    student = copy.deepcopy(teacher)
    student = apply_hybrid_compression_to_model(student, compression_ratio=0.1) # 10% ì••ì¶•ë¥  ëª©í‘œ
    
    # TODO: ì§€ì‹ ì¦ë¥˜ íŒŒì¸íŠœë‹ ì¶”ê°€
    
    comp_results, comp_time = test_model(student, tokenizer, device, prompts, "í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶•")

    # ê²°ê³¼ ë¹„êµ
    print("\n" + "="*60 + "\nì„±ëŠ¥ ë¹„êµ ê²°ê³¼\n" + "="*60)
    
    speed_ratio = comp_time / orig_time if orig_time > 0 else 0
    print(f"ì†ë„ ë¹„ìœ¨: {speed_ratio:.3f} (ì›ë³¸ ëŒ€ë¹„)")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ë‹¨ìˆœ íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ)
    orig_params = sum(p.numel() for p in teacher.parameters())
    comp_params = sum(p.numel() for p in student.parameters())
    mem_ratio = comp_params / orig_params if orig_params > 0 else 0
    print(f"íŒŒë¼ë¯¸í„° ë¹„ìœ¨: {mem_ratio:.3f} ({1/mem_ratio if mem_ratio > 0 else 0:.1f}x ì••ì¶•)")

    exact_output_matches = 0
    for i, (o, p) in enumerate(zip(orig_results, comp_results), 1):
        if o[1] == p[1]:
            print(f"[{i}] ì¶œë ¥ ì¼ì¹˜")
            exact_output_matches += 1
        else:
            print(f"[{i}] ì¶œë ¥ ë¶ˆì¼ì¹˜\n    ì›ë³¸: {o[1]}\n    ì••ì¶•: {p[1]}")
    
    output_match_rate = exact_output_matches / len(prompts)
    print(f"ì¶œë ¥ ì¼ì¹˜ìœ¨: {output_match_rate:.1%}")

def test_model(model, tokenizer, device, prompts, model_name, max_length=50):
    model.to(device).eval()
    results = []
    total_time = 0.0
    
    print(f"\n=== {model_name} í…ŒìŠ¤íŠ¸ ===")
    for idx, prompt in enumerate(prompts, 1):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        start = time.time()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=max_length, do_sample=False, pad_token_id=tokenizer.eos_token_id)
        elapsed = time.time() - start
        gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        total_time += elapsed
        print(f"[{idx}] '{prompt}' -> {gen_text} ({elapsed:.3f}s)")
        results.append((prompt, gen_text, elapsed))
    
    avg_time = total_time / len(prompts) if prompts else 0
    print(f"{model_name} í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ")
    return results, avg_time

if __name__ == "__main__":
    main() 