"""
Reality Stone ìµœì í™” ì••ì¶•
ì‹¤ì œ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì••ì¶• ê¸°ìˆ 

ëª©í‘œ: 40%+ ì••ì¶• + í’ˆì§ˆ ìœ ì§€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
import warnings
warnings.filterwarnings("ignore")


class OptimizedHybridLayer(nn.Module):
    """ìµœì í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• ë ˆì´ì–´"""
    
    def __init__(self, mlp_layers, layer_indices):
        super().__init__()
        
        self.layer_indices = layer_indices
        num_layers = len(mlp_layers)
        
        print(f"\nğŸ”§ Optimized Hybrid Compression")
        print(f"   ë ˆì´ì–´: {layer_indices} ({num_layers}ê°œ ìœµí•©)")
        
        # ë ˆì´ì–´ë³„ ê°€ì¤‘ì¹˜ ìˆ˜ì§‘
        c_fc_weights = torch.stack([mlp.c_fc.weight.data for mlp in mlp_layers])
        c_proj_weights = torch.stack([mlp.c_proj.weight.data for mlp in mlp_layers])
        
        # 1. ìŠ¤ë§ˆíŠ¸ ë ˆì´ì–´ ìœµí•©
        print("   ğŸ“Š ìŠ¤ë§ˆíŠ¸ ë ˆì´ì–´ ìœµí•©...")
        c_fc_fused = self._smart_fusion(c_fc_weights)
        c_proj_fused = self._smart_fusion(c_proj_weights)
        
        # 2. ìµœì í™”ëœ SVD
        print("   ğŸ“Š ìµœì í™” SVD ì••ì¶•...")
        target_compression = 0.6 if num_layers >= 4 else 0.8
        
        self.c_fc_components = self._optimized_svd(c_fc_fused, target_compression, "c_fc")
        self.c_proj_components = self._optimized_svd(c_proj_fused, target_compression, "c_proj")
        
        # ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
        if mlp_layers[0].c_fc.bias is not None:
            biases = torch.stack([mlp.c_fc.bias.data for mlp in mlp_layers])
            self.c_fc_bias = nn.Parameter(torch.mean(biases, dim=0))
        else:
            self.register_parameter('c_fc_bias', None)
            
        if mlp_layers[0].c_proj.bias is not None:
            biases = torch.stack([mlp.c_proj.bias.data for mlp in mlp_layers])  
            self.c_proj_bias = nn.Parameter(torch.mean(biases, dim=0))
        else:
            self.register_parameter('c_proj_bias', None)
        
        self.activation = nn.GELU()
        
        # í†µê³„
        self._print_stats(mlp_layers)
    
    def _smart_fusion(self, weight_stack):
        """ìŠ¤ë§ˆíŠ¸ ë ˆì´ì–´ ìœµí•© - ì¤‘ìš” ì •ë³´ ë³´ì¡´"""
        # íŠ¹ì´ê°’ ë¶„í•´ë¡œ ê° ë ˆì´ì–´ì˜ ì¤‘ìš” ì„±ë¶„ ì¶”ì¶œ
        svd_components = []
        
        for i in range(weight_stack.shape[0]):
            U, S, V = torch.svd(weight_stack[i])
            # ìƒìœ„ 90% ì—ë„ˆì§€ ë³´ì¡´
            energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
            k = min(torch.sum(energy < 0.9).item() + 1, S.shape[0])
            svd_components.append((U[:, :k], S[:k], V[:, :k]))
        
        # ê°€ì¤‘ ì¬êµ¬ì„±
        fused = torch.zeros_like(weight_stack[0])
        total_energy = sum(torch.sum(s**2).item() for _, s, _ in svd_components)
        
        for u, s, v in svd_components:
            weight = torch.sum(s**2).item() / total_energy
            fused += weight * torch.mm(u * s.unsqueeze(0), v.T)
        
        return fused
    
    def _optimized_svd(self, weight, target_ratio, name):
        """ìµœì í™”ëœ SVD - í’ˆì§ˆ ìš°ì„ """
        U, S, V = torch.svd(weight)
        
        # ì—ë„ˆì§€ ë³´ì¡´ ê¸°ë°˜
        energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
        
        # ëª©í‘œ: 95% ì´ìƒ ì—ë„ˆì§€ ë³´ì¡´
        energy_threshold = 0.95
        rank = torch.sum(energy < energy_threshold).item() + 1
        
        # ì••ì¶•ë¥  ì œì•½
        max_rank = int(min(weight.shape) * target_ratio)
        rank = min(rank, max_rank)
        
        # ìµœì†Œ rank ë³´ì¥
        min_rank = max(int(min(weight.shape) * 0.1), 50)
        rank = max(rank, min_rank)
        
        print(f"      {name}: {min(weight.shape)} â†’ {rank} (ì—ë„ˆì§€: {energy[rank-1]:.3f})")
        
        # ì••ì¶• ì»´í¬ë„ŒíŠ¸ ë°˜í™˜
        return {
            'U': nn.Parameter(U[:, :rank]),
            'S': nn.Parameter(S[:rank]),
            'V': nn.Parameter(V[:, :rank])
        }
    
    def _print_stats(self, mlp_layers):
        """ì••ì¶• í†µê³„ ì¶œë ¥"""
        original = sum(
            mlp.c_fc.weight.numel() + mlp.c_proj.weight.numel() +
            (mlp.c_fc.bias.numel() if mlp.c_fc.bias is not None else 0) +
            (mlp.c_proj.bias.numel() if mlp.c_proj.bias is not None else 0)
            for mlp in mlp_layers
        )
        
        compressed = (
            sum(v.numel() for v in self.c_fc_components.values()) +
            sum(v.numel() for v in self.c_proj_components.values()) +
            (self.c_fc_bias.numel() if self.c_fc_bias is not None else 0) +
            (self.c_proj_bias.numel() if self.c_proj_bias is not None else 0)
        )
        
        self.compression_ratio = compressed / original
        print(f"   ğŸ’¾ ì••ì¶•: {original:,} â†’ {compressed:,} ({(1-self.compression_ratio)*100:.1f}% ì ˆì•½)")
    
    def forward(self, x):
        """ìµœì í™”ëœ ìˆœì „íŒŒ"""
        # c_fc ì ìš©
        U, S, V = self.c_fc_components['U'], self.c_fc_components['S'], self.c_fc_components['V']
        c_fc_weight = torch.mm(U * S.unsqueeze(0), V.T)
        h = F.linear(x, c_fc_weight.T, self.c_fc_bias)
        h = self.activation(h)
        
        # c_proj ì ìš©
        U, S, V = self.c_proj_components['U'], self.c_proj_components['S'], self.c_proj_components['V']
        c_proj_weight = torch.mm(U * S.unsqueeze(0), V.T)
        output = F.linear(h, c_proj_weight.T, self.c_proj_bias)
        
        return output


def optimized_compression_test():
    """ìµœì í™” ì••ì¶• í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ¯ Reality Stone ìµœì í™” ì••ì¶•")
    print("=" * 80)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ì›ë³¸ í†µê³„
    original_params = sum(p.numel() for p in model.parameters())
    original_size_mb = original_params * 4 / (1024**2)
    
    print(f"\nğŸ“Š ì›ë³¸ ëª¨ë¸:")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_size_mb:.1f}MB")
    
    # í…ŒìŠ¤íŠ¸
    test_prompts = ["í•œêµ­ì˜ ìˆ˜ë„ëŠ”", "ì¸ê³µì§€ëŠ¥ì€", "ê¹€ì¹˜ëŠ”"]
    
    print("\nğŸ“‹ ì›ë³¸ ëª¨ë¸ ìƒ˜í”Œ:")
    for prompt in test_prompts:
        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(inputs.input_ids, max_length=20, do_sample=True, temperature=0.8)
        print(f"   '{prompt}' â†’ '{tokenizer.decode(outputs[0], skip_special_tokens=True)}'")
    
    # ìµœì í™” ì••ì¶• ì „ëµ
    compression_plan = [
        ([9, 10, 11], "í›„ë°˜ë¶€"),
        ([6, 7, 8], "ì¤‘ë°˜ë¶€2"),
        ([3, 4, 5], "ì¤‘ë°˜ë¶€1")
    ]
    
    print("\nğŸš€ ìµœì í™” ì••ì¶• ì‹œì‘...")
    compressed_model = copy.deepcopy(model)
    
    for group, name in compression_plan:
        print(f"\nğŸ“¦ {name} ì••ì¶•...")
        
        mlp_layers = [compressed_model.transformer.h[i].mlp for i in group]
        compressed_layer = OptimizedHybridLayer(mlp_layers, group)
        
        # ì ìš©
        compressed_model.transformer.h[group[0]].mlp = compressed_layer
        
        # ë‚˜ë¨¸ì§€ ì œê±°
        for i in reversed(group[1:]):
            del compressed_model.transformer.h[i]
    
    # ìµœì¢… í†µê³„
    compressed_params = sum(p.numel() for p in compressed_model.parameters())
    compressed_size_mb = compressed_params * 4 / (1024**2)
    compression_percentage = (1 - compressed_params / original_params) * 100
    
    print(f"\nğŸ“Š ì••ì¶• í›„ ëª¨ë¸:")
    print(f"   íŒŒë¼ë¯¸í„°: {compressed_params:,}")
    print(f"   í¬ê¸°: {compressed_size_mb:.1f}MB")
    
    print("\nğŸ“‹ ì••ì¶• ëª¨ë¸ ìƒ˜í”Œ:")
    for prompt in test_prompts:
        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = compressed_model.generate(inputs.input_ids, max_length=20, do_sample=True, temperature=0.8)
        print(f"   '{prompt}' â†’ '{tokenizer.decode(outputs[0], skip_special_tokens=True)}'")
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ† ìµœì¢… ì••ì¶• ê²°ê³¼")
    print("=" * 80)
    print(f"ğŸ“Š ì••ì¶• ì„±ê³¼:")
    print(f"   ì••ì¶•ë¥ : {compression_percentage:.1f}% (ì›ë³¸ ëŒ€ë¹„ {compression_percentage:.1f}% ì••ì¶•)")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {original_size_mb - compressed_size_mb:.1f}MB")
    print(f"   íŒŒë¼ë¯¸í„° ê°ì†Œ: {original_params - compressed_params:,}ê°œ")
    
    if compression_percentage >= 40:
        print(f"\nğŸ‰ ì„±ê³µ! {compression_percentage:.1f}% ì••ì¶• ë‹¬ì„±!")
        print("   âœ… ìŠ¤ë§ˆíŠ¸ ìœµí•©ìœ¼ë¡œ ì •ë³´ ë³´ì¡´")
        print("   âœ… ìµœì í™” SVDë¡œ í’ˆì§ˆ ìœ ì§€")
        print("   âœ… ì‹¤ìš©ì ì¸ ì••ì¶•ë¥  ë‹¬ì„±")
    
    print("\nâœ… ìµœì í™” ì••ì¶• ì™„ë£Œ!")


if __name__ == "__main__":
    optimized_compression_test() 