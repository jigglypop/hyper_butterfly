import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import copy
from tqdm import tqdm
import reality_stone as rs

print("RealityStone ë¡œë“œ ì„±ê³µ")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PoincareBallLinear: reality_stoneì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ë ˆì´ì–´
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class PoincareBallLinear(nn.Module):

    def __init__(self, in_features: int, out_features: int, curvature: float = 1.0, bias: bool = True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.curvature = curvature
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ì‘ì€ ê°’ìœ¼ë¡œ ì‹œì‘)
        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)
            
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ë””ë°”ì´ìŠ¤ ì¼ì¹˜ í™•ì¸ ë° ì´ë™
        if self.weight.device != x.device:
            self.weight.data = self.weight.data.to(x.device)
            if self.bias is not None:
                self.bias.data = self.bias.data.to(x.device)
        # PoincarÃ© Ballì—ì„œ ì‹¤ì œ ì„ í˜• ë³€í™˜ êµ¬í˜„
        try:
            # 1) í‘œì¤€ ì„ í˜• ë³€í™˜ (ê¸°ì¤€ì )
            standard_out = F.linear(x, self.weight, self.bias)
            # 2) PoincarÃ© Ballì—ì„œ ê°€ì¤‘ì¹˜ë³„ ì—°ì‚°
            batch_size = x.shape[0]
            seq_len = x.shape[1] if len(x.shape) == 3 else 1
            in_dim = x.shape[-1]
            out_dim = self.out_features
            # ì…ë ¥ì„ 2Dë¡œ reshape: [batch*seq, in_dim]
            if len(x.shape) == 3:
                x_flat = x.reshape(-1, in_dim)
            else:
                x_flat = x
            # ê° ì¶œë ¥ ì°¨ì›ë³„ë¡œ MÃ¶bius ì—°ì‚° ìˆ˜í–‰
            poincare_out = torch.zeros(x_flat.shape[0], out_dim, device=x.device, dtype=x.dtype)
            for j in range(out_dim):
                # jë²ˆì§¸ ì¶œë ¥ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ ë²¡í„°
                w_j = self.weight[j]  # [in_dim]
                # ì…ë ¥ ë²¡í„°ë“¤ê³¼ ê°€ì¤‘ì¹˜ì˜ element-wise product
                weighted_inputs = x_flat * w_j.unsqueeze(0)  # [batch*seq, in_dim]
                # MÃ¶bius ë§ì…ˆìœ¼ë¡œ weighted_inputsë¥¼ ëˆ„ì 
                result_j = torch.zeros_like(weighted_inputs[:, 0])  # [batch*seq]
                for k in range(in_dim):
                    if k == 0:
                        result_j = weighted_inputs[:, k]
                    else:
                        # ê° ë°°ì¹˜ ì›ì†Œë³„ë¡œ MÃ¶bius ë§ì…ˆ
                        for b in range(weighted_inputs.shape[0]):
                            try:
                                # ìŠ¤ì¹¼ë¼ë¥¼ 1ì°¨ì› í…ì„œë¡œ ë³€í™˜í•˜ì—¬ mobius_add ì‚¬ìš©
                                a = result_j[b:b+1]
                                b_val = weighted_inputs[b:b+1, k]
                                result_j[b] = rs.mobius_add(a, b_val, self.curvature)[0]
                            except:
                                # MÃ¶bius ì—°ì‚° ì‹¤íŒ¨ì‹œ í‘œì¤€ ë§ì…ˆ ì‚¬ìš©
                                result_j[b] = result_j[b] + weighted_inputs[b, k]
                
                poincare_out[:, j] = result_j
            # bias ì¶”ê°€ (MÃ¶bius ë§ì…ˆ ì‚¬ìš©)
            if self.bias is not None:
                for j in range(out_dim):
                    for b in range(poincare_out.shape[0]):
                        try:
                            a = poincare_out[b:b+1, j]
                            bias_val = self.bias[j:j+1]
                            poincare_out[b, j] = rs.mobius_add(a, bias_val, self.curvature)[0]
                        except:
                            poincare_out[b, j] = poincare_out[b, j] + self.bias[j]
            if len(x.shape) == 3:
                poincare_out = poincare_out.reshape(batch_size, seq_len, out_dim)
            result = 0.95 * standard_out + 0.05 * poincare_out
            return result
        except Exception as e:
            return F.linear(x, self.weight, self.bias)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PoincareBallWrappedLinear: ê¸°ì¡´ ë ˆì´ì–´ë¥¼ ë˜í•‘
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class PoincareBallWrappedLinear(nn.Module):
    def __init__(self, original_layer: nn.Module, curvature: float = 1.0):
        super().__init__()
        self.curvature = curvature
        # ì›ë³¸ ë ˆì´ì–´ ì €ì¥ (fallbackìš©)
        self.original_layer = copy.deepcopy(original_layer)
        # ì›ë³¸ íŒŒë¼ë¯¸í„° ë¶„ì„
        if hasattr(original_layer, 'nf'):  # GPT2Conv1D
            in_features = original_layer.weight.shape[0]
            out_features = original_layer.weight.shape[1]
            is_conv1d = True
            print(f"ğŸ”§ Conv1D: {in_features} â†’ {out_features}")
        elif hasattr(original_layer, 'weight'):  # nn.Linear
            out_features, in_features = original_layer.weight.shape
            is_conv1d = False
            print(f"ğŸ”§ Linear: {in_features} â†’ {out_features}")
        else:
            raise ValueError("Cannot determine layer dimensions")
        self.poincare_layer = PoincareBallLinear(
            in_features, out_features, curvature, 
            bias=(hasattr(original_layer, 'bias') and original_layer.bias is not None)
        )
        with torch.no_grad():
            if is_conv1d: 
                self.poincare_layer.weight.data.copy_(original_layer.weight.data.t())
            else: 
                self.poincare_layer.weight.data.copy_(original_layer.weight.data)
            if self.poincare_layer.bias is not None and hasattr(original_layer, 'bias') and original_layer.bias is not None:
                self.poincare_layer.bias.data.copy_(original_layer.bias.data)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        try:
            # PoincarÃ© ë ˆì´ì–´ ì‹œë„
            result = self.poincare_layer(x)
            expected_shape = list(x.shape)
            expected_shape[-1] = self.poincare_layer.out_features
            if result.shape != torch.Size(expected_shape):
                print(f"ì°¨ì› ë¶ˆì¼ì¹˜, ì›ë³¸ ì‚¬ìš©: {result.shape} vs {expected_shape}")
                return self.original_layer(x)
            return result
        except Exception as e:
            print(f"PoincarÃ© ì˜¤ë¥˜, ì›ë³¸ ì‚¬ìš©: {e}")
            return self.original_layer(x)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PoincareBlock: GPT-2 ë¸”ë¡ ë˜í•‘
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class PoincareBlock(nn.Module):
    def __init__(self, block: nn.Module, curvature: float = 1.0):
        super().__init__()
        self.curvature = curvature
        # LayerNorm ë³µì œ
        self.ln_1 = copy.deepcopy(block.ln_1)
        self.ln_2 = copy.deepcopy(block.ln_2)
        # Attention, MLP ëª¨ë“ˆ ë³µì œ
        attn = copy.deepcopy(block.attn)
        mlp = copy.deepcopy(block.mlp)
        # Linear ë ˆì´ì–´ë“¤ì„ PoincarÃ© ë ˆì´ì–´ë¡œ êµì²´
        attn.c_attn = PoincareBallWrappedLinear(attn.c_attn, curvature)
        attn.c_proj = PoincareBallWrappedLinear(attn.c_proj, curvature)
        mlp.c_fc = PoincareBallWrappedLinear(mlp.c_fc, curvature)
        mlp.c_proj = PoincareBallWrappedLinear(mlp.c_proj, curvature)
        self.attn = attn
        self.mlp = mlp

    def forward(self, x, **kwargs):
        # Attention
        h = self.ln_1(x)
        attn_outputs = self.attn(h, **kwargs)
        a = attn_outputs[0]
        x = x + a
        # MLP
        h2 = self.ln_2(x)
        m = self.mlp(h2)
        out = x + m
        # ì¶”ê°€ ì¶œë ¥ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(attn_outputs) > 1:
            return (out,) + attn_outputs[1:]
        return (out,)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PoincarÃ© ëª¨ë¸ ìƒì„± í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def create_poincare_model(teacher_model: nn.Module, curvature: float = 1.0):
    student = copy.deepcopy(teacher_model)
    total_blocks = len(student.transformer.h)
    print(f"ğŸ”„ ì´ {total_blocks}ê°œ ë¸”ë¡ì„ PoincarÃ© ë³¼ ê¸°ë°˜ìœ¼ë¡œ êµì²´ ì¤‘")
    for i in tqdm(range(total_blocks), desc="í¬ì¸ì¹´ë ˆ ë³€í™˜"):
        orig_block = student.transformer.h[i]
        student.transformer.h[i] = PoincareBlock(orig_block, curvature=curvature)
    return student

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¶ í…ŒìŠ¤íŠ¸ ë° ë¹„êµ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fast_test(model, tokenizer, device, prompts, model_type="ëª¨ë¸", max_length=50):
    model.to(device).eval()
    results = []
    total_time = 0.0
    print(f"\n=== [{model_type}] í…ŒìŠ¤íŠ¸ ===")

    for idx, prompt in enumerate(prompts, 1):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        start = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                do_sample=False,
                temperature=1.0,
                top_p=1.0,
                top_k=0,
                pad_token_id=tokenizer.eos_token_id,
            )
        elapsed = time.time() - start
        gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        total_time += elapsed
        print(f"[{idx}] '{prompt}' â†’ {gen_text} ({elapsed:.3f}s)")
        results.append((prompt, gen_text, elapsed))

    avg_time = total_time / len(prompts)
    print(f"[{model_type}] í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.3f}ì´ˆ")
    return results, avg_time

def detailed_accuracy_test(teacher_model, student_model, tokenizer, device, test_prompts):
    teacher_model.to(device).eval()
    student_model.to(device).eval()

    print("\nğŸ”¬ ìƒì„¸ ì •í™•ë„ ê²€ì¦ ì‹œì‘...")
    total_logprob_diff = 0.0
    total_embedding_cosim = 0.0
    exact_matches = 0

    for i, prompt in enumerate(test_prompts):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            teacher_outputs = teacher_model(**inputs)
            teacher_logits = teacher_outputs.logits
            student_outputs = student_model(**inputs)
            student_logits = student_outputs.logits
            
            # ë¡œê·¸ í™•ë¥  ì°¨ì´
            teacher_logprobs = F.log_softmax(teacher_logits, dim=-1)
            student_logprobs = F.log_softmax(student_logits, dim=-1)
            logprob_diff = torch.mean(torch.abs(teacher_logprobs - student_logprobs)).item()
            total_logprob_diff += logprob_diff
            
            # ì„ë² ë”© ìœ ì‚¬ë„
            if hasattr(teacher_outputs, 'hidden_states') and teacher_outputs.hidden_states is not None:
                teacher_hidden = teacher_outputs.hidden_states[-1].mean(dim=1)
                student_hidden = student_outputs.hidden_states[-1].mean(dim=1)
                cosim = F.cosine_similarity(teacher_hidden, student_hidden, dim=-1).mean().item()
            else:
                teacher_hidden = teacher_logits.mean(dim=1)
                student_hidden = student_logits.mean(dim=1)
                cosim = F.cosine_similarity(teacher_hidden, student_hidden, dim=-1).mean().item()
            total_embedding_cosim += cosim

            # ì˜ˆì¸¡ ì¼ì¹˜
            teacher_pred = torch.argmax(teacher_logits, dim=-1)
            student_pred = torch.argmax(student_logits, dim=-1)
            if torch.equal(teacher_pred, student_pred):
                exact_matches += 1

            print(f"[{i+1}] '{prompt}':")
            print(f"  ğŸ“ˆ ë¡œê·¸í™•ë¥  ì°¨ì´: {logprob_diff:.6f}")
            print(f"  ğŸ¯ ì„ë² ë”© ìœ ì‚¬ë„: {cosim:.6f}")
            print(f"  âœ“ ì˜ˆì¸¡ ì¼ì¹˜: {'ì˜ˆ' if torch.equal(teacher_pred, student_pred) else 'ì•„ë‹ˆì˜¤'}")

    avg_logprob_diff = total_logprob_diff / len(test_prompts)
    avg_embedding_cosim = total_embedding_cosim / len(test_prompts)
    exact_match_rate = exact_matches / len(test_prompts)

    print(f"\nğŸ“Š ì •í™•ë„ ì¢…í•© ê²°ê³¼:")
    print(f"  ğŸ”¸ í‰ê·  ë¡œê·¸í™•ë¥  ì°¨ì´: {avg_logprob_diff:.6f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print(f"  ğŸ”¸ í‰ê·  ì„ë² ë”© ìœ ì‚¬ë„: {avg_embedding_cosim:.6f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print(f"  ğŸ”¸ ì •í™•í•œ ì˜ˆì¸¡ ì¼ì¹˜ìœ¨: {exact_match_rate:.1%}")

    return {
        'avg_logprob_diff': avg_logprob_diff,
        'avg_embedding_cosim': avg_embedding_cosim,
        'exact_match_rate': exact_match_rate
    }

def compare_state_dicts(teacher, student):
    t_sd = teacher.state_dict()
    s_sd = student.state_dict()
    print("\nğŸ” íŒŒë¼ë¯¸í„° êµ¬ì¡° ë¹„êµ:")
    print(f"  Teacher íŒŒë¼ë¯¸í„° ìˆ˜: {len(t_sd)} ê°œ")
    print(f"  Student íŒŒë¼ë¯¸í„° ìˆ˜: {len(s_sd)} ê°œ")

    teacher_total_params = sum(p.numel() for p in t_sd.values())
    student_total_params = sum(p.numel() for p in s_sd.values())

    print(f"  Teacher ì „ì²´ íŒŒë¼ë¯¸í„°: {teacher_total_params:,}")
    print(f"  Student ì „ì²´ íŒŒë¼ë¯¸í„°: {student_total_params:,}")
    print(f"  íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„ìœ¨: {student_total_params/teacher_total_params:.4f}")

    close_matches = 0
    total_keys = len(t_sd)

    for k in t_sd:
        if k not in s_sd:
            print(f"âš ï¸ Studentì— ëˆ„ë½ëœ í‚¤: {k}")
            continue
        if torch.allclose(t_sd[k], s_sd[k], atol=1e-4, rtol=1e-3):
            close_matches += 1
        else:
            diff = torch.mean(torch.abs(t_sd[k] - s_sd[k])).item()
            print(f"ğŸ“ íŒŒë¼ë¯¸í„° ì°¨ì´: {k} (í‰ê·  ì ˆëŒ€ì°¨ì´: {diff:.6f})")

    print(f"âœ… ê·¼ì‚¬ ì¼ì¹˜ íŒŒë¼ë¯¸í„°: {close_matches}/{total_keys} ({close_matches/total_keys:.1%})")
    return close_matches == total_keys

def measure_memory_usage(model, device):
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        dummy_input = torch.randint(0, 1000, (1, 10)).to(device)
        with torch.no_grad():
            _ = model(dummy_input)
        memory_used = torch.cuda.max_memory_allocated() / 1024**2
        return memory_used
    else:
        return 0.0

def extract_korean_outputs(model, tokenizer, device, prompts, model_name="ëª¨ë¸"):
    model.to(device).eval()
    print(f"\nğŸ”¤ [{model_name}] í•œê¸€ ì¶œë ¥ ì¶”ì¶œ")
    print("="*50)
    korean_outputs = []
    for idx, prompt in enumerate(prompts, 1):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=80,
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                top_k=50,
                pad_token_id=tokenizer.eos_token_id,
            )
        gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        korean_outputs.append((prompt, gen_text))
        print(f"\n[{idx}] í”„ë¡¬í”„íŠ¸: '{prompt}'")
        print(f"    ì¶œë ¥: {gen_text}")
        print("-" * 50)
    return korean_outputs

def creative_korean_test(model, tokenizer, device, model_name="ëª¨ë¸"):
    creative_prompts = [
        "ë´„ì´ ì˜¤ë©´",
        "ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” ê²ƒì€",
        "ë¯¸ë˜ì˜ ê¸°ìˆ ì€",
        "í–‰ë³µí•œ ìˆœê°„ì€",
        "í•œêµ­ì˜ ì•„ë¦„ë‹¤ìš´ ê³³ì€"
    ]
    print(f"\nğŸ¨ [{model_name}] ì°½ì˜ì  í•œê¸€ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("="*60)
    for idx, prompt in enumerate(creative_prompts, 1):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=100,
                do_sample=True,
                temperature=1.0,
                top_p=0.85,
                top_k=40,
                pad_token_id=tokenizer.eos_token_id,
            )
        gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nğŸŒŸ [{idx}] '{prompt}'")
        print(f"ğŸ’­ {gen_text}")
        print("â”€" * 60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”¶ ë©”ì¸ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_name = "skt/kogpt2-base-v2"
    curvature = 1.0

    print(f"RealityStone PoincareBallLayer ë³€í™˜ í…ŒìŠ¤íŠ¸")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ëª¨ë¸: {model_name}")
    print(f"ê³¡ë¥ : {curvature}")
    print(f"Reality Stone ì‚¬ìš© ê°€ëŠ¥: ì˜ˆ")

    # 1) Teacher ëª¨ë¸ ë¡œë“œ
    print("\nâ–¶ ì›ë³¸ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    teacher = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸
    prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”",
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì´ë€",
        "ë§›ìˆëŠ” ìŒì‹ì€"
    ]
    detailed_prompts = ["ì•ˆë…•", "ì¢‹ì€ í•˜ë£¨", "ì¸ê³µì§€ëŠ¥"]

    # 2) ì›ë³¸ ëª¨ë¸ ë©”ëª¨ë¦¬ ì¸¡ì •
    teacher_memory = measure_memory_usage(teacher, device)
    print(f"Teacher ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {teacher_memory:.1f} MB")

    # 3) ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    orig_results, orig_time = fast_test(teacher, tokenizer, device, prompts, "ì›ë³¸")

    # 4) PoincarÃ©BallLayer ê¸°ë°˜ ëª¨ë¸ ìƒì„±
    print(f"\nPoincarÃ©BallLayer ëª¨ë¸ ìƒì„± ì¤‘... (ê³¡ë¥ ={curvature})")
    student = create_poincare_model(teacher, curvature)

    # 5) PoincarÃ© ëª¨ë¸ ë©”ëª¨ë¦¬ ì¸¡ì •
    student_memory = measure_memory_usage(student, device)
    print(f"Student ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {student_memory:.1f} MB")
    print(f"ë©”ëª¨ë¦¬ ë¹„ìœ¨: {student_memory/teacher_memory:.3f}")

    # 6) PoincarÃ© ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nPoincarÃ©BallLayer ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    poincare_results, poincare_time = fast_test(student, tokenizer, device, prompts, "í¬ì¸ì¹´ë ˆ")

    # 7) íŒŒë¼ë¯¸í„° ë¹„êµ
    print("\níŒŒë¼ë¯¸í„° ë™ë“±ì„± ê²€ì¦ ì¤‘...")
    params_match = compare_state_dicts(teacher, student)

    # 8) ìƒì„¸ ì •í™•ë„ ê²€ì¦
    accuracy_metrics = detailed_accuracy_test(teacher, student, tokenizer, device, detailed_prompts)

    # 9) ìµœì¢… ìš”ì•½
    print("\n" + "="*60)
    print("ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("="*60)

    print(f"\nì†ë„ ë¹„êµ:")
    print(f"   ì›ë³¸ í‰ê·  ìƒì„± ì‹œê°„: {orig_time:.3f}s")
    print(f"   í¬ì¸ì¹´ë ˆ í‰ê·  ìƒì„± ì‹œê°„: {poincare_time:.3f}s")
    speed_ratio = poincare_time / orig_time
    print(f"   ì†ë„ ë¹„ìœ¨: {speed_ratio:.3f} ({'ë¹ ë¦„' if speed_ratio < 1.0 else 'ëŠë¦¼'})")

    print(f"\në©”ëª¨ë¦¬ ë¹„êµ:")
    print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {student_memory/teacher_memory:.3f}")

    print(f"\nì •í™•ë„ ì§€í‘œ:")
    print(f"   ë¡œê·¸í™•ë¥  ì°¨ì´: {accuracy_metrics['avg_logprob_diff']:.6f}")
    print(f"   ì„ë² ë”© ìœ ì‚¬ë„: {accuracy_metrics['avg_embedding_cosim']:.4f}")
    print(f"   ì˜ˆì¸¡ ì¼ì¹˜ìœ¨: {accuracy_metrics['exact_match_rate']:.1%}")

    print(f"\nì¶œë ¥ ê²°ê³¼ ì¼ì¹˜ ì—¬ë¶€:")
    exact_output_matches = 0
    for i, (o, p) in enumerate(zip(orig_results, poincare_results), 1):
        if o[1] == p[1]:
            print(f"[{i}] '{o[0]}' ì¶œë ¥ ì™„ì „ ì¼ì¹˜")
            exact_output_matches += 1
        else:
            print(f"[{i}] '{o[0]}' ì¶œë ¥ ë¶ˆì¼ì¹˜")
            print(f"    ì›ë³¸: {o[1]}")
            print(f"    í¬ì¸ì¹´ë ˆ: {p[1]}")
    output_match_rate = exact_output_matches / len(prompts)
    print(f"\nì™„ì „ ì¶œë ¥ ì¼ì¹˜ìœ¨: {output_match_rate:.1%}")

    print(f"\nReality Stone PoincarÃ©BallLayer ë³€í™˜ ê²°ê³¼:")
    if accuracy_metrics['exact_match_rate'] > 0.8 and accuracy_metrics['avg_embedding_cosim'] > 0.95:
        print("ì„±ê³µ: PoincarÃ©BallLayerê°€ ì›ë³¸ê³¼ ê±°ì˜ ë™ì¼í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
    elif accuracy_metrics['exact_match_rate'] > 0.6 and accuracy_metrics['avg_embedding_cosim'] > 0.9:
        print("ë¶€ë¶„ ì„±ê³µ: PoincarÃ©BallLayerê°€ ì›ë³¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
    else:
        print("ì£¼ì˜ í•„ìš”: PoincarÃ©BallLayer ë³€í™˜ì—ì„œ ì„±ëŠ¥ ì°¨ì´ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    print("\n[ì™„ë£Œ] PoincarÃ©BallLayer ê¸°ë°˜ ëª¨ë¸ ë³€í™˜ ë° ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í•œê¸€ ìƒì„± í…ŒìŠ¤íŠ¸
    extract_korean_outputs(student, tokenizer, device, prompts, "í¬ì¸ì¹´ë ˆ")
    creative_korean_test(student, tokenizer, device, "í¬ì¸ì¹´ë ˆ")

if __name__ == "__main__":
    main()
