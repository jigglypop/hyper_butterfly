import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import time
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
from collections import Counter
import copy

try:
    import reality_stone as rs
    print("âœ… RealityStone ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ!")
    RS_AVAILABLE = True
except ImportError:
    print("âš ï¸ RealityStone ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ê¸°ë³¸ ì••ì¶• ì‚¬ìš©")
    RS_AVAILABLE = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ë³´ìˆ˜ì  ê³ í’ˆì§ˆ ì••ì¶•ê¸° (í’ˆì§ˆ ìš°ì„ )
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ConservativeSVDCompressor:
    """ë³´ìˆ˜ì  SVD ì••ì¶•ê¸° (í’ˆì§ˆ ìš°ì„ , ì•ˆì „í•œ ì••ì¶•ë¥ )"""
    
    def __init__(self, W: torch.Tensor, compression_ratio=0.3):
        """
        Args:
            W: ê°€ì¤‘ì¹˜ í–‰ë ¬ [out_f, in_f] 
            compression_ratio: ë³´ìˆ˜ì  ì••ì¶•ë¥  (30% = 70% íŒŒë¼ë¯¸í„° ìœ ì§€)
        """
        self.out_f, self.in_f = W.shape
        self.compression_ratio = compression_ratio
        
        print(f"    ğŸ”§ ë³´ìˆ˜ì  ì••ì¶•: {W.shape}, ì••ì¶•ë¥ ={compression_ratio:.1%}")
        
        self._apply_conservative_compression(W)
    
    def _apply_conservative_compression(self, W: torch.Tensor):
        """ë³´ìˆ˜ì  SVD ì••ì¶• (í’ˆì§ˆ ìš°ì„ )"""
        
        # SVD ë¶„í•´
        U, S, V = torch.svd(W.float())
        
        # ì—ë„ˆì§€ ê¸°ë°˜ ë­í¬ ì„ íƒ (95% ì—ë„ˆì§€ ë³´ì¡´)
        energy_cumsum = torch.cumsum(S**2, dim=0)
        total_energy = energy_cumsum[-1]
        energy_threshold = total_energy * 0.95  # 95% ì—ë„ˆì§€ ë³´ì¡´
        
        energy_rank = torch.sum(energy_cumsum < energy_threshold).item() + 1
        
        # ë³´ìˆ˜ì  ë­í¬ (ë” ë§ì€ íŒŒë¼ë¯¸í„° ìœ ì§€)
        conservative_rank = max(
            min(W.shape) // 2,  # ìµœì†Œ ì ˆë°˜ì€ ìœ ì§€
            int(min(W.shape) * (1 - self.compression_ratio))  # ë³´ìˆ˜ì  ê³„ì‚°
        )
        
        # ìµœì¢… ë­í¬ (ì—ë„ˆì§€ì™€ ë³´ìˆ˜ì  ì¤‘ í° ê°’)
        final_rank = min(max(energy_rank, conservative_rank), len(S))
        
        # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ì €ì¥
        self.U = nn.Parameter(U[:, :final_rank].to(W.dtype))
        self.S = nn.Parameter(S[:final_rank].to(W.dtype))
        self.V = nn.Parameter(V[:, :final_rank].to(W.dtype))
        
        # ì••ì¶• í†µê³„
        original_params = W.numel()
        compressed_params = self.U.numel() + self.S.numel() + self.V.numel()
        actual_ratio = compressed_params / original_params
        
        print(f"       âœ… ë³´ìˆ˜ì  ì••ì¶•: rank {final_rank}, ì‹¤ì œ ì••ì¶•ë¥  {actual_ratio:.1%}")
        print(f"          ì—ë„ˆì§€ ë³´ì¡´: {95:.0f}%, íŒŒë¼ë¯¸í„° ìœ ì§€: {actual_ratio*100:.0f}%")
    
    def reconstruct(self) -> torch.Tensor:
        """ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ë³µì›"""
        return self.U @ torch.diag(self.S) @ self.V.t()
    
    def apply(self, x: torch.Tensor) -> torch.Tensor:
        """ì••ì¶•ëœ ì—°ì‚° ì ìš©"""
        # 3ë‹¨ê³„ íš¨ìœ¨ì  ê³„ì‚°
        step1 = x @ self.V  # [batch, rank]
        step2 = step1 * self.S.unsqueeze(0)  # [batch, rank]
        step3 = step2 @ self.U.t()  # [batch, out_features]
        return step3

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ë³´ìˆ˜ì  Linear ë ˆì´ì–´ (Conv1D ì§€ì›)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ConservativeLinear(nn.Module):
    """ë³´ìˆ˜ì  Linear ë ˆì´ì–´ (í’ˆì§ˆ ìš°ì„ )"""
    
    def __init__(self, lin, compression_ratio=0.3):
        super().__init__()
        
        if hasattr(lin, 'weight'):
            W = lin.weight.data.clone()
            
            # Conv1D ì²˜ë¦¬
            if hasattr(lin, 'nf'):  # GPT2 Conv1D
                self.in_features = W.shape[0]  # [768, 2304] í˜•íƒœ
                self.out_features = W.shape[1]
                self.is_conv1d = True
                # Conv1DëŠ” ì´ë¯¸ ì „ì¹˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì••ì¶•ì„ ìœ„í•´ ë‹¤ì‹œ ì „ì¹˜
                W = W.t()  # [out_features, in_features]ë¡œ ë³€í™˜
                print(f"ğŸ”§ Conv1D ë³´ìˆ˜ì ì••ì¶•: in={self.in_features}, out={self.out_features}")
            else:  # nn.Linear
                self.in_features = lin.in_features
                self.out_features = lin.out_features
                self.is_conv1d = False
                print(f"ğŸ”§ Linear ë³´ìˆ˜ì ì••ì¶•: in={self.in_features}, out={self.out_features}")
            
            # ë³´ìˆ˜ì  ì••ì¶•ê¸° ì ìš©
            self.compressor = ConservativeSVDCompressor(W, compression_ratio)
            
            # ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
            if hasattr(lin, 'bias') and lin.bias is not None:
                self.bias = nn.Parameter(lin.bias.data.clone())
            else:
                self.bias = None
        else:
            raise ValueError("Input layer must have weight attribute")
    
    def forward(self, x):
        if self.is_conv1d:
            # Conv1D: weightë¥¼ ë³µì›í•˜ê³  ì „ì¹˜í•˜ì—¬ ì‚¬ìš©
            W_compressed = self.compressor.reconstruct()  # [out_f, in_f]
            W_conv1d = W_compressed.t()  # [in_f, out_f] Conv1D í˜•íƒœ
            out = x @ W_conv1d
        else:
            # Linear: ì§ì ‘ ì ìš©
            out = self.compressor.apply(x)
        
        if self.bias is not None:
            out = out + self.bias
        
        return out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ë³´ìˆ˜ì  Block (ë ˆì´ì–´ë³„ ì°¨ë³„ ì••ì¶•)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ConservativeBlock(nn.Module):
    """ë³´ìˆ˜ì  Block (ë ˆì´ì–´ë³„ ì°¨ë³„ ì••ì¶•)"""
    
    def __init__(self, block, base_ratio=0.25, layer_idx=0, total_layers=12):
        super().__init__()
        self.ln1 = block.ln_1
        self.ln2 = block.ln_2
        attn, mlp = block.attn, block.mlp
        
        # ë ˆì´ì–´ë³„ ì°¨ë³„ ì••ì¶•ë¥  (ì¤‘ìš” ë ˆì´ì–´ëŠ” ëœ ì••ì¶•)
        layer_ratio = self._get_layer_compression_ratio(layer_idx, total_layers, base_ratio)
        
        print(f"ğŸ”§ ë³´ìˆ˜ì  ë ˆì´ì–´ {layer_idx}: ì••ì¶•ë¥  {layer_ratio:.1%}")
        
        # ê° ì„œë¸Œë ˆì´ì–´ ì••ì¶•
        attn.c_attn = ConservativeLinear(attn.c_attn, layer_ratio)
        attn.c_proj = ConservativeLinear(attn.c_proj, layer_ratio)
        mlp.c_fc = ConservativeLinear(mlp.c_fc, layer_ratio)
        mlp.c_proj = ConservativeLinear(mlp.c_proj, layer_ratio)
        
        self.attn, self.mlp = attn, mlp
    
    def _get_layer_compression_ratio(self, layer_idx, total_layers, base_ratio):
        """ë ˆì´ì–´ë³„ ì°¨ë³„ ì••ì¶•ë¥ """
        
        normalized_idx = layer_idx / total_layers
        
        if layer_idx == 0 or layer_idx == total_layers - 1:
            # ì²«ì§¸/ë§ˆì§€ë§‰ ë ˆì´ì–´: ê°€ì¥ ë³´ìˆ˜ì 
            return base_ratio * 0.5
        elif normalized_idx < 0.3 or normalized_idx > 0.7:
            # ì•ìª½/ë’¤ìª½ ë ˆì´ì–´: ë³´ìˆ˜ì 
            return base_ratio * 0.7
        else:
            # ì¤‘ê°„ ë ˆì´ì–´: ê¸°ë³¸ ì••ì¶•
            return base_ratio
    
    def forward(self, x, **kwargs):
        h = self.ln1(x)
        attn_outputs = self.attn(h, **kwargs)
        a = attn_outputs[0]
        x = x + a
        h2 = self.ln2(x)
        m = self.mlp(h2)
        output = x + m
        
        if len(attn_outputs) > 1:
            return (output,) + attn_outputs[1:]
        else:
            return (output,)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ë³´ìˆ˜ì  ì••ì¶• íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def apply_conservative_compression(model, compression_ratio=0.25):
    """ë³´ìˆ˜ì  ì••ì¶• íŒŒì´í”„ë¼ì¸ (í’ˆì§ˆ ìš°ì„ )"""
    
    total = sum(p.numel() for p in model.parameters())
    total_layers = len(model.transformer.h)
    
    print(f"Before: {total:,} params ({total/1e6:.1f}M)")
    print(f"ğŸ”§ ë³´ìˆ˜ì  ì••ì¶•: ëª©í‘œ={compression_ratio:.1%} (í’ˆì§ˆ ìš°ì„ )")
    
    # ë³´ìˆ˜ì  ë ˆì´ì–´ ì„ íƒ (ê°€ì¥ìë¦¬ ë³´ì¡´)
    compress_layers = list(range(1, total_layers-1))  # ì²«ì§¸/ë§ˆì§€ë§‰ ì œì™¸
    
    print(f"   ì••ì¶• ëŒ€ìƒ: {len(compress_layers)}/{total_layers} ë ˆì´ì–´")
    
    # ë³´ìˆ˜ì  ì••ì¶• ì§„í–‰
    for i in tqdm(compress_layers, desc="ğŸ”§ ë³´ìˆ˜ì  ì••ì¶•"):
        if i < len(model.transformer.h):
            try:
                model.transformer.h[i] = ConservativeBlock(
                    model.transformer.h[i], compression_ratio, i, total_layers
                )
            except Exception as e:
                print(f"   âš ï¸ ë ˆì´ì–´ {i} ì••ì¶• ì‹¤íŒ¨: {e}")
                continue
    
    total2 = sum(p.numel() for p in model.parameters())
    actual_compression = total2 / total
    
    print(f"After:  {total2:,} params ({total2/1e6:.1f}M)")
    print(f"ğŸ”§ ì‹¤ì œ ì••ì¶•ë¥ : {actual_compression:.1%} ({1/actual_compression:.1f}Ã— ì••ì¶•)")
    print(f"âœ… ë©”ëª¨ë¦¬ ì ˆì•½: {(1-actual_compression)*100:.1f}%")
    
    return model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ì •í™•í•œ í’ˆì§ˆ í‰ê°€ (ê¹¨ì§„ í…ìŠ¤íŠ¸ ê°ì§€)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def accurate_quality_evaluation(generated_text, prompt):
    """ì •í™•í•œ í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ (ê¹¨ì§„ í…ìŠ¤íŠ¸ ì—„ê²© ê°ì§€)"""
    
    generated_only = generated_text[len(prompt):].strip()
    if len(generated_only) < 2:
        return 0.0
    
    score = 3.0  # ì‹œì‘ ì ìˆ˜
    
    # 1. ê¹¨ì§„ í…ìŠ¤íŠ¸ ê°ì§€ (ê°€ì¥ ì¤‘ìš”!)
    broken_patterns = [
        r'[ê°€-í£]{1}[a-zA-Zê°€-í£]{1}[ê°€-í£]{1}',  # í•œê¸€-ì˜ì–´-í•œê¸€ íŒ¨í„´
        r'í‹°ì•„|í‹°ìŠ¤|ë¥´íŠ¸|ë³‘ì •|ì‚´ì„|ë² ì•„|ê´´ë¼|ëœí™€',      # ì´ìƒí•œ ìŒì ˆ ì¡°í•©
        r'[ê°€-í£]{10,}',                          # 10ê¸€ì ì´ìƒ ì—°ì† í•œê¸€
        r'[ã„±-ã…ã…-ã…£]',                          # ë¶ˆì™„ì „í•œ í•œê¸€
    ]
    
    for pattern in broken_patterns:
        if re.search(pattern, generated_only):
            score -= 2.0  # ê¹¨ì§„ í…ìŠ¤íŠ¸ ë°œê²¬ì‹œ í° í˜ë„í‹°
            break
    
    # 2. ë°˜ë³µ íŒ¨í„´ ê°ì§€
    char_repeats = len(re.findall(r'(.)\1{2,}', generated_only))
    if char_repeats > 0:
        score -= min(1.0, char_repeats * 0.5)
    
    # 3. í•œêµ­ì–´ ë¬¸ë²• êµ¬ì¡°
    korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'í•´ìš”', 'ì–´ìš”', 'ì•„ìš”', 'ë„¤ìš”', 'ìŠµë‹ˆë‹¤']
    has_proper_ending = any(generated_only.endswith(ending) for ending in korean_endings)
    
    if has_proper_ending:
        score += 0.5
    
    # 4. ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ í¬í•¨
    meaningful_words = ['ë‚ ì”¨', 'ì¢‹', 'ë‚˜ì˜', 'ì•ˆë…•', 'ê°ì‚¬', 'ì£„ì†¡', 'ì˜¤ëŠ˜', 'ë‚´ì¼']
    has_meaningful = any(word in generated_only for word in meaningful_words)
    
    if has_meaningful:
        score += 0.3
    
    return max(0.0, min(3.0, score))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ì§‘ì¤‘ Knowledge Distillation (ë” ê¸´ íŒŒì¸íŠœë‹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def intensive_knowledge_distillation(teacher_model, student_model, tokenizer, 
                                   total_steps=1500, base_lr=5e-6, temperature=2.5):
    """ì§‘ì¤‘ Knowledge Distillation (ê¸´ íŒŒì¸íŠœë‹)"""
    
    print(f"\nğŸ§  ì§‘ì¤‘ Knowledge Distillation íŒŒì¸íŠœë‹")
    print(f"   ğŸ“Š ìŠ¤í…: {total_steps}, í•™ìŠµë¥ : {base_lr}, ì˜¨ë„: {temperature}")
    
    # ì²´ê³„ì ì¸ í•œêµ­ì–´ í›ˆë ¨ ë°ì´í„°
    train_texts = [
        # ê¸°ë³¸ ì¸ì‚¬ (ì™„ë²½í•œ ë¬¸ì¥)
        "ì•ˆë…•í•˜ì„¸ìš”.", "ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.", "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤.", "ì¢‹ì€ ì €ë…ì…ë‹ˆë‹¤.",
        "ì•ˆë…•íˆ ê°€ì„¸ìš”.", "ì•ˆë…•íˆ ê³„ì„¸ìš”.", "ê°ì‚¬í•©ë‹ˆë‹¤.", "ê³ ë§™ìŠµë‹ˆë‹¤.",
        
        # ë‚ ì”¨ í‘œí˜„ (ì™„ë²½í•œ ë¬¸ì¥)
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§‘ìŠµë‹ˆë‹¤.", "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ íë¦½ë‹ˆë‹¤.", "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤.",
        "ë¹„ê°€ ì˜µë‹ˆë‹¤.", "ëˆˆì´ ì˜µë‹ˆë‹¤.", "ë°”ëŒì´ ë¶‘ë‹ˆë‹¤.", "ë‚ ì”¨ê°€ ì¶¥ìŠµë‹ˆë‹¤.",
        
        # ì¼ìƒ í‘œí˜„ (ì™„ë²½í•œ ë¬¸ì¥) 
        "ë°¥ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤.", "ë¬¼ì„ ë§ˆì…¨ìŠµë‹ˆë‹¤.", "ì±…ì„ ì½ì—ˆìŠµë‹ˆë‹¤.", "ê³µë¶€ë¥¼ í–ˆìŠµë‹ˆë‹¤.",
        "ìš´ë™ì„ í–ˆìŠµë‹ˆë‹¤.", "ìŒì•…ì„ ë“¤ì—ˆìŠµë‹ˆë‹¤.", "ì˜í™”ë¥¼ ë´¤ìŠµë‹ˆë‹¤.", "ì‡¼í•‘ì„ í–ˆìŠµë‹ˆë‹¤.",
        
        # ê°ì • í‘œí˜„ (ì™„ë²½í•œ ë¬¸ì¥)
        "ê¸°ë¶„ì´ ì¢‹ìŠµë‹ˆë‹¤.", "ê¸°ë¶„ì´ ë‚˜ì©ë‹ˆë‹¤.", "í–‰ë³µí•©ë‹ˆë‹¤.", "ìŠ¬í”•ë‹ˆë‹¤.",
        "ì¦ê²ìŠµë‹ˆë‹¤.", "í”¼ê³¤í•©ë‹ˆë‹¤.", "í¸ì•ˆí•©ë‹ˆë‹¤.", "ì‹ ë‚©ë‹ˆë‹¤.",
        
        # ì§ˆë¬¸ê³¼ ì‘ë‹µ (ì™„ë²½í•œ ë¬¸ì¥)
        "ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?", "ë­ í•˜ì„¸ìš”?", "ì–´ë”” ê°€ì„¸ìš”?", "ì–¸ì œ ì˜¤ì„¸ìš”?",
        "ë„¤, ë§ìŠµë‹ˆë‹¤.", "ì•„ë‹ˆìš”, í‹€ë ¸ìŠµë‹ˆë‹¤.", "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.", "ì•Œê² ìŠµë‹ˆë‹¤.",
        
        # ë³µí•© ë¬¸ì¥ (ìì—°ìŠ¤ëŸ¬ìš´)
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ ì‚°ì±…ì„ í–ˆìŠµë‹ˆë‹¤.", "ì¹œêµ¬ì™€ í•¨ê»˜ ì˜í™”ë¥¼ ë´¤ìŠµë‹ˆë‹¤.",
        "ë„ì„œê´€ì—ì„œ ì±…ì„ ì½ì—ˆìŠµë‹ˆë‹¤.", "ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤.",
    ]
    
    teacher_model.eval()
    student_model.train()
    
    # ë‹¨ê³„ë³„ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=base_lr, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=base_lr*0.1)
    
    total_loss = 0.0
    best_loss = float('inf')
    patience_count = 0
    
    progress_bar = tqdm(range(total_steps), desc="ğŸ§  ì§‘ì¤‘ íŒŒì¸íŠœë‹")
    
    for step in progress_bar:
        # ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìˆœí™˜
        text = train_texts[step % len(train_texts)]
        inputs = tokenizer(text, return_tensors="pt", max_length=32, truncation=True, padding=True)
        
        if inputs.input_ids.shape[1] < 3:
            continue
            
        input_ids = inputs.input_ids
        labels = input_ids[:, 1:].clone()
        input_ids = input_ids[:, :-1]
        
        optimizer.zero_grad()
        
        # Teacherì™€ Student ì¶œë ¥
        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids)
        
        student_outputs = student_model(input_ids)
        
        # Knowledge Distillation Loss
        teacher_probs = F.softmax(teacher_outputs.logits / temperature, dim=-1)
        student_log_probs = F.log_softmax(student_outputs.logits / temperature, dim=-1)
        kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)
        
        # Language Model Loss  
        lm_loss = F.cross_entropy(
            student_outputs.logits.view(-1, student_outputs.logits.size(-1)), 
            labels.view(-1), 
            ignore_index=-100
        )
        
        # ê°€ì¤‘ ì†ì‹¤ (ì´ˆê¸°ì—ëŠ” KD ìœ„ì£¼, í›„ë°˜ì—ëŠ” LM ìœ„ì£¼)
        kd_weight = max(0.5, 0.9 - (step / total_steps) * 0.4)  # 0.9 â†’ 0.5
        lm_weight = 1 - kd_weight
        
        total_loss_step = kd_weight * kd_loss + lm_weight * lm_loss
        total_loss += total_loss_step.item()
        
        # ì—­ì „íŒŒ
        total_loss_step.backward()
        torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        
        # ì§„í–‰ ìƒí™© ë° ì¡°ê¸° ì¢…ë£Œ
        if step % 50 == 0:
            avg_loss = total_loss / (step + 1)
            current_lr = optimizer.param_groups[0]['lr']
            
            progress_bar.set_postfix({
                'avg_loss': f'{avg_loss:.4f}',
                'lr': f'{current_lr:.2e}',
                'kd_w': f'{kd_weight:.2f}',
                'lm_w': f'{lm_weight:.2f}'
            })
            
            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_count = 0
            else:
                patience_count += 1
                if patience_count > 200:  # 200 ìŠ¤í… ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                    print(f"\n   ì¡°ê¸° ì¢…ë£Œ: {step} ìŠ¤í…ì—ì„œ ìµœì í™” ì™„ë£Œ")
                    break
    
    print(f"   ìµœì¢… í‰ê·  ì†ì‹¤: {total_loss / (step + 1):.4f}")
    print("âœ… ì§‘ì¤‘ Knowledge Distillation ì™„ë£Œ!")
    
    return student_model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ì •í™•í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def accurate_test(model, tokenizer, model_type="ëª¨ë¸"):
    """ì •í™•í•œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (ê¹¨ì§„ í…ìŠ¤íŠ¸ ì—„ê²© ê°ì§€)"""
    
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”", 
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì´ë€",
        "ë§›ìˆëŠ” ìŒì‹ì€"
    ]
    
    print(f"\n=== {model_type} ì •í™•í•œ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ===")
    results = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n[{i}/5] '{prompt}'")
        
        try:
            t0 = time.time()
            
            # ë³´ìˆ˜ì  ìƒì„± (í’ˆì§ˆ ìš°ì„ )
            inputs = tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=25,
                    do_sample=True,
                    temperature=0.7,       # ë³´ìˆ˜ì  ì˜¨ë„
                    top_p=0.9,            # ë†’ì€ í™•ë¥  ìœ ì§€
                    top_k=50,             # ë” ë„“ì€ ì„ íƒ
                    repetition_penalty=1.5,  # ì ë‹¹í•œ ë°˜ë³µ ë°©ì§€
                    no_repeat_ngram_size=3,  # 3-gram ë°˜ë³µ ë°©ì§€
                    pad_token_id=tokenizer.eos_token_id,
                    min_length=len(inputs.input_ids[0]) + 3,  # ìµœì†Œ ê¸¸ì´
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            elapsed = time.time() - t0
            
            print(f"  ìƒì„±: {generated_text}")
            print(f"  ì‹œê°„: {elapsed:.3f}ì´ˆ")
            
            # ì •í™•í•œ í’ˆì§ˆ í‰ê°€
            quality_score = accurate_quality_evaluation(generated_text, prompt)
            print(f"  í’ˆì§ˆ: {quality_score:.2f}/3.0")
            
            # ê¹¨ì§„ í…ìŠ¤íŠ¸ ê°ì§€
            generated_only = generated_text[len(prompt):].strip()
            is_broken = bool(re.search(r'í‹°ì•„|í‹°ìŠ¤|ë¥´íŠ¸|ë³‘ì •|ì‚´ì„|ë² ì•„|ê´´ë¼|ëœí™€', generated_only))
            if is_broken:
                print(f"  âš ï¸ ê¹¨ì§„ í…ìŠ¤íŠ¸ ê°ì§€!")
            
            results.append({
                'prompt': prompt,
                'generated': generated_text,
                'time': elapsed,
                'quality': quality_score,
                'is_broken': is_broken
            })
            
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬: {e}")
            results.append({
                'prompt': prompt,
                'generated': f"ERROR: {e}",
                'time': 0,
                'quality': 0,
                'is_broken': True
            })
    
    # í†µê³„
    avg_time = sum(r['time'] for r in results) / len(results) if results else 0
    avg_quality = sum(r['quality'] for r in results) / len(results) if results else 0
    broken_count = sum(1 for r in results if r['is_broken'])
    
    print(f"\nğŸ“Š {model_type} ì •í™•í•œ í†µê³„:")
    print(f"  í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ")
    print(f"  í‰ê·  í’ˆì§ˆ: {avg_quality:.2f}/3.0")
    print(f"  ê¹¨ì§„ í…ìŠ¤íŠ¸: {broken_count}/5ê°œ")
    print(f"  ì„±ê³µë¥ : {(5-broken_count)/5*100:.0f}%")
    
    return results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ë©”ì¸ í•¨ìˆ˜ (ì •í™•ë„ ìµœìš°ì„ )
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    model_name = "skt/kogpt2-base-v2"
    print("ğŸ¯ ì •í™•ë„ ìš°ì„  ë³´ìˆ˜ì  ì••ì¶• ì‹œìŠ¤í…œ v1.0")
    print("=" * 60)
    print("ğŸ”§ ëª©í‘œ: í’ˆì§ˆ ìœ ì§€ + ì ë‹¹í•œ ì••ì¶• + ê¹¨ì§„ í…ìŠ¤íŠ¸ ë°©ì§€")
    print("Loading modelâ€¦")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    teacher_model = AutoModelForCausalLM.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # 1ë‹¨ê³„: ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n" + "="*60)
    print("ğŸ“Š ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    original_results = accurate_test(teacher_model, tokenizer, "ì›ë³¸")

    # 2ë‹¨ê³„: ë³´ìˆ˜ì  ì••ì¶•
    print("\n" + "="*60)
    print("ğŸ”§ ë³´ìˆ˜ì  ì••ì¶• ì ìš© (í’ˆì§ˆ ìš°ì„ )")
    
    student_model = copy.deepcopy(teacher_model)
    student_model = apply_conservative_compression(
        student_model, 
        compression_ratio=0.25  # 25% ì••ì¶• (75% íŒŒë¼ë¯¸í„° ìœ ì§€)
    )
    
    # 3ë‹¨ê³„: ì••ì¶• ì§í›„ í…ŒìŠ¤íŠ¸
    print("\n" + "="*60)
    print("ğŸ“Š ì••ì¶• ì§í›„ í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
    compressed_results = accurate_test(student_model, tokenizer, "ì••ì¶•í›„")
    
    # 4ë‹¨ê³„: ì§‘ì¤‘ íŒŒì¸íŠœë‹
    print("\n" + "="*60)
    print("ğŸ§  ì§‘ì¤‘ Knowledge Distillation íŒŒì¸íŠœë‹")
    student_model = intensive_knowledge_distillation(
        teacher_model, student_model, tokenizer,
        total_steps=1500,  # 1500 ìŠ¤í…
        base_lr=5e-6,      # ë‚®ì€ í•™ìŠµë¥ 
        temperature=2.5    # ì ë‹¹í•œ ì˜¨ë„
    )
    
    # 5ë‹¨ê³„: ìµœì¢… í…ŒìŠ¤íŠ¸
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… í’ˆì§ˆ í‰ê°€")
    final_results = accurate_test(student_model, tokenizer, "ìµœì¢…")
    
    # 6ë‹¨ê³„: ì¢…í•© ë¶„ì„
    print("\n" + "="*60)
    print("ğŸ† ì •í™•ë„ ìš°ì„  ì••ì¶• ìµœì¢… ë¶„ì„")
    print("="*60)
    
    # ì„±ëŠ¥ ì§€í‘œ
    orig_quality = sum(r['quality'] for r in original_results) / len(original_results)
    orig_broken = sum(1 for r in original_results if r['is_broken'])
    
    comp_quality = sum(r['quality'] for r in compressed_results) / len(compressed_results)
    comp_broken = sum(1 for r in compressed_results if r['is_broken'])
    
    final_quality = sum(r['quality'] for r in final_results) / len(final_results)
    final_broken = sum(1 for r in final_results if r['is_broken'])
    
    # ì••ì¶• í†µê³„
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    student_params = sum(p.numel() for p in student_model.parameters())
    compression_ratio = student_params / teacher_params
    
    print(f"ğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
    print(f"   ì›ë³¸:     í’ˆì§ˆ {orig_quality:.2f}, ê¹¨ì§„ í…ìŠ¤íŠ¸ {orig_broken}/5")
    print(f"   ì••ì¶•í›„:   í’ˆì§ˆ {comp_quality:.2f}, ê¹¨ì§„ í…ìŠ¤íŠ¸ {comp_broken}/5")  
    print(f"   ìµœì¢…:     í’ˆì§ˆ {final_quality:.2f}, ê¹¨ì§„ í…ìŠ¤íŠ¸ {final_broken}/5")
    
    print(f"\nğŸ“ˆ ê°œì„  íš¨ê³¼:")
    quality_retention = final_quality / orig_quality
    improvement = final_quality - comp_quality
    print(f"   í’ˆì§ˆ ìœ ì§€ìœ¨: {quality_retention*100:.1f}%")
    print(f"   íŒŒì¸íŠœë‹ ê°œì„ : +{improvement:.2f}ì ")
    print(f"   í…ìŠ¤íŠ¸ ë³µêµ¬: {comp_broken} â†’ {final_broken} ê¹¨ì§„ í…ìŠ¤íŠ¸")
    
    print(f"\nğŸ’¾ ì••ì¶• ì„±ê³¼:")
    print(f"   íŒŒë¼ë¯¸í„°: {teacher_params:,} â†’ {student_params:,}")
    print(f"   ì••ì¶• ë¹„ìœ¨: {compression_ratio:.1%} ({1/compression_ratio:.1f}Ã— ì••ì¶•)")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {(1-compression_ratio)*100:.1f}%")
    
    # ìµœì¢… íŒì •
    if final_broken == 0 and final_quality >= orig_quality * 0.9:
        grade = "ğŸ† ì„±ê³µ! (Aê¸‰)"
        message = "ê¹¨ì§„ í…ìŠ¤íŠ¸ ì—†ì´ 90%+ í’ˆì§ˆ ìœ ì§€!"
    elif final_broken <= 1 and final_quality >= orig_quality * 0.8:
        grade = "ğŸ¥‡ ì–‘í˜¸ (Bê¸‰)"  
        message = "ëŒ€ë¶€ë¶„ ì •ìƒ í…ìŠ¤íŠ¸, 80%+ í’ˆì§ˆ ìœ ì§€"
    elif final_broken <= 2 and final_quality >= orig_quality * 0.7:
        grade = "ğŸ¥ˆ ë³´í†µ (Cê¸‰)"
        message = "ì¼ë¶€ ê°œì„  íš¨ê³¼ ìˆìŒ"
    else:
        grade = "ğŸ”§ ê°œì„  í•„ìš” (Dê¸‰)"
        message = "ì¶”ê°€ ìµœì í™” í•„ìš”"
    
    print(f"\n{grade}: {message}")
    
    if final_broken > 0:
        print(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: ì••ì¶•ë¥ ì„ ë” ë‚®ì¶”ê±°ë‚˜ íŒŒì¸íŠœë‹ ì—°ì¥")
    if quality_retention < 0.85:
        print(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: ë” ë³´ìˆ˜ì ì¸ ì••ì¶• ì „ëµ ì ìš©")
    
    print(f"\nğŸŒŸ ìµœì¢… ê²°ë¡ :")
    print(f"   ë³´ìˆ˜ì  ì••ì¶•ìœ¼ë¡œ {(1-compression_ratio)*100:.0f}% ë©”ëª¨ë¦¬ ì ˆì•½í•˜ë©´ì„œ")
    print(f"   ì›ë³¸ í’ˆì§ˆì˜ {quality_retention*100:.0f}%ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.")
    if final_broken == 0:
        print(f"   âœ… ê¹¨ì§„ í…ìŠ¤íŠ¸ ì™„ì „ ë°©ì§€ ì„±ê³µ!")

if __name__ == "__main__":
    main() 