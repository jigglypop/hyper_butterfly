import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import reality_stone as rs
import gc

class SinglePoincareBallLinear(nn.Module):
    def __init__(self, in_features: int, out_features: int, curvature: float = 1.0, bias: bool = True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.curvature = curvature
        self.weight = nn.Parameter(torch.randn(out_features, in_features) * 0.1)
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)
            
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.weight.device != x.device:
            self.weight.data = self.weight.data.to(x.device)
            if self.bias is not None:
                self.bias.data = self.bias.data.to(x.device)
                
        linear_out = F.linear(x, self.weight, self.bias)
        
        try:
            if x.is_cuda and hasattr(rs, 'poincare_ball_forward_cuda'):
                x_norm = torch.norm(x, dim=-1, keepdim=True)
                out_norm = torch.norm(linear_out, dim=-1, keepdim=True)
                scale = 0.01
                
                x_safe = x * torch.tanh(x_norm * scale) / (x_norm + 1e-8)
                out_safe = linear_out * torch.tanh(out_norm * scale) / (out_norm + 1e-8)
                
                hyperbolic_out = rs.poincare_ball_forward_cuda(x_safe, out_safe, self.curvature, 0.01)
                hyp_norm = torch.norm(hyperbolic_out, dim=-1, keepdim=True)
                result = hyperbolic_out * out_norm / (hyp_norm + 1e-8)
                
                return 0.99 * linear_out + 0.01 * result
            else:
                return linear_out
        except:
            return linear_out

def replace_linear_layers_inplace(model: nn.Module, curvature: float = 1.0):
    total_replaced = 0
    
    for name, module in model.named_modules():
        if hasattr(module, 'c_attn') and hasattr(module.c_attn, 'weight'):
            old_layer = module.c_attn
            if hasattr(old_layer, 'nf'):
                in_features = old_layer.weight.shape[0]
                out_features = old_layer.weight.shape[1]
                new_layer = SinglePoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data.t())
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            else:
                out_features, in_features = old_layer.weight.shape
                new_layer = SinglePoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data)
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ êµì²´
            del module.c_attn
            torch.cuda.empty_cache()
            module.c_attn = new_layer
            total_replaced += 1
            
        if hasattr(module, 'c_proj') and hasattr(module.c_proj, 'weight'):
            old_layer = module.c_proj
            if hasattr(old_layer, 'nf'):
                in_features = old_layer.weight.shape[0]
                out_features = old_layer.weight.shape[1]
                new_layer = SinglePoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data.t())
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            else:
                out_features, in_features = old_layer.weight.shape
                new_layer = SinglePoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data)
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            
            del module.c_proj
            torch.cuda.empty_cache()
            module.c_proj = new_layer
            total_replaced += 1
            
        if hasattr(module, 'c_fc') and hasattr(module.c_fc, 'weight'):
            old_layer = module.c_fc
            if hasattr(old_layer, 'nf'):
                in_features = old_layer.weight.shape[0]
                out_features = old_layer.weight.shape[1]
                new_layer = SinglePoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data.t())
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            else:
                out_features, in_features = old_layer.weight.shape
                new_layer = SinglePoincareBallLinear(in_features, out_features, curvature, bias=(old_layer.bias is not None))
                with torch.no_grad():
                    new_layer.weight.data.copy_(old_layer.weight.data)
                    if new_layer.bias is not None and old_layer.bias is not None:
                        new_layer.bias.data.copy_(old_layer.bias.data)
            
            del module.c_fc
            torch.cuda.empty_cache()
            module.c_fc = new_layer
            total_replaced += 1
    
    print(f"ì´ {total_replaced}ê°œ ë ˆì´ì–´ êµì²´ ì™„ë£Œ")
    return model

def create_inplace_poincare_model(teacher_model: nn.Module, curvature: float = 1.0):
    """ë”¥ì¹´í”¼ ì—†ì´ in-placeë¡œ Poincare ëª¨ë¸ ìƒì„±"""
    print("In-place í¬ì¸ì¹´ë ˆ ë³€í™˜ ì‹œì‘... (ë”¥ì¹´í”¼ ì—†ìŒ)")
    student = replace_linear_layers_inplace(teacher_model, curvature)
    return student

def test_model(model, tokenizer, device, prompts, model_name, max_length=50):
    model.to(device).eval()
    results = []
    total_time = 0.0
    
    print(f"\n=== {model_name} í…ŒìŠ¤íŠ¸ ===")
    for idx, prompt in enumerate(prompts, 1):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        start = time.time()
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=max_length, do_sample=False, temperature=1.0, top_p=1.0, top_k=0, pad_token_id=tokenizer.eos_token_id)
        elapsed = time.time() - start
        gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        total_time += elapsed
        print(f"[{idx}] '{prompt}' -> {gen_text} ({elapsed:.3f}s)")
        results.append((prompt, gen_text, elapsed))
    
    avg_time = total_time / len(prompts)
    print(f"{model_name} í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ")
    return results, avg_time

def test_korean_generation_safe(model, tokenizer, device, model_name):
    """ì•ˆì „í•œ í•œê¸€ ìƒì„± í…ŒìŠ¤íŠ¸ (CUDA ì˜¤ë¥˜ ë°©ì§€)"""
    model.to(device).eval()
    korean_prompts = [
        "í•œêµ­ì˜ ì•„ë¦„ë‹¤ìš´ ê³³ì€",
        "ì˜¤ëŠ˜ì€ ì¢‹ì€ ë‚ ì…ë‹ˆë‹¤",
        "ë§›ìˆëŠ” í•œêµ­ ìŒì‹ì€"
    ]
    
    print(f"\n=== {model_name} í•œê¸€ ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ===")
    for idx, prompt in enumerate(korean_prompts, 1):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        try:
            with torch.no_grad():
                # ë” ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ìƒì„±
                outputs = model.generate(
                    **inputs, 
                    max_length=60, 
                    do_sample=True, 
                    temperature=0.7, 
                    top_p=0.8, 
                    top_k=40, 
                    pad_token_id=tokenizer.eos_token_id,
                    early_stopping=True
                )
            gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"[{idx}] {prompt}")
            print(f"    {gen_text}")
        except Exception as e:
            print(f"[{idx}] {prompt}")
            print(f"    ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("-" * 50)

def precise_memory_measure(device, label=""):
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        gc.collect()
        torch.cuda.synchronize()
        memory_mb = torch.cuda.memory_allocated() / 1024**2
        print(f"{label}: {memory_mb:.1f} MB")
        return memory_mb
    return 0.0

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_name = "skt/kogpt2-base-v2"
    curvature = 1.0
    
    print("ë”¥ì¹´í”¼ ì—†ëŠ” RealityStone Poincare Ball í…ŒìŠ¤íŠ¸")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 1ë‹¨ê³„: ì›ë³¸ ëª¨ë¸ ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì¸¡ì •
    print("\n=== ë©”ëª¨ë¦¬ ì¶”ì  ===")
    initial_memory = precise_memory_measure(device, "ì´ˆê¸° ìƒíƒœ")
    
    teacher = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    after_load_memory = precise_memory_measure(device, "ì›ë³¸ ëª¨ë¸ ë¡œë“œ í›„")
    
    # 2ë‹¨ê³„: In-placeë¡œ Poincare ëª¨ë¸ ìƒì„± (ë”¥ì¹´í”¼ ì—†ìŒ!)
    student = create_inplace_poincare_model(teacher, curvature)
    after_conversion_memory = precise_memory_measure(device, "í¬ì¸ì¹´ë ˆ ë³€í™˜ í›„")
    
    print(f"\në©”ëª¨ë¦¬ ë³€í™”:")
    print(f"  ì›ë³¸ ë¡œë“œ: +{after_load_memory - initial_memory:.1f} MB")
    print(f"  í¬ì¸ì¹´ë ˆ ë³€í™˜: {after_conversion_memory - after_load_memory:+.1f} MB")
    print(f"  ìµœì¢… ë©”ëª¨ë¦¬ ë¹„ìœ¨: {after_conversion_memory / after_load_memory:.3f}")
    
    # 3ë‹¨ê³„: ë¹„êµ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì›ë³¸ ëª¨ë¸ ìƒˆë¡œ ë¡œë“œ
    print(f"\në¹„êµ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì›ë³¸ ëª¨ë¸ ìƒˆë¡œ ë¡œë“œ...")
    teacher_for_comparison = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    comparison_memory = precise_memory_measure(device, "ë¹„êµìš© ì›ë³¸ ë¡œë“œ í›„")
    
    # 4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    prompts = ["ì•ˆë…•í•˜ì„¸ìš”", "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”", "í•œêµ­ì˜ ìˆ˜ë„ëŠ”", "ì¸ê³µì§€ëŠ¥ì´ë€", "ë§›ìˆëŠ” ìŒì‹ì€"]
    
    print(f"\n" + "="*60)
    print("ìµœì¢… ì›ë³¸ vs Poincare ëª¨ë¸ ë¹„êµ")
    print("="*60)
    
    orig_results, orig_time = test_model(teacher_for_comparison, tokenizer, device, prompts, "ì›ë³¸")
    poincare_results, poincare_time = test_model(student, tokenizer, device, prompts, "Poincare")
    
    # 5ë‹¨ê³„: ê²°ê³¼ ë¶„ì„
    print(f"\n" + "="*60)
    print("ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("="*60)
    
    speed_ratio = poincare_time / orig_time
    final_memory_ratio = after_conversion_memory / after_load_memory
    
    print(f"ì†ë„ ë¹„ìœ¨: {speed_ratio:.3f} (ì›ë³¸ ëŒ€ë¹„)")
    print(f"ë©”ëª¨ë¦¬ ë¹„ìœ¨: {final_memory_ratio:.3f} (ë”¥ì¹´í”¼ ì—†ìŒ!)")
    
    exact_output_matches = 0
    for i, (o, p) in enumerate(zip(orig_results, poincare_results), 1):
        if o[1] == p[1]:
            print(f"[{i}] ì¶œë ¥ ì¼ì¹˜")
            exact_output_matches += 1
        else:
            print(f"[{i}] ì¶œë ¥ ë¶ˆì¼ì¹˜")
            print(f"    ì›ë³¸: {o[1]}")
            print(f"    Poincare: {p[1]}")
    
    output_match_rate = exact_output_matches / len(prompts)
    print(f"ì¶œë ¥ ì¼ì¹˜ìœ¨: {output_match_rate:.1%}")
    
    # 6ë‹¨ê³„: ì•ˆì „í•œ í•œê¸€ ìƒì„± í…ŒìŠ¤íŠ¸
    test_korean_generation_safe(teacher_for_comparison, tokenizer, device, "ì›ë³¸")
    test_korean_generation_safe(student, tokenizer, device, "Poincare")
    
    # 7ë‹¨ê³„: ìµœì¢… ê²°ë¡ 
    print(f"\n" + "="*60)
    print("ìµœì¢… ê²°ë¡ ")
    print("="*60)
    
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:")
    if final_memory_ratio < 1.2:
        print("  âœ… ë©”ëª¨ë¦¬ ìµœì í™” ëŒ€ì„±ê³µ! (ë”¥ì¹´í”¼ ë¬¸ì œ í•´ê²°)")
    elif final_memory_ratio < 1.5:
        print("  ğŸŸ¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ë‹¹íˆ ì¦ê°€")
    else:
        print("  âŒ ì—¬ì „íˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ")
    
    print(f"âš¡ ì†ë„ ì„±ëŠ¥:")
    if speed_ratio < 2.0:
        print("  âœ… ì†ë„ ìµœì í™” ì„±ê³µ!")
    elif speed_ratio < 3.0:
        print("  ğŸŸ¡ ì†ë„ ì €í•˜ ìˆì§€ë§Œ í—ˆìš© ë²”ìœ„")
    else:
        print("  âŒ ì†ë„ ì €í•˜ ì‹¬ê°")
    
    print(f"ğŸ¯ ì •í™•ë„:")
    if output_match_rate == 1.0:
        print("  âœ… 100% ì •í™•ë„ ìœ ì§€")
    elif output_match_rate >= 0.8:
        print("  ğŸŸ¡ ë†’ì€ ì •í™•ë„ ìœ ì§€")
    else:
        print("  âŒ ì •í™•ë„ ì €í•˜")
    
    # ë©”ëª¨ë¦¬ ì ˆì•½ëŸ‰ ê³„ì‚°
    if final_memory_ratio < 1.5:
        original_ratio = 1.984  # ì´ì „ ë”¥ì¹´í”¼ ë²„ì „
        improvement = ((original_ratio - final_memory_ratio) / original_ratio) * 100
        print(f"\nğŸš€ ë”¥ì¹´í”¼ ì œê±°ë¡œ ë©”ëª¨ë¦¬ {improvement:.1f}% ì ˆì•½!")

if __name__ == "__main__":
    main() 