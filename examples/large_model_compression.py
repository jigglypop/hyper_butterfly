#!/usr/bin/env python3
"""
ğŸš€ ëŒ€í˜• ëª¨ë¸ìš© RealityStone ì••ì¶• ì‹œìŠ¤í…œ v9.1
ê¸°ì¡´ ì„±ê³µí•œ ë¦¬ë§Œ+FFT+SVD ë¡œì§ì„ ëŒ€í˜• ëª¨ë¸ë¡œ í™•ì¥
"""

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import time
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
import re
from collections import Counter
import copy
import gc
import os

# ê¸°ì¡´ ì„±ê³µí•œ ì••ì¶• ì‹œìŠ¤í…œ ì„í¬íŠ¸ (ìƒëŒ€ ê²½ë¡œë¡œ ìˆ˜ì •)
from reality_stone.examples.fftsvds import (
    enhanced_stereographic_projection,
    enhanced_inverse_stereographic_projection, 
    advanced_riemann_distance,
    advanced_mobius_transform,
    FastSVDCompressor,
    AdvancedFFTSVDCompressor,
    EnhancedRealityStoneLinear,
    SimplifiedRiemannCompressor,
    ultra_knowledge_distillation_fine_tune,
    generate_with_anti_repetition,
    advanced_quality_evaluation,
    RS_AVAILABLE
)

print("âœ… ê¸°ì¡´ ì„±ê³µí•œ RealityStone ì••ì¶• ì‹œìŠ¤í…œ ë¡œë“œ!")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ëŒ€í˜• ëª¨ë¸ ì§€ì› ëª©ë¡ (ê¸°ì¡´ê³¼ ë™ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUPPORTED_LARGE_MODELS = {
    "EleutherAI/polyglot-ko-1.3b": {
        "size": "1.3B",
        "params": 1_300_000_000, 
        "description": "í•œêµ­ì–´ Polyglot 1.3B",
        "compression_target": 0.05,  # ê·¹í•œ 5% ì••ì¶•!
        "memory_gb": 6
    },
    "microsoft/DialoGPT-medium": {
        "size": "345M",
        "params": 345_000_000,
        "description": "DialoGPT Medium",
        "compression_target": 0.08,  # 8% ì••ì¶•
        "memory_gb": 2
    },
    "beomi/KoAlpaca-Polyglot-5.8B": {
        "size": "5.8B", 
        "params": 5_800_000_000,
        "description": "í•œêµ­ì–´ Alpaca 5.8B",
        "compression_target": 0.15,  # ë³´ìˆ˜ì  15%
        "memory_gb": 23
    },
    "EleutherAI/gpt-j-6b": {
        "size": "6B", 
        "params": 6_000_000_000,
        "description": "GPT-J 6B",
        "compression_target": 0.20,  # ë§¤ìš° ë³´ìˆ˜ì 
        "memory_gb": 24
    }
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ ëŒ€í˜• ëª¨ë¸ìš© RealityStone ì••ì¶• ë ˆì´ì–´ (ê¸°ì¡´ ë¡œì§ í™•ì¥)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LargeModelRealityStoneLinear(nn.Module):
    """ëŒ€í˜• ëª¨ë¸ìš© RealityStone Linear (ê°„ë‹¨í•œ SVD ì••ì¶•)"""
    
    def __init__(self, lin, compression_ratio=0.05, compression_type='auto', model_dtype=torch.float32):
        super().__init__()
        
        if hasattr(lin, 'weight'):
            W = lin.weight.data.clone().float()  # ë¬´ì¡°ê±´ float32ë¡œ ë³€í™˜
            
            # ë ˆì´ì–´ ì •ë³´ ì¶”ì¶œ
            if hasattr(lin, 'nf'):  # Conv1D
                self.in_features = W.shape[1]
                self.out_features = W.shape[0]
                W = W.t()
                layer_type = "Conv1D"
            else:  # nn.Linear
                self.in_features = lin.in_features
                self.out_features = lin.out_features  
                layer_type = "Linear"
            
            param_count = W.numel()
            print(f"ğŸ”— {layer_type} ê°„ë‹¨ì••ì¶•: {W.shape} ({param_count/1e6:.1f}M íŒŒë¼ë¯¸í„°)")
            
            # ê°„ë‹¨í•œ SVD ì••ì¶• (ë°ì´í„° íƒ€ì… ì•ˆì „)
            U, S, V = torch.svd(W.float())
            rank = max(8, int(min(W.shape) * compression_ratio * 2))
            rank = min(rank, len(S))
            
            # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ì €ì¥ (float32)
            self.U = nn.Parameter(U[:, :rank].float())
            self.S = nn.Parameter(S[:rank].float())
            self.V = nn.Parameter(V[:, :rank].float())
            
            print(f"       âœ… SVD ì••ì¶•ì™„ë£Œ: rank {rank}")
            
            # ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
            if hasattr(lin, 'bias') and lin.bias is not None:
                self.bias = nn.Parameter(lin.bias.data.clone().float())
            else:
                self.bias = None
                
            print(f"     âœ… ì••ì¶• ì™„ë£Œ (float32)")
        else:
            raise ValueError("Input layer must have weight attribute")

    def forward(self, x):
        # ëª¨ë“  ê³„ì‚°ì„ float32ë¡œ í†µì¼
        x = x.float()
        # SVD ì—°ì‚°: x @ V @ diag(S) @ U.t()
        result = x @ self.V @ torch.diag(self.S) @ self.U.t()
        if self.bias is not None:
            result = result + self.bias
        return result.float()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ëŒ€í˜• ëª¨ë¸ìš© RealityStone ë¸”ë¡ (ê¸°ì¡´ ë¡œì§ í™•ì¥)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LargeModelRealityStoneBlock(nn.Module):
    """ëŒ€í˜• ëª¨ë¸ìš© RealityStone ë¸”ë¡ (ë°ì´í„° íƒ€ì… í†µì¼)"""
    
    def __init__(self, block, compression_ratio=0.05, layer_idx=0, total_layers=32,
                 model_type='gpt_neox', model_dtype=torch.float16):
        super().__init__()
        
        self.model_type = model_type
        self.model_dtype = model_dtype
        
        # ëª¨ë¸ íƒ€ì…ë³„ êµ¬ì¡° ì²˜ë¦¬
        if model_type == 'gpt_neox':
            self._compress_gpt_neox_block(block, compression_ratio, layer_idx, total_layers)
        elif model_type == 'gpt2':
            self._compress_gpt2_block(block, compression_ratio, layer_idx, total_layers)
        else:
            self._compress_generic_block(block, compression_ratio, layer_idx, total_layers)
    
    def _compress_gpt_neox_block(self, block, compression_ratio, layer_idx, total_layers):
        """GPT-NeoX êµ¬ì¡° ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
        
        # ì ì‘ì  ì••ì¶•ë¥  ê³„ì‚° (ê¸°ì¡´ ë¡œì§ í™œìš©)
        normalized_idx = layer_idx / total_layers
        
        if normalized_idx < 0.2:  # ì´ˆê¸°ì¸µ
            layer_ratio = compression_ratio * 1.5
        elif normalized_idx < 0.8:  # ì¤‘ê°„ì¸µ - ê·¹í•œ ì••ì¶•
            layer_ratio = compression_ratio * 0.5
        else:  # ë§ë‹¨ì¸µ
            layer_ratio = compression_ratio * 1.2
        
        print(f"ğŸŒ GPT-NeoX ë ˆì´ì–´ {layer_idx}: ê·¹í•œì••ì¶•ë¥  {layer_ratio:.1%}")
        
        # ê° ì„œë¸Œëª¨ë“ˆ ì••ì¶• (ë°ì´í„° íƒ€ì… ì „ë‹¬)
        if hasattr(block, 'input_layernorm'):
            self.input_layernorm = block.input_layernorm
        if hasattr(block, 'post_attention_layernorm'): 
            self.post_attention_layernorm = block.post_attention_layernorm
            
        # Attention ì••ì¶•
        if hasattr(block, 'attention'):
            self.attention = self._compress_attention_module(
                block.attention, layer_ratio
            )
        
        # MLP ì••ì¶•
        if hasattr(block, 'mlp'):
            self.mlp = self._compress_mlp_module(
                block.mlp, layer_ratio
            )
    
    def _compress_gpt2_block(self, block, compression_ratio, layer_idx, total_layers):
        """GPT-2 êµ¬ì¡° ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
        
        # ê¸°ì¡´ ì„±ê³µí•œ EnhancedRealityStoneBlock ë¡œì§ í™œìš©
        self.ln1 = block.ln_1
        self.ln2 = block.ln_2
        attn, mlp = block.attn, block.mlp

        # ì ì‘ì  ì••ì¶•ë¥ 
        normalized_idx = layer_idx / total_layers
        if normalized_idx < 0.3:
            layer_ratio = compression_ratio * 1.5
        elif normalized_idx < 0.7:
            layer_ratio = compression_ratio * 0.5  # ê·¹í•œ
        else:
            layer_ratio = compression_ratio * 1.2

        print(f"ğŸŒ GPT-2 ë ˆì´ì–´ {layer_idx}: ê·¹í•œì••ì¶•ë¥  {layer_ratio:.1%}")

        # ì„œë¸Œë ˆì´ì–´ ì••ì¶• (ë°ì´í„° íƒ€ì… ì „ë‹¬)
        attn.c_attn = LargeModelRealityStoneLinear(attn.c_attn, layer_ratio, 'auto', self.model_dtype)
        attn.c_proj = LargeModelRealityStoneLinear(attn.c_proj, layer_ratio, 'auto', self.model_dtype)
        mlp.c_fc = LargeModelRealityStoneLinear(mlp.c_fc, layer_ratio, 'auto', self.model_dtype)
        mlp.c_proj = LargeModelRealityStoneLinear(mlp.c_proj, layer_ratio, 'auto', self.model_dtype)
        
        self.attn, self.mlp = attn, mlp
    
    def _compress_generic_block(self, block, compression_ratio, layer_idx, total_layers):
        """ì¼ë°˜ì ì¸ ë¸”ë¡ ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
        # ëª¨ë“  Linear ë ˆì´ì–´ ì°¾ì•„ì„œ ì••ì¶•
        for name, module in block.named_children():
            if isinstance(module, nn.Linear):
                compressed_module = LargeModelRealityStoneLinear(
                    module, compression_ratio, 'auto', self.model_dtype
                )
                setattr(self, name, compressed_module)
            else:
                setattr(self, name, module)
    
    def _compress_attention_module(self, attention, compression_ratio):
        """ì–´í…ì…˜ ëª¨ë“ˆ ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
        
        # query_key_value ì••ì¶• (ì¼ë°˜ì ì¸ êµ¬ì¡°)
        if hasattr(attention, 'query_key_value'):
            attention.query_key_value = LargeModelRealityStoneLinear(
                attention.query_key_value, compression_ratio, 'riemann', self.model_dtype
            )
        
        # dense ì••ì¶•
        if hasattr(attention, 'dense'):
            attention.dense = LargeModelRealityStoneLinear(
                attention.dense, compression_ratio, 'fast_svd', self.model_dtype
            )
        
        return attention
    
    def _compress_mlp_module(self, mlp, compression_ratio):
        """MLP ëª¨ë“ˆ ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
        
        # dense_h_to_4h ì••ì¶•
        if hasattr(mlp, 'dense_h_to_4h'):
            mlp.dense_h_to_4h = LargeModelRealityStoneLinear(
                mlp.dense_h_to_4h, compression_ratio, 'fft_svd', self.model_dtype
            )
        
        # dense_4h_to_h ì••ì¶•
        if hasattr(mlp, 'dense_4h_to_h'):
            mlp.dense_4h_to_h = LargeModelRealityStoneLinear(
                mlp.dense_4h_to_h, compression_ratio, 'riemann', self.model_dtype
            )
        
        return mlp
    
    def forward(self, x, **kwargs):
        """ìˆœì „íŒŒ (ëª¨ë¸ íƒ€ì…ë³„)"""
        
        # ì…ë ¥ì„ float32ë¡œ ë³€í™˜ (ë°ì´í„° íƒ€ì… í†µì¼)
        x = x.float()
        
        if self.model_type == 'gpt_neox':
            # GPT-NeoX ìˆœì „íŒŒ
            h = self.input_layernorm(x)
            attn_outputs = self.attention(h, **kwargs)
            if isinstance(attn_outputs, tuple):
                a = attn_outputs[0]
            else:
                a = attn_outputs
            x = x + a
            
            h2 = self.post_attention_layernorm(x)
            m = self.mlp(h2)
            output = x + m
            
            return (output,)
            
        elif self.model_type == 'gpt2':
            # GPT-2 ìˆœì „íŒŒ (ê¸°ì¡´ ì„±ê³µ ë¡œì§)
            h = self.ln1(x)
            attn_outputs = self.attn(h, **kwargs)
            a = attn_outputs[0]
            x = x + a
            h2 = self.ln2(x)
            m = self.mlp(h2)
            output = x + m
            
            if len(attn_outputs) > 1:
                return (output,) + attn_outputs[1:]
            else:
                return (output,)
        else:
            # ì¼ë°˜ì ì¸ ìˆœì „íŒŒ
            return x

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ ëŒ€í˜• ëª¨ë¸ ì••ì¶• íŒŒì´í”„ë¼ì¸ (ê¸°ì¡´ ì„±ê³µ ë¡œì§ í™•ì¥)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def detect_model_architecture(model):
    """ëª¨ë¸ ì•„í‚¤í…ì²˜ ìë™ ê°ì§€"""
    
    config = model.config
    
    if hasattr(config, 'model_type'):
        if config.model_type == 'gpt_neox':
            return 'gpt_neox', len(model.gpt_neox.layers)
        elif config.model_type in ['gpt2', 'kogpt2']:
            return 'gpt2', len(model.transformer.h)
        elif config.model_type == 'llama':
            return 'llama', len(model.model.layers)
    
    # í´ë°±: êµ¬ì¡° ê¸°ë°˜ ê°ì§€
    if hasattr(model, 'gpt_neox'):
        return 'gpt_neox', len(model.gpt_neox.layers)
    elif hasattr(model, 'transformer'):
        return 'gpt2', len(model.transformer.h)
    elif hasattr(model, 'model'):
        return 'llama', len(model.model.layers)
    
    return 'unknown', 0

def apply_large_model_reality_stone_compression(model, compression_ratio=0.05, 
                                               strategy='adaptive'):
    """ëŒ€í˜• ëª¨ë¸ìš© RealityStone ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
    
    total_before = sum(p.numel() for p in model.parameters())
    
    # ëª¨ë¸ ë°ì´í„° íƒ€ì… í™•ì¸ ë° í†µì¼
    model_dtype = next(model.parameters()).dtype
    print(f"ğŸ”§ ëª¨ë¸ ë°ì´í„° íƒ€ì…: {model_dtype}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.cuda.is_available():
        device = "cuda"
        model = model.cuda()
    else:
        device = "cpu"
        model = model.cpu()
    
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    
    print(f"ğŸ”¥ ëŒ€í˜• ëª¨ë¸ RealityStone ì••ì¶• ì‹œì‘")
    print(f"   Before: {total_before:,} params ({total_before/1e9:.2f}B)")
    
    # ë°ì´í„° íƒ€ì… ë¬¸ì œ í•´ê²°: ëª¨ë¸ ì „ì²´ë¥¼ float32ë¡œ ë³€í™˜
    if model_dtype != torch.float32:
        print(f"   ğŸ”§ ë°ì´í„° íƒ€ì… í†µì¼: {model_dtype} â†’ float32")
        model = model.float()
        model_dtype = torch.float32
    
    print(f"   ì••ì¶•ë¥ : {compression_ratio:.1%} (ëª©í‘œ: {(1-compression_ratio)*100:.0f}% ì ˆì•½)")
    print(f"   ì „ëµ: {strategy}")
    print(f"   ğŸ’ ê¸°ë²•: ë¦¬ë§Œê¸°í•˜í•™ + FFT+SVD + RealityStone")
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°ì§€
    model_type, total_layers = detect_model_architecture(model)
    print(f"   ğŸ—ï¸ ê°ì§€ëœ êµ¬ì¡°: {model_type} ({total_layers} ë ˆì´ì–´)")
    
    # êµ¬ì¡°ë³„ ì••ì¶• ì ìš© (ë°ì´í„° íƒ€ì… ì „ë‹¬)
    if model_type == 'gpt_neox':
        compressed_count = apply_gpt_neox_compression(
            model, compression_ratio, strategy, total_layers, model_dtype
        )
    elif model_type == 'gpt2':
        compressed_count = apply_gpt2_compression(
            model, compression_ratio, strategy, total_layers, model_dtype
        )
    else:
        print(f"   âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” êµ¬ì¡°: {model_type}")
        return model
    
    # ì••ì¶• ê²°ê³¼
    total_after = sum(p.numel() for p in model.parameters())
    actual_compression = total_after / total_before
    memory_saved = (1 - actual_compression) * 100
    
    print(f"\nâœ… ëŒ€í˜• ëª¨ë¸ ì••ì¶• ì™„ë£Œ!")
    print(f"   After:  {total_after:,} params ({total_after/1e9:.2f}B)")
    print(f"   ì••ì¶•ë¥ : {actual_compression:.1%} ({1/actual_compression:.1f}Ã— ì••ì¶•)")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
    print(f"   ì„±ê³µ ë ˆì´ì–´: {compressed_count}/{total_layers}")
    
    return model

def apply_gpt_neox_compression(model, compression_ratio, strategy, total_layers, model_dtype):
    """GPT-NeoX êµ¬ì¡° ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
    
    layers = model.gpt_neox.layers
    
    # ì „ëµë³„ ë ˆì´ì–´ ì„ íƒ (ê¸°ì¡´ ë¡œì§)
    if strategy == 'adaptive':
        compress_layers = list(range(total_layers))
    elif strategy == 'conservative':
        compress_layers = list(range(2, total_layers-2))
    else:  # aggressive
        compress_layers = list(range(1, total_layers-1))
    
    print(f"   ì••ì¶• ëŒ€ìƒ: {len(compress_layers)}/{total_layers} ë ˆì´ì–´")
    
    compressed_count = 0
    for i in tqdm(compress_layers, desc="ğŸŒ GPT-NeoX ì••ì¶•"):
        try:
            layers[i] = LargeModelRealityStoneBlock(
                layers[i], compression_ratio, i, total_layers, 'gpt_neox', model_dtype
            )
            compressed_count += 1
        except Exception as e:
            print(f"   âŒ ë ˆì´ì–´ {i} ì••ì¶• ì‹¤íŒ¨: {e}")
            continue
    
    return compressed_count

def apply_gpt2_compression(model, compression_ratio, strategy, total_layers, model_dtype):
    """GPT-2 êµ¬ì¡° ì••ì¶• (ë°ì´í„° íƒ€ì… í†µì¼)"""
    
    layers = model.transformer.h
    
    # ê¸°ì¡´ ì„±ê³µí•œ ì „ëµ ì‚¬ìš©
    if strategy == 'adaptive':
        compress_layers = list(range(total_layers))
    elif strategy == 'conservative':
        compress_layers = list(range(2, total_layers-2))
    else:
        compress_layers = list(range(1, total_layers-1))
    
    print(f"   ì••ì¶• ëŒ€ìƒ: {len(compress_layers)}/{total_layers} ë ˆì´ì–´")
    
    compressed_count = 0
    for i in tqdm(compress_layers, desc="ğŸŒ GPT-2 ì••ì¶•"):
        try:
            layers[i] = LargeModelRealityStoneBlock(
                layers[i], compression_ratio, i, total_layers, 'gpt2', model_dtype
            )
            compressed_count += 1
        except Exception as e:
            print(f"   âŒ ë ˆì´ì–´ {i} ì••ì¶• ì‹¤íŒ¨: {e}")
            continue
    
    return compressed_count

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ëŒ€í˜• ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ìˆ˜ì •ëœ ë²„ì „)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_large_model_generation_fixed(model, tokenizer, model_type="ì›ë³¸"):
    """ëŒ€í˜• ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸ (ìˆ˜ì •ëœ ë²„ì „)"""
    
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”", 
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì´ë€",
        "ë§›ìˆëŠ” ìŒì‹ì€"
    ]
    
    print(f"\n=== {model_type} ëŒ€í˜• ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    results = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n[{i}/5] '{prompt}'")
        
        try:
            t0 = time.time()
            
            # ì§ì ‘ generate í˜¸ì¶œ (ë¬¸ì œ ìˆëŠ” í•¨ìˆ˜ ëŒ€ì‹ )
            inputs = tokenizer(prompt, return_tensors="pt")
            
            # ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
            input_ids = inputs['input_ids'] if 'input_ids' in inputs else inputs.input_ids
            
            # ë””ë°”ì´ìŠ¤ í†µì¼
            if torch.cuda.is_available():
                input_ids = input_ids.cuda()
                model = model.cuda()
            
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    max_length=input_ids.shape[1] + 15,
                    do_sample=True,
                    temperature=0.8,
                    top_p=0.9,
                    repetition_penalty=1.2,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    no_repeat_ngram_size=3
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            elapsed = time.time() - t0
            
            print(f"  ìƒì„±: {generated_text}")
            print(f"  ì‹œê°„: {elapsed:.3f}ì´ˆ")
            
            # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€
            generated_only = generated_text[len(prompt):].strip()
            if len(generated_only) > 5:
                quality_score = min(3.0, len(generated_only.split()) / 3)
                if any(bad in generated_only for bad in ['/', ':', '##']):
                    quality_score *= 0.5
            else:
                quality_score = 0.1
            
            print(f"  í’ˆì§ˆ: {quality_score:.2f}/3.0")
            
            results.append({
                'prompt': prompt,
                'generated': generated_text,
                'time': elapsed,
                'quality': quality_score
            })
            
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬: {e}")
            results.append({
                'prompt': prompt,
                'generated': f"ERROR: {e}",
                'time': 0,
                'quality': 0
            })
    
    # í†µê³„
    avg_time = sum(r['time'] for r in results) / len(results) if results else 0
    avg_quality = sum(r['quality'] for r in results) / len(results) if results else 0
    
    print(f"\nğŸ“Š {model_type} í†µê³„:")
    print(f"  í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ")
    print(f"  í‰ê·  í’ˆì§ˆ: {avg_quality:.2f}/3.0")
    
    return results

def select_and_load_large_model():
    """ëª¨ë¸ ì„ íƒ ë° ë¡œë“œ"""
    
    print("ğŸš€ ëŒ€í˜• ëª¨ë¸ ì„ íƒ ì¤‘...")
    
    # GPU ë©”ëª¨ë¦¬ ì²´í¬
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í•„í„°ë§
        suitable_models = []
        for name, info in SUPPORTED_LARGE_MODELS.items():
            if info['memory_gb'] <= gpu_memory:
                suitable_models.append((name, info))
        
        if not suitable_models:
            print("âŒ ë¡œë“œ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return None, None, None, None
        
        # ê°€ì¥ í° ëª¨ë¸ ì„ íƒ
        model_name, model_info = max(suitable_models, key=lambda x: x[1]['params'])
        print(f"ğŸ¯ ì„ íƒëœ ëª¨ë¸: {model_name} ({model_info['size']})")
        
    else:
        # CPU ì „ìš© - ê°€ì¥ ì‘ì€ ëª¨ë¸
        model_name = "microsoft/DialoGPT-medium"
        model_info = SUPPORTED_LARGE_MODELS[model_name]
        print(f"ğŸ–¥ï¸ CPU ëª¨ë“œ: {model_name}")
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        if torch.cuda.is_available():
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True
            )
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {total_params:,} íŒŒë¼ë¯¸í„°")
        
        return model, tokenizer, model_name, model_info
        
    except Exception as e:
        print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None, None

def complete_compression_with_finetuning():
    """ì™„ì „í•œ ì••ì¶•+íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸"""
    
    print("ğŸš€ ì™„ì „í•œ ëŒ€í˜• ëª¨ë¸ RealityStone ì••ì¶•+íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print("ğŸ’ ì••ì¶•â†’í…ŒìŠ¤íŠ¸â†’íŒŒì¸íŠœë‹â†’ìµœì¢…ê²€ì¦ ì „ì²´ íŒŒì´í”„ë¼ì¸")
    
    # 1ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ
    teacher_model, tokenizer, model_name, model_info = select_and_load_large_model()
    if teacher_model is None:
        return
    
    # 2ë‹¨ê³„: ì›ë³¸ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ“Š ì›ë³¸ ëŒ€í˜• ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    original_results = test_large_model_generation_fixed(teacher_model, tokenizer, "ì›ë³¸")
    
    # 3ë‹¨ê³„: RealityStone ì••ì¶• ì ìš©
    print("\n" + "="*80)
    print("ğŸ”¥ RealityStone ê·¹í•œ ì••ì¶• ì ìš©")
    
    student_model = copy.deepcopy(teacher_model)
    student_model = apply_large_model_reality_stone_compression(
        student_model,
        compression_ratio=model_info['compression_target'],
        strategy='adaptive'
    )
    
    # 4ë‹¨ê³„: ì••ì¶• í›„ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ“Š ì••ì¶• í›„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    compressed_results = test_large_model_generation_fixed(student_model, tokenizer, "ì••ì¶• í›„")
    
    # 5ë‹¨ê³„: Knowledge Distillation íŒŒì¸íŠœë‹
    print("\n" + "="*80)
    print("ğŸ§  Knowledge Distillation íŒŒì¸íŠœë‹")
    
    try:
        student_model = ultra_knowledge_distillation_fine_tune(
            teacher_model, student_model, tokenizer,
            total_steps=500,   # ëŒ€í˜• ëª¨ë¸ìš© ì ë‹¹í•œ ìŠ¤í…
            base_lr=3e-6,      # ë” ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ
            temperature=2.5    # ì ì ˆí•œ ì˜¨ë„
        )
        finetuning_success = True
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
        print("ğŸ“Š ì••ì¶•ëœ ëª¨ë¸ ê²°ê³¼ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")
        finetuning_success = False
    
    # 6ë‹¨ê³„: ìµœì¢… í…ŒìŠ¤íŠ¸ (íŒŒì¸íŠœë‹ ìˆì—ˆë‹¤ë©´)
    if finetuning_success:
        print("\n" + "="*80)
        print("ğŸ“Š íŒŒì¸íŠœë‹ í›„ ìµœì¢… í…ŒìŠ¤íŠ¸")
        final_results = test_large_model_generation_fixed(student_model, tokenizer, "ìµœì¢…")
    else:
        final_results = compressed_results
    
    # 7ë‹¨ê³„: ì™„ì „í•œ ê²°ê³¼ ë¶„ì„
    print("\n" + "="*80)
    print("ğŸ† ì™„ì „í•œ RealityStone ì••ì¶• ìµœì¢… ë¶„ì„")
    print("="*80)
    
    # ì„±ëŠ¥ ì§€í‘œ
    orig_quality = sum(r['quality'] for r in original_results) / len(original_results)
    comp_quality = sum(r['quality'] for r in compressed_results) / len(compressed_results)
    final_quality = sum(r['quality'] for r in final_results) / len(final_results)
    
    orig_time = sum(r['time'] for r in original_results) / len(original_results)
    comp_time = sum(r['time'] for r in compressed_results) / len(compressed_results)
    final_time = sum(r['time'] for r in final_results) / len(final_results)
    
    # ì••ì¶• í†µê³„
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    student_params = sum(p.numel() for p in student_model.parameters())
    compression_ratio = student_params / teacher_params
    memory_saved = (1 - compression_ratio) * 100
    quality_retention = final_quality / orig_quality if orig_quality > 0 else 1
    quality_improvement = final_quality - comp_quality if finetuning_success else 0
    speed_improvement = orig_time / final_time if final_time > 0 else 1
    
    print(f"ğŸ“Š ì™„ì „í•œ ì„±ëŠ¥ ë¶„ì„:")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   íŒŒë¼ë¯¸í„°: {teacher_params:,} â†’ {student_params:,}")
    print(f"   ì••ì¶•ë¥ : {compression_ratio:.1%} ({1/compression_ratio:.1f}Ã— ì••ì¶•)")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}%")
    print(f"   í’ˆì§ˆ: ì›ë³¸ {orig_quality:.2f} â†’ ì••ì¶• {comp_quality:.2f} â†’ ìµœì¢… {final_quality:.2f}")
    print(f"   ì†ë„: ì›ë³¸ {orig_time:.3f}ì´ˆ â†’ ìµœì¢… {final_time:.3f}ì´ˆ ({speed_improvement:.1f}Ã—)")
    
    if finetuning_success:
        print(f"   íŒŒì¸íŠœë‹ ê°œì„ : {quality_improvement:+.2f}ì ")
    
    # ìµœì¢… ì„±ê³µ íŒì •
    if memory_saved >= 85 and quality_retention >= 0.8:
        grade = "ğŸ† ì™„ì „ ëŒ€ì„±ê³µ!"
        message = f"RealityStoneìœ¼ë¡œ {memory_saved:.0f}% ì ˆì•½ + í’ˆì§ˆ {quality_retention*100:.0f}% ìœ ì§€!"
    elif memory_saved >= 70 and quality_retention >= 0.7:
        grade = "ğŸ¥‡ ëŒ€ì„±ê³µ!"
        message = f"ìƒë‹¹í•œ ì••ì¶• ì„±ê³¼ + í’ˆì§ˆ ìœ ì§€!"
    elif memory_saved >= 50 and quality_retention >= 0.5:
        grade = "ğŸ¥ˆ ì„±ê³µ!"
        message = f"ì ˆë°˜ ì´ìƒ ì••ì¶• + ì ì • í’ˆì§ˆ ìœ ì§€!"
    else:
        grade = "ğŸ”§ ë¶€ë¶„ ì„±ê³µ"
        message = f"ì••ì¶•ì€ ì„±ê³µ, í’ˆì§ˆ ìµœì í™” í•„ìš”"
    
    print(f"\nğŸ¯ ìµœì¢… í‰ê°€: {grade}")
    print(f"   {message}")
    print(f"   ğŸ’ ê¸°ìˆ : RealityStone + ë¦¬ë§Œê¸°í•˜í•™ + FFT+SVD")
    print(f"   ğŸ—ï¸ ì•„í‚¤í…ì²˜: {model_info['size']} ëŒ€í˜• ëª¨ë¸")
    print(f"   ğŸ§  íŒŒì¸íŠœë‹: {'ì„±ê³µ' if finetuning_success else 'ì‹¤íŒ¨'}")
    
    # ìƒì„± ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“ ìƒì„± ê²°ê³¼ ìƒ˜í”Œ:")
    for i, result in enumerate(final_results[:3], 1):
        print(f"   [{i}] {result['prompt']} â†’ {result['generated']}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del teacher_model, student_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print(f"\nâœ¨ ì™„ì „í•œ ì••ì¶•+íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    
    return {
        'compression_ratio': compression_ratio,
        'memory_saved': memory_saved,
        'quality_retention': quality_retention,
        'speed_improvement': speed_improvement,
        'finetuning_success': finetuning_success,
        'final_grade': grade
    }

def main_large_model():
    """ë©”ì¸ í•¨ìˆ˜ (ì™„ì „í•œ ë²„ì „)"""
    
    # ì™„ì „í•œ ì••ì¶•+íŒŒì¸íŠœë‹ ì‹¤í–‰
    return complete_compression_with_finetuning()

if __name__ == "__main__":
    main_large_model() 