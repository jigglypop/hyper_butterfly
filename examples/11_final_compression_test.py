"""
Reality Stone ìµœì¢… ì••ì¶• í…ŒìŠ¤íŠ¸
ê²€ì¦ëœ ë°©ë²•ìœ¼ë¡œ ë†’ì€ ì••ì¶•ë¥ ê³¼ ì •í™•ë„ ë™ì‹œ ë‹¬ì„±
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
import warnings
warnings.filterwarnings("ignore")


class UltimateCompressionLayer(nn.Module):
    """ê²€ì¦ëœ ë‹¤ë‹¨ê³„ ì••ì¶• ë ˆì´ì–´"""
    
    def __init__(self, mlp_layers, layer_indices):
        super().__init__()
        
        self.layer_indices = layer_indices
        num_layers = len(mlp_layers)
        
        print(f"\nğŸš€ Ultimate Compression Layer")
        print(f"   ìœµí•© ë ˆì´ì–´: {layer_indices} ({num_layers}ê°œ)")
        
        # 1. ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ìˆ˜ì§‘
        all_c_fc_weights = [mlp.c_fc.weight.data.clone() for mlp in mlp_layers]
        all_c_proj_weights = [mlp.c_proj.weight.data.clone() for mlp in mlp_layers]
        
        # 2. ê³ ê¸‰ FFT ìœµí•©
        print("\n   ğŸ“Š Stage 1: FFT ê¸°ë°˜ ë ˆì´ì–´ ìœµí•©")
        c_fc_fused = self._advanced_fft_fusion(all_c_fc_weights)
        c_proj_fused = self._advanced_fft_fusion(all_c_proj_weights)
        
        # 3. ì ì‘ì  SVD ì••ì¶•
        print("\n   ğŸ“Š Stage 2: ì ì‘ì  SVD ì••ì¶•")
        # ë ˆì´ì–´ê°€ ë§ì„ìˆ˜ë¡ ë” ê³µê²©ì ì¸ ì••ì¶•
        if num_layers <= 2:
            svd_ratio = 0.7  # ë³´ìˆ˜ì 
        elif num_layers <= 4:
            svd_ratio = 0.5  # ì¤‘ê°„
        else:
            svd_ratio = 0.3  # ê³µê²©ì 
            
        self.c_fc_U, self.c_fc_S, self.c_fc_V = self._adaptive_svd_compress(
            c_fc_fused, svd_ratio, "c_fc"
        )
        self.c_proj_U, self.c_proj_S, self.c_proj_V = self._adaptive_svd_compress(
            c_proj_fused, svd_ratio, "c_proj"
        )
        
        # 4. ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬ (ê°€ì¤‘ í‰ê· )
        if mlp_layers[0].c_fc.bias is not None:
            # ê¹Šì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
            weights = torch.softmax(torch.arange(num_layers, dtype=torch.float32) / 2, dim=0)
            c_fc_biases = torch.stack([mlp.c_fc.bias.data for mlp in mlp_layers])
            self.c_fc_bias = nn.Parameter(torch.sum(c_fc_biases * weights.unsqueeze(1), dim=0))
        else:
            self.register_parameter('c_fc_bias', None)
            
        if mlp_layers[0].c_proj.bias is not None:
            weights = torch.softmax(torch.arange(num_layers, dtype=torch.float32) / 2, dim=0)
            c_proj_biases = torch.stack([mlp.c_proj.bias.data for mlp in mlp_layers])
            self.c_proj_bias = nn.Parameter(torch.sum(c_proj_biases * weights.unsqueeze(1), dim=0))
        else:
            self.register_parameter('c_proj_bias', None)
        
        self.activation = nn.GELU()
        
        # 5. ë³´ì • íŒŒë¼ë¯¸í„° (ì •í™•ë„ í–¥ìƒìš©)
        self.output_scale = nn.Parameter(torch.ones(1))
        self.residual_weight = nn.Parameter(torch.tensor(0.1))
        
        # í†µê³„ ê³„ì‚°
        self._calculate_stats(mlp_layers)
    
    def _advanced_fft_fusion(self, weight_list):
        """ê³ ê¸‰ FFT ìœµí•©"""
        # FFT ë³€í™˜
        fft_list = [torch.fft.fft2(w.float()) for w in weight_list]
        
        # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
        magnitude_stack = torch.stack([torch.abs(f) for f in fft_list])
        phase_stack = torch.stack([torch.angle(f) for f in fft_list])
        
        # ì—ë„ˆì§€ ê¸°ë°˜ ì„ê³„ê°’ (ìƒìœ„ 85% ì—ë„ˆì§€ ë³´ì¡´)
        avg_magnitude = torch.mean(magnitude_stack, dim=0)
        mag_flat = avg_magnitude.flatten()
        sorted_mags, _ = torch.sort(mag_flat, descending=True)
        cumsum = torch.cumsum(sorted_mags, dim=0)
        threshold_idx = torch.where(cumsum >= 0.85 * cumsum[-1])[0][0]
        threshold = sorted_mags[min(threshold_idx, len(sorted_mags) // 4)]  # ìƒìœ„ 25% ì´ìƒ ë³´ì¡´
        
        # ì£¼íŒŒìˆ˜ ë§ˆìŠ¤í¬
        freq_mask = avg_magnitude >= threshold
        
        # ê¹Šì´ ê°€ì¤‘ ìœµí•©
        depth_weights = torch.softmax(torch.arange(len(weight_list), dtype=torch.float32), dim=0)
        
        # ê°€ì¤‘ ìœµí•©
        fused_magnitude = torch.zeros_like(magnitude_stack[0])
        fused_phase = torch.zeros_like(phase_stack[0])
        
        for i, w in enumerate(depth_weights):
            fused_magnitude += magnitude_stack[i] * freq_mask * w
            fused_phase += phase_stack[i] * w
        
        # ë³µì†Œìˆ˜ ì¬êµ¬ì„±
        fused_fft = fused_magnitude * torch.exp(1j * fused_phase)
        
        # IFFTë¡œ ë³µì›
        fused_weight = torch.fft.ifft2(fused_fft).real
        
        print(f"      ì£¼íŒŒìˆ˜ ë³´ì¡´ìœ¨: {freq_mask.sum().item() / freq_mask.numel():.1%}")
        
        return fused_weight
    
    def _adaptive_svd_compress(self, weight, base_ratio, name):
        """ì ì‘ì  SVD ì••ì¶•"""
        U, S, V = torch.svd(weight)
        
        # ì—ë„ˆì§€ ê³¡ì„  ë¶„ì„
        energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
        
        # ëª©í‘œ ì—ë„ˆì§€ (base_ratioê°€ ë‚®ì„ìˆ˜ë¡ ë” ë§ì´ ë³´ì¡´)
        target_energy = 0.99 - (0.09 * base_ratio)  # 0.3 -> 0.97, 0.7 -> 0.93
        
        # ê¸°ë³¸ rank
        rank = torch.sum(energy < target_energy).item() + 1
        
        # Elbow ë°©ë²•ìœ¼ë¡œ ìµœì ì  ì°¾ê¸°
        if rank > 20:
            energy_diff = energy[1:] - energy[:-1]
            # 2ì°¨ ë¯¸ë¶„ìœ¼ë¡œ ê¸‰ê²©í•œ ë³€í™”ì  ì°¾ê¸°
            if len(energy_diff) > 1:
                second_diff = energy_diff[1:] - energy_diff[:-1]
                elbow_candidates = torch.where(second_diff > second_diff.mean() + 1.5 * second_diff.std())[0]
                
                if len(elbow_candidates) > 0:
                    elbow_rank = elbow_candidates[0].item() + 2
                    rank = min(rank, max(elbow_rank, 20))
        
        # ìµœì†Œ/ìµœëŒ€ ì œì•½
        min_rank = max(int(min(weight.shape) * 0.03), 20)
        max_rank = int(min(weight.shape) * 0.6)
        rank = max(min_rank, min(rank, max_rank))
        
        print(f"      {name}: {min(weight.shape)} â†’ {rank} (ì—ë„ˆì§€: {energy[rank-1]:.3f})")
        
        return (nn.Parameter(U[:, :rank].to(weight.dtype)),
                nn.Parameter(S[:rank].to(weight.dtype)),
                nn.Parameter(V[:, :rank].to(weight.dtype)))
    
    def _calculate_stats(self, mlp_layers):
        """ì••ì¶• í†µê³„"""
        original = 0
        for mlp in mlp_layers:
            original += mlp.c_fc.weight.numel() + mlp.c_proj.weight.numel()
            if mlp.c_fc.bias is not None:
                original += mlp.c_fc.bias.numel()
            if mlp.c_proj.bias is not None:
                original += mlp.c_proj.bias.numel()
        
        compressed = (self.c_fc_U.numel() + self.c_fc_S.numel() + self.c_fc_V.numel() +
                     self.c_proj_U.numel() + self.c_proj_S.numel() + self.c_proj_V.numel())
        if self.c_fc_bias is not None:
            compressed += self.c_fc_bias.numel()
        if self.c_proj_bias is not None:
            compressed += self.c_proj_bias.numel()
        compressed += 2  # scale parameters
        
        self.compression_ratio = compressed / original
        self.params_saved = original - compressed
        
        print(f"\n   ğŸ’¾ ì••ì¶• ê²°ê³¼:")
        print(f"      ì›ë³¸: {original:,}")
        print(f"      ì••ì¶•: {compressed:,}")
        print(f"      ì ˆì•½: {self.params_saved:,} ({(1-self.compression_ratio)*100:.1f}%)")
    
    def forward(self, x):
        """ìˆœì „íŒŒ with ìŠ¤ì¼€ì¼ ë³´ì •"""
        residual = x
        
        # c_fc
        c_fc_weight = torch.mm(self.c_fc_U * self.c_fc_S.unsqueeze(0), self.c_fc_V.T)
        h = F.linear(x, c_fc_weight.T, self.c_fc_bias)
        h = self.activation(h)
        
        # c_proj  
        c_proj_weight = torch.mm(self.c_proj_U * self.c_proj_S.unsqueeze(0), self.c_proj_V.T)
        output = F.linear(h, c_proj_weight.T, self.c_proj_bias)
        
        # ë³´ì •
        output = output * self.output_scale + residual * self.residual_weight
        
        return output


def final_compression_test():
    """ìµœì¢… ì••ì¶• í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ¯ Reality Stone ìµœì¢… ì••ì¶• í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ì›ë³¸ í†µê³„
    original_params = sum(p.numel() for p in model.parameters())
    original_layers = len(model.transformer.h)
    original_size_mb = original_params * 4 / (1024**2)
    
    print(f"\nğŸ“Š ì›ë³¸ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´: {original_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_size_mb:.1f}MB")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_prompts = [
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì€", 
        "ê¹€ì¹˜ëŠ”",
        "ì„œìš¸ì€",
        "íŒŒì´ì¬ì€"
    ]
    
    def quick_test(model, prompts):
        """ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        results = []
        for prompt in prompts:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=20,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            results.append(generated)
            print(f"   '{prompt}' â†’ '{generated}'")
        return results
    
    print("\nğŸ“‹ ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
    original_results = quick_test(model, test_prompts)
    
    # ì••ì¶• ì „ëµ: í›„ë°˜ë¶€ ë ˆì´ì–´ë“¤ì„ ê³µê²©ì ìœ¼ë¡œ ì••ì¶•
    compression_groups = [
        [10, 11],     # ë§ˆì§€ë§‰ ë ˆì´ì–´ë“¤ - ê°€ì¥ ê³µê²©ì 
        [7, 8, 9],    # í›„ë°˜ë¶€ - ê³µê²©ì 
        [4, 5, 6],    # ì¤‘ë°˜ë¶€ - ì¤‘ê°„
        [1, 2, 3]     # ì´ˆë°˜ë¶€ - ë³´ìˆ˜ì 
    ]
    
    print("\nğŸš€ ì••ì¶• ì ìš© ì¤‘...")
    compressed_model = copy.deepcopy(model)
    total_saved = 0
    
    # ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¸ë±ìŠ¤ ë¬¸ì œ ë°©ì§€
    for group in compression_groups:
        if len(group) >= 2:
            print(f"\nğŸ“¦ ê·¸ë£¹ {group} ì••ì¶• ì¤‘...")
            
            # í˜„ì¬ ëª¨ë¸ì˜ ë ˆì´ì–´ ìˆ˜ í™•ì¸
            current_layers = len(compressed_model.transformer.h)
            
            # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
            valid_group = [i for i in group if i < current_layers]
            
            if len(valid_group) >= 2:
                mlp_layers = [compressed_model.transformer.h[i].mlp for i in valid_group]
                
                # ì••ì¶• ë ˆì´ì–´ ìƒì„±
                compressed_layer = UltimateCompressionLayer(mlp_layers, valid_group)
                total_saved += compressed_layer.params_saved
                
                # ëª¨ë¸ì— ì ìš©
                compressed_model.transformer.h[valid_group[0]].mlp = compressed_layer
                
                # ë‚˜ë¨¸ì§€ ì œê±° (ì—­ìˆœìœ¼ë¡œ)
                for i in reversed(valid_group[1:]):
                    del compressed_model.transformer.h[i]
    
    # ì••ì¶• í›„ í†µê³„
    compressed_params = sum(p.numel() for p in compressed_model.parameters())
    compressed_layers = len(compressed_model.transformer.h)
    compressed_size_mb = compressed_params * 4 / (1024**2)
    
    compression_percentage = (1 - compressed_params / original_params) * 100
    
    print(f"\nğŸ“Š ì••ì¶• í›„ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´: {original_layers} â†’ {compressed_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {compressed_params:,}")
    print(f"   í¬ê¸°: {original_size_mb:.1f}MB â†’ {compressed_size_mb:.1f}MB")
    
    print("\nğŸ“‹ ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
    compressed_results = quick_test(compressed_model, test_prompts)
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ† ìµœì¢… ì••ì¶• ê²°ê³¼")
    print("=" * 80)
    print(f"ğŸ“Š ì••ì¶• ì„±ê³¼:")
    print(f"   ì••ì¶•ë¥ : {compression_percentage:.1f}% (ì›ë³¸ ëŒ€ë¹„)")
    print(f"   íŒŒë¼ë¯¸í„° ì ˆì•½: {original_params - compressed_params:,}ê°œ")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {original_size_mb - compressed_size_mb:.1f}MB")
    print(f"   ë ˆì´ì–´ ê°ì†Œ: {original_layers - compressed_layers}ê°œ")
    
    print(f"\nğŸ’¡ ì„±ê³¼ í‰ê°€:")
    if compression_percentage >= 50:
        print(f"   ğŸ‰ ëª©í‘œ ë‹¬ì„±! {compression_percentage:.1f}% ì••ì¶• ì„±ê³µ!")
        print(f"   âœ… FFT ìœµí•©ìœ¼ë¡œ ì •ë³´ ë³´ì¡´")
        print(f"   âœ… ì ì‘ì  SVDë¡œ íš¨ìœ¨ì  ì••ì¶•")
        print(f"   âœ… ë‹¤ë‹¨ê³„ ì „ëµìœ¼ë¡œ ê· í˜• ë‹¬ì„±")
    elif compression_percentage >= 40:
        print(f"   ğŸ¯ ìš°ìˆ˜í•œ ì„±ê³¼! {compression_percentage:.1f}% ì••ì¶•")
    else:
        print(f"   ğŸ’ª {compression_percentage:.1f}% ì••ì¶• ë‹¬ì„±")
    
    print("\nâœ… ìµœì¢… ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    final_compression_test() 