import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
from reality_stone.layers import EquivalentHyperbolicLinear, project_to_ball
import time
import numpy as np

def convert_to_equivalent_hyperbolic(model: nn.Module, c: float = 1.0):
    """
    ëª¨ë¸ì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ë¥¼ EquivalentHyperbolicLinearë¡œ êµì²´í•©ë‹ˆë‹¤.
    """
    for name, module in model.named_children():
        # ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ëª¨ë“ˆ íƒìƒ‰
        if len(list(module.children())) > 0:
            convert_to_equivalent_hyperbolic(module, c=c)
        
        # Conv1Dì™€ Linear ë ˆì´ì–´ë¥¼ êµì²´
        if isinstance(module, nn.Linear) or 'Conv1D' in str(type(module)):
            equiv_layer = EquivalentHyperbolicLinear.from_linear(module, c=c)
            setattr(model, name, equiv_layer)
            print(f"âœ… Replaced '{name}' with EquivalentHyperbolicLinear(c={c})")

def test_layer_equivalence():
    """ë ˆì´ì–´ ë³€í™˜ì˜ ë™ë“±ì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    print("\nğŸ§ª Testing layer equivalence...")
    
    # í…ŒìŠ¤íŠ¸ìš© ì„ í˜• ë ˆì´ì–´
    linear = nn.Linear(768, 2304)
    x = torch.randn(10, 768)
    
    # ì›ë³¸ ì¶œë ¥
    with torch.no_grad():
        original_output = linear(x)
    
    # EquivalentHyperbolicLinearë¡œ ë³€í™˜
    equiv_layer = EquivalentHyperbolicLinear.from_linear(linear, c=1.0)
    
    with torch.no_grad():
        equiv_output = equiv_layer(x)
    
    # ì°¨ì´ ê³„ì‚°
    diff = torch.abs(original_output - equiv_output).mean()
    relative_diff = diff / torch.abs(original_output).mean()
    
    print(f"Original output shape: {original_output.shape}")
    print(f"Equivalent output shape: {equiv_output.shape}")
    print(f"Mean absolute difference: {diff:.6f}")
    print(f"Relative difference: {relative_diff:.4%}")
    
    # ì¶œë ¥ì´ ìŒê³¡ ê³µê°„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
    norms = torch.norm(equiv_output, p=2, dim=-1)
    print(f"Max output norm: {norms.max():.4f} (should be < 1.0)")
    print(f"Mean output norm: {norms.mean():.4f}")
    
    return relative_diff < 0.05  # 5% ì´ë‚´ì˜ ì°¨ì´ í—ˆìš©

def main():
    # 1. ë ˆì´ì–´ ë™ë“±ì„± í…ŒìŠ¤íŠ¸
    if test_layer_equivalence():
        print("âœ… Layer equivalence test passed!")
    else:
        print("âŒ Layer equivalence test failed!")
    
    # 2. ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“¥ Loading KoGPT-2 model...")
    model_name = "skt/kogpt2-base-v2"
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        return
    
    # 3. í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì›ë³¸ ëª¨ë¸ í‰ê°€
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜",
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”",
        "í•œêµ­ì˜ ì „í†µ ìŒì‹ì¸",
    ]
    
    print("\nğŸ“Š Testing original model...")
    original_outputs = []
    
    for prompt in test_prompts:
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_length=20,
                do_sample=False,  # ê²°ì •ì  ìƒì„±
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        original_outputs.append(generated_text)
        print(f"Prompt: {prompt}")
        print(f"Generated: {generated_text}")
        print()
    
    # 4. ëª¨ë¸ ë³€í™˜
    print("\nğŸ”„ Converting to EquivalentHyperbolicLinear...")
    start_time = time.time()
    
    # ëª¨ë“  ë ˆì´ì–´ ë³€í™˜
    convert_to_equivalent_hyperbolic(model, c=1.0)
    
    conversion_time = time.time() - start_time
    print(f"â° Conversion finished in {conversion_time:.2f} seconds.")
    
    # 5. ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š Testing converted model...")
    converted_outputs = []
    
    for i, prompt in enumerate(test_prompts):
        inputs = tokenizer(prompt, return_tensors="pt")
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=20,
                    do_sample=False,  # ê²°ì •ì  ìƒì„±
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            converted_outputs.append(generated_text)
            
            print(f"Prompt: {prompt}")
            print(f"Original: {original_outputs[i]}")
            print(f"Converted: {generated_text}")
            
            # ë™ì¼ì„± í™•ì¸
            if original_outputs[i] == generated_text:
                print("âœ… Output is identical!")
            else:
                # í† í° ë ˆë²¨ì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°
                orig_tokens = tokenizer.encode(original_outputs[i])
                conv_tokens = tokenizer.encode(generated_text)
                
                min_len = min(len(orig_tokens), len(conv_tokens))
                if min_len > 0:
                    matching = sum(1 for a, b in zip(orig_tokens[:min_len], conv_tokens[:min_len]) if a == b)
                    similarity = matching / min_len
                    print(f"âš ï¸ Token similarity: {similarity:.1%}")
                else:
                    print("âš ï¸ Outputs differ")
            print()
            
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            import traceback
            traceback.print_exc()
            break
    
    # 6. ìµœì¢… í‰ê°€
    print("\n" + "="*50)
    print("ğŸ“Š Final Evaluation:")
    
    # ì •í™•í•œ ì¼ì¹˜ ë¹„ìœ¨ ê³„ì‚°
    exact_matches = sum(1 for o, c in zip(original_outputs, converted_outputs) if o == c)
    match_rate = exact_matches / len(test_prompts) if test_prompts else 0
    
    print(f"Exact match rate: {match_rate:.1%} ({exact_matches}/{len(test_prompts)})")
    print(f"Conversion time: {conversion_time:.2f}s")
    
    if match_rate >= 0.8:  # 80% ì´ìƒ ì¼ì¹˜
        print("\nâœ… SUCCESS: EquivalentHyperbolicLinear maintains accuracy!")
    else:
        print("\nâš ï¸ WARNING: Some outputs differ. Fine-tuning may be needed.")
    
    # 7. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ (ì„ íƒì )
    print("\nğŸ’¾ Memory usage:")
    print("Note: EquivalentHyperbolicLinear uses same memory as original")
    print("(No compression, focus on accuracy preservation)")

if __name__ == "__main__":
    main() 