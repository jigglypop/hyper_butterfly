import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
from reality_stone.layers import EquivalentHyperbolicLinear
import time
import os
import psutil
import gc

def get_model_size(model):
    """ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (MB ë‹¨ìœ„)"""
    param_size = 0
    buffer_size = 0
    
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb

def save_model(model, tokenizer, save_path):
    """ëª¨ë¸ì„ ì €ìž¥í•˜ê³  ë””ìŠ¤í¬ í¬ê¸°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤"""
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    
    # ë””ìŠ¤í¬ í¬ê¸° ê³„ì‚°
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(save_path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            total_size += os.path.getsize(filepath)
    
    return total_size / 1024 / 1024  # MB ë‹¨ìœ„

def convert_to_equivalent_hyperbolic(model: nn.Module, c: float = 1.0):
    """ëª¨ë¸ì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ë¥¼ EquivalentHyperbolicLinearë¡œ êµì²´í•©ë‹ˆë‹¤."""
    conversion_count = 0
    
    for name, module in model.named_children():
        # ìž¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ëª¨ë“ˆ íƒìƒ‰
        if len(list(module.children())) > 0:
            sub_count = convert_to_equivalent_hyperbolic(module, c=c)
            conversion_count += sub_count
        
        # Conv1Dì™€ Linear ë ˆì´ì–´ë¥¼ êµì²´
        if isinstance(module, nn.Linear) or 'Conv1D' in str(type(module)):
            equiv_layer = EquivalentHyperbolicLinear.from_linear(module, c=c)
            setattr(model, name, equiv_layer)
            conversion_count += 1
    
    return conversion_count

def benchmark_generation(model, tokenizer, prompts, num_runs=3):
    """ìƒì„± ì†ë„ë¥¼ ë²¤ì¹˜ë§ˆí¬í•©ë‹ˆë‹¤"""
    times = []
    
    for _ in range(num_runs):
        start_time = time.time()
        
        for prompt in prompts:
            inputs = tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=50,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
        
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = sum(times) / len(times)
    return avg_time

def main():
    print("="*70)
    print("Full Model Conversion and Comparison Test")
    print("="*70)
    
    # 1. ì›ë³¸ ëª¨ë¸ ë¡œë“œ
    print("\nðŸ“¥ Loading original KoGPT-2 model...")
    model_name = "skt/kogpt2-base-v2"
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    original_model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •ì„ ìœ„í•œ ì´ˆê¸°í™”
    gc.collect()
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # 2. ì›ë³¸ ëª¨ë¸ ë¶„ì„
    print("\nðŸ“Š Original Model Analysis:")
    original_size = get_model_size(original_model)
    print(f"  Memory size: {original_size:.2f} MB")
    
    # ì›ë³¸ ëª¨ë¸ ì €ìž¥
    print("  Saving original model...")
    original_disk_size = save_model(original_model, tokenizer, "./original_model")
    print(f"  Disk size: {original_disk_size:.2f} MB")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€",
        "ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì€",
        "í•œêµ­ì˜ ì „í†µ ë¬¸í™”ëŠ”",
        "ë¯¸ëž˜ì˜ ê¸°ìˆ ì€",
        "ìžì—°ê³¼ í™˜ê²½ì„"
    ]
    
    # ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    print("\nâ±ï¸  Benchmarking original model...")
    original_time = benchmark_generation(original_model, tokenizer, test_prompts)
    print(f"  Average generation time: {original_time:.3f} seconds")
    
    # 3. ëª¨ë¸ ë³€í™˜
    print("\nðŸ”„ Converting to EquivalentHyperbolicLinear...")
    start_conversion = time.time()
    
    # ë³€í™˜ì„ ìœ„í•œ ëª¨ë¸ ë³µì‚¬ (ì›ë³¸ ë³´ì¡´)
    converted_model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)
    
    # ë³€í™˜ ìˆ˜í–‰
    num_converted = convert_to_equivalent_hyperbolic(converted_model, c=1.0)
    
    conversion_time = time.time() - start_conversion
    print(f"  Converted {num_converted} layers in {conversion_time:.2f} seconds")
    
    # 4. ë³€í™˜ëœ ëª¨ë¸ ë¶„ì„
    print("\nðŸ“Š Converted Model Analysis:")
    converted_size = get_model_size(converted_model)
    print(f"  Memory size: {converted_size:.2f} MB")
    
    # ë³€í™˜ëœ ëª¨ë¸ ì €ìž¥
    print("  Saving converted model...")
    converted_disk_size = save_model(converted_model, tokenizer, "./hyperbolic_model")
    print(f"  Disk size: {converted_disk_size:.2f} MB")
    
    # ë³€í™˜ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    print("\nâ±ï¸  Benchmarking converted model...")
    converted_time = benchmark_generation(converted_model, tokenizer, test_prompts)
    print(f"  Average generation time: {converted_time:.3f} seconds")
    
    # 5. ì •í™•ë„ ë¹„êµ
    print("\nðŸŽ¯ Accuracy Comparison:")
    accuracy_prompts = ["ì•ˆë…•í•˜ì„¸ìš”", "ì˜¤ëŠ˜ì€", "ì¸ê³µì§€ëŠ¥"]
    
    matches = 0
    total = len(accuracy_prompts)
    
    for prompt in accuracy_prompts:
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            # ì›ë³¸ ëª¨ë¸
            orig_out = original_model.generate(
                inputs.input_ids, max_length=20, do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
            orig_text = tokenizer.decode(orig_out[0], skip_special_tokens=True)
            
            # ë³€í™˜ëœ ëª¨ë¸
            conv_out = converted_model.generate(
                inputs.input_ids, max_length=20, do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
            conv_text = tokenizer.decode(conv_out[0], skip_special_tokens=True)
            
            if orig_text == conv_text:
                matches += 1
                print(f"  âœ… '{prompt}' -> Identical output")
            else:
                print(f"  âŒ '{prompt}' -> Different output")
    
    accuracy = matches / total * 100
    print(f"\n  Accuracy: {accuracy:.1f}% ({matches}/{total} identical)")
    
    # 6. ìµœì¢… ë¹„êµ ìš”ì•½
    print("\n" + "="*70)
    print("ðŸ“Š FINAL COMPARISON SUMMARY")
    print("="*70)
    
    print("\nðŸ—„ï¸ Storage Comparison:")
    print(f"  Original model:")
    print(f"    - Memory: {original_size:.2f} MB")
    print(f"    - Disk: {original_disk_size:.2f} MB")
    print(f"  Hyperbolic model:")
    print(f"    - Memory: {converted_size:.2f} MB")
    print(f"    - Disk: {converted_disk_size:.2f} MB")
    print(f"  Memory ratio: {converted_size/original_size:.2%}")
    print(f"  Disk ratio: {converted_disk_size/original_disk_size:.2%}")
    
    print("\nâš¡ Speed Comparison:")
    print(f"  Original: {original_time:.3f}s")
    print(f"  Hyperbolic: {converted_time:.3f}s")
    print(f"  Speed ratio: {converted_time/original_time:.2f}x")
    
    print("\nðŸŽ¯ Accuracy:")
    print(f"  {accuracy:.1f}% identical outputs")
    
    # 7. ê²°ë¡ 
    print("\n" + "="*70)
    if accuracy >= 90 and converted_disk_size <= original_disk_size * 1.1:
        print("âœ… SUCCESS: EquivalentHyperbolicLinear maintains accuracy")
        print("   with comparable storage requirements!")
    else:
        print("âš ï¸ WARNING: Some trade-offs detected")
    print("="*70)
    
    # ì •ë¦¬
    print("\nðŸ§¹ Cleaning up saved models...")
    import shutil
    shutil.rmtree("./original_model", ignore_errors=True)
    shutil.rmtree("./hyperbolic_model", ignore_errors=True)
    print("âœ… Cleanup complete!")

if __name__ == "__main__":
    main() 