import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
from reality_stone.layers import HyperbolicLinear, project_to_ball
import time

def convert_to_hyperbolic(model: nn.Module, c: float = 1.0):
    """
    ëª¨ë¸ì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ë¥¼ HyperbolicLinearë¡œ êµì²´í•©ë‹ˆë‹¤.
    """
    for name, module in model.named_children():
        # ì¬ê·€ì ìœ¼ë¡œ í•˜ìœ„ ëª¨ë“ˆ íƒìƒ‰
        if len(list(module.children())) > 0:
            convert_to_hyperbolic(module, c=c)
        
        # Conv1Dì™€ Linear ë ˆì´ì–´ë¥¼ HyperbolicLinearë¡œ êµì²´
        if isinstance(module, nn.Linear) or 'Conv1D' in str(type(module)):
            hyperbolic_layer = HyperbolicLinear.from_linear(module, c=c)
            setattr(model, name, hyperbolic_layer)
            print(f"âœ… Replaced '{name}' with HyperbolicLinear(c={c})")

def main():
    print("Loading KoGPT-2 model")
    model_name = "skt/kogpt2-base-v2"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        return

    # 2. ëª¨ë¸ì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ë¥¼ í‘¸ì•µì¹´ë ˆ ê³µìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë˜í¼ ì¶”ê°€
    #    GPT-2ì˜ ê²½ìš°, ì„ë² ë”© ì´í›„ ì²« ì…ë ¥ì€ ìœ í´ë¦¬ë“œ ê³µê°„ì— ìˆìœ¼ë¯€ë¡œ,
    #    ì²« ë²ˆì§¸ í•˜ì´í¼ë³¼ë¦­ ë ˆì´ì–´ì— ë“¤ì–´ê°€ê¸° ì „ì— íˆ¬ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
    class InputProjector(nn.Module):
        def __init__(self, model):
            super().__init__()
            self.model = model
        def forward(self, *args, **kwargs):
            # kwargsì—ì„œ inputs_embeds ë˜ëŠ” hidden_statesë¥¼ ì°¾ì•„ íˆ¬ì˜
            if 'inputs_embeds' in kwargs and kwargs['inputs_embeds'] is not None:
                kwargs['inputs_embeds'] = project_to_ball(kwargs['inputs_embeds'])
            elif 'hidden_states' in kwargs and kwargs['hidden_states'] is not None:
                 kwargs['hidden_states'] = project_to_ball(kwargs['hidden_states'])
            
            return self.model(*args, **kwargs)

    # 3. ë ˆì´ì–´ ë³€í™˜
    print("\nğŸ”„ Converting layers to HyperbolicLinear...")
    start_time = time.time()
    convert_to_hyperbolic(model, c=1.0)
    
    # ëª¨ë¸ì˜ transformerë¥¼ InputProjectorë¡œ ê°ì‹¸ê¸°
    model.transformer = InputProjector(model.transformer)
    print("âœ… Wrapped transformer with InputProjector.")
    
    conversion_time = time.time() - start_time
    print(f"â° Conversion finished in {conversion_time:.2f} seconds.")

    print("\nConverted model architecture:")
    print(model)

    # 4. ë³€í™˜ëœ ëª¨ë¸ë¡œ ìƒì„± í…ŒìŠ¤íŠ¸
    print("\nâœï¸ Testing generation with the converted model...")
    prompt = "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?"
    inputs = tokenizer(prompt, return_tensors="pt")

    try:
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_length=60,
                do_sample=True,
                temperature=0.8,
                top_k=50,
                pad_token_id=tokenizer.eos_token_id
            )
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nPrompt: {prompt}")
        print(f"Generated: {generated_text}")
        print("\nâœ… Conversion test successful!")
    except Exception as e:
        print(f"âŒ Generation failed after conversion: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 