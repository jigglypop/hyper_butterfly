import os
import gc
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
from reality_stone.layers import EquivalentHyperbolicLinear, RBELinear
from reality_stone.layers.rbe import calculate_compression_stats, encode_model_to_seeds
import time
import sys
from tqdm import tqdm
import ctypes
import numpy as np
import copy

def get_model_size(model):
    """ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (MB ë‹¨ìœ„)"""
    param_size = 0
    buffer_size = 0
    
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb

def save_model(model, tokenizer, save_path):
    """ëª¨ë¸ì˜ state_dictì™€ tokenizer, configë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(save_path, exist_ok=True)
    
    # ëª¨ë¸ ì €ì¥ (ê°€ì¤‘ì¹˜, ì„¤ì •, í† í¬ë‚˜ì´ì € ë“±)
    model.save_pretrained(save_path, safe_serialization=True)
    if tokenizer:
        tokenizer.save_pretrained(save_path)
    
    # ë””ìŠ¤í¬ í¬ê¸° ê³„ì‚°
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(save_path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            total_size += os.path.getsize(filepath)
    
    return total_size / 1024 / 1024  # MB ë‹¨ìœ„

def convert_to_equivalent_hyperbolic(model: nn.Module, c: float = 1.0):
    """ëª¨ë¸ì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ë¥¼ EquivalentHyperbolicLinearë¡œ êµì²´í•©ë‹ˆë‹¤."""
    # ë¨¼ì € ëª¨ë“  ë³€í™˜ ëŒ€ìƒ ë ˆì´ì–´ë¥¼ ì°¾ìŒ
    layers_to_convert = []
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) or 'Conv1D' in str(type(module)):
            layers_to_convert.append((name, module))
    
    # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for name, module in tqdm(layers_to_convert, desc="Converting to Hyperbolic"):
        # ë¶€ëª¨ ëª¨ë“ˆê³¼ ì†ì„±ëª… ì°¾ê¸°
        parts = name.split('.')
        parent = model
        for part in parts[:-1]:
            parent = getattr(parent, part)
        
        # EquivalentHyperbolicLinearë¡œ ë³€í™˜
        equiv_layer = EquivalentHyperbolicLinear.from_linear(module, c=c)
        setattr(parent, parts[-1], equiv_layer)
    
    return len(layers_to_convert)

def convert_to_rbe(model, use_fast_compression=True):
    num_converted = 0
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and not isinstance(module, RBELinear):
            parent_name = '.'.join(name.split('.')[:-1])
            child_name = name.split('.')[-1]
            parent = model
            for part in parent_name.split('.'):
                if part:
                    parent = getattr(parent, part)
            
            # ë ˆì´ì–´ í¬ê¸°ì— ë”°ë¥¸ ë¸”ë¡ í¬ê¸° ê²°ì •
            in_features = module.in_features
            out_features = module.out_features
            total_params = in_features * out_features
            
            # ë ˆì´ì–´ í¬ê¸°ì— ë”°ë¥¸ ë¸”ë¡ í¬ê¸° ì¡°ì •
            if total_params > 1_000_000:
                block_size = 32  # ëŒ€í˜• ë ˆì´ì–´ëŠ” ì‘ì€ ë¸”ë¡
            elif total_params > 100_000:
                block_size = 64  # ì¤‘í˜• ë ˆì´ì–´
            else:
                block_size = 128  # ì†Œí˜• ë ˆì´ì–´ëŠ” í° ë¸”ë¡
                
            # RBELinearë¡œ êµì²´ - ë¹ ë¥¸ ì••ì¶• ëª¨ë“œ ì‚¬ìš©
            rbe_layer = RBELinear(
                in_features, 
                out_features, 
                bias=module.bias is not None, 
                block_size=block_size,
                use_fast_compression=use_fast_compression
            )
            
            # ë¸”ë¡ ì •ë³´ ì¶œë ¥
            if hasattr(rbe_layer, 'block_info') and rbe_layer.block_info:
                info = rbe_layer.block_info
                num_blocks = info['out_blocks'] * info['in_blocks']
                print(f"   Block size: {info['block_size']}Ã—{info['block_size']}")
                print(f"   Number of blocks: {num_blocks}")
            
            # ê¸°ì¡´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ë³µì‚¬ëŠ” ì´ë¯¸ __init__ì—ì„œ ì²˜ë¦¬ë¨
            if module.bias is not None:
                rbe_layer.bias.data = module.bias.data.clone()
                
            setattr(parent, child_name, rbe_layer)
            num_converted += 1
            
            # RMSE ê³„ì‚° (ë¹ ë¥¸ ì••ì¶• ëª¨ë“œì—ì„œëŠ” ê·¼ì‚¬ì¹˜)
            if use_fast_compression:
                print(f"   Fast compression mode enabled")
            else:
                rmse = rbe_layer.get_rmse()
                print(f"   RMSE: {rmse:.6f}")
    
    return num_converted

def benchmark_generation(model, tokenizer, prompts, num_runs=3):
    """ìƒì„± ì†ë„ë¥¼ ë²¤ì¹˜ë§ˆí¬í•©ë‹ˆë‹¤"""
    times = []
    
    for _ in range(num_runs):
        start_time = time.time()
        
        for prompt in prompts:
            inputs = tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=50,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
        
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = sum(times) / len(times)
    return avg_time

def main():
    # í™˜ê²½ ì„¤ì •
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    torch.manual_seed(42)
    np.random.seed(42)
    
    # ëª¨ë¸ ë¡œë“œ
    print("Loading model...")
    try:
        config = AutoConfig.from_pretrained('hyperbolic_model/')
        tokenizer = AutoTokenizer.from_pretrained('hyperbolic_model/')
        original_model = AutoModelForCausalLM.from_pretrained(
            'hyperbolic_model/',
            torch_dtype=torch.float32  # float16 ëŒ€ì‹  float32 ì‚¬ìš©
        )
        if torch.cuda.is_available():
            original_model = original_model.cuda()
    except Exception as e:
        print(f"Error loading model: {e}")
        return
    
    # 1. ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ìƒëµ ê°€ëŠ¥)
    print("\nğŸ“Š Original model info:")
    total_params = sum(p.numel() for p in original_model.parameters())
    print(f"Total parameters: {total_params:,}")
    
    # 2. ë¹ ë¥¸ RBE ì••ì¶•
    print("\nğŸ”„ Converting to RBE with fast compression...")
    start_time = time.time()
    
    # ëª¨ë¸ì„ CPUë¡œ ì´ë™í•˜ì—¬ ì••ì¶•
    rbe_model = copy.deepcopy(original_model).cpu()
    
    # Linear ë ˆì´ì–´ë¥¼ ì§ì ‘ ì°¾ì•„ì„œ ë³€í™˜
    linear_count = 0
    for name, module in rbe_model.named_modules():
        if isinstance(module, nn.Linear):
            linear_count += 1
    
    print(f"Found {linear_count} Linear layers to compress")
    
    # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë³€í™˜
    converted = 0
    for name, module in tqdm(list(rbe_model.named_modules()), desc="Compressing layers"):
        if isinstance(module, nn.Linear) and not isinstance(module, RBELinear):
            parent_name = '.'.join(name.split('.')[:-1])
            child_name = name.split('.')[-1]
            parent = rbe_model
            for part in parent_name.split('.'):
                if part:
                    parent = getattr(parent, part)
            
            # ë ˆì´ì–´ í¬ê¸°ì— ë”°ë¥¸ ë¸”ë¡ í¬ê¸° ê²°ì •
            total_params = module.in_features * module.out_features
            if total_params > 1_000_000:
                block_size = 32
            elif total_params > 100_000:
                block_size = 64
            else:
                block_size = 128
            
            # RBELinearë¡œ êµì²´
            rbe_layer = RBELinear(
                module.in_features,
                module.out_features,
                bias=module.bias is not None,
                block_size=block_size,
                use_fast_compression=True  # ë¹ ë¥¸ ì••ì¶• ì‚¬ìš©
            )
            
            # í¸í–¥ ë³µì‚¬
            if module.bias is not None:
                rbe_layer.bias.data = module.bias.data.clone()
            
            setattr(parent, child_name, rbe_layer)
            converted += 1
    
    compression_time = time.time() - start_time
    print(f"\nâœ… Converted {converted} layers in {compression_time:.1f}s")
    print(f"Average time per layer: {compression_time/converted:.2f}s")
    
    # 3. ì••ì¶• í†µê³„
    stats = calculate_compression_stats(rbe_model)
    print(f"\nğŸ“Š Compression Statistics:")
    print(f"Original parameters: {stats['total_params']:,}")
    print(f"Compressed size: {stats['compressed_params']:,} seeds")
    print(f"Overall compression ratio: {stats['compression_ratio']:.0f}:1")
    
    # 4. GPUë¡œ ì´ë™í•˜ì—¬ ìƒì„± í…ŒìŠ¤íŠ¸
    print("\nğŸš€ Testing compressed model generation...")
    rbe_model = rbe_model.cuda()
    
    test_prompts = [
        "The meaning of life is",
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”",
        "Once upon a time",
    ]
    
    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        inputs = tokenizer(prompt, return_tensors="pt").to('cuda')
        
        with torch.no_grad():
            outputs = rbe_model.generate(
                **inputs,
                max_new_tokens=30,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"Generated: {generated}")
    
    print("\nâœ… Test completed!")
    
    # 5. ëª¨ë¸ ì €ì¥ (ì„ íƒì )
    save_compressed = input("\nSave compressed model? (y/n): ")
    if save_compressed.lower() == 'y':
        save_path = 'compressed_rbe_model'
        os.makedirs(save_path, exist_ok=True)
        
        # ì‹œë“œ ì •ë³´ ì €ì¥
        seeds_dict = encode_model_to_seeds(rbe_model)
        torch.save(seeds_dict, f'{save_path}/rbe_seeds.pt')
        
        # ì„¤ì • ì €ì¥
        config.save_pretrained(save_path)
        tokenizer.save_pretrained(save_path)
        
        print(f"Model saved to {save_path}/")
        print(f"Seeds file size: {os.path.getsize(f'{save_path}/rbe_seeds.pt') / 1024 / 1024:.2f} MB")

if __name__ == "__main__":
    main() 