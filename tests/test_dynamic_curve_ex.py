import time
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import faulthandler; faulthandler.enable()
import reality_stone as rs

# üî• ÌÜµÌï© ÏÉÅÏàò Ï†ïÏùò - ÌïòÎìúÏΩîÎî© Ï†úÍ±∞
class HyperbolicConfig:
    # Í∏∞Î≥∏ ÌïòÏù¥ÌçºÎ≥ºÎ¶≠ ÌååÎùºÎØ∏ÌÑ∞
    BASE_CURVATURE = 1e-3
    DEFAULT_T = 0.7
    DEFAULT_L = 2
    
    # ÎèôÏ†Å Í≥°Î•† Î≤îÏúÑ (ÏõêÎ≥∏ Í∏∞Ï§Ä ÏÉÅÎåÄÏ†Å)
    DYNAMIC_CURVATURE_MIN_RATIO = 0.01   # BASE_CURVATURE * 0.1
    DYNAMIC_CURVATURE_MAX_RATIO = 10000.0  # BASE_CURVATURE * 10.0
    CONSERVATIVE_MIN_RATIO = 0.5        # Î≥¥ÏàòÏ†Å Î≤ÑÏ†Ñ
    CONSERVATIVE_MAX_RATIO = 2.0
    
    # ÏàòÏπò ÏïàÏ†ïÏÑ±
    GRADIENT_CLIP_NORM = 1.0
    SAFE_CLAMP_MIN = 0.01
    SAFE_CLAMP_MAX = 100.0
    NAN_FALLBACK_ENABLED = True
    
    # Ï≤¥ÎπÑÏÖ∞ÌîÑ ÌååÎùºÎØ∏ÌÑ∞
    CHEBYSHEV_ORDER = 25
    CHEBYSHEV_SCALE = 0.5
    CHEBYSHEV_OFFSET = 0.5
    
    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî
    WEIGHT_INIT_STD_ORIGINAL = 0.01
    WEIGHT_INIT_STD_IMPROVED = 0.02
    
    # ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞
    LEARNING_RATE = 1e-3
    WEIGHT_DECAY = 1e-4
    BATCH_SIZE = 256
    EPOCHS = 10

# Í∏∞Ï°¥ ÏõêÎ≥∏ Î™®Îç∏
class GeodesicMLP(nn.Module):
    def __init__(self, in_dim=784, hid=128, out_dim=10, 
                 c=HyperbolicConfig.BASE_CURVATURE, 
                 L=HyperbolicConfig.DEFAULT_L, 
                 t=HyperbolicConfig.DEFAULT_T):
        super().__init__()
        self.c = c
        self.L = L
        self.t = t
        
        # ÏõêÎ≥∏Í≥º ÎèôÏùºÌïú Ï¥àÍ∏∞Ìôî
        std = HyperbolicConfig.WEIGHT_INIT_STD_ORIGINAL
        self.weights1 = nn.Parameter(torch.randn(in_dim, hid) * std)
        self.bias1 = nn.Parameter(torch.zeros(hid))
        self.weights2 = nn.Parameter(torch.randn(hid, hid) * std)
        self.bias2 = nn.Parameter(torch.zeros(hid))
        self.out_weights = nn.Parameter(torch.randn(hid, out_dim) * std)
        self.out_bias = nn.Parameter(torch.zeros(out_dim))

    def forward(self, x):
        x = x.view(x.size(0), -1)
        h = x @ self.weights1 + self.bias1
        h = torch.tanh(h)  
        u = h @ self.weights2 + self.bias2
        u = torch.sigmoid(u) 
        z = rs.poincare_ball_layer(h, u, self.c, self.t)
        
        if HyperbolicConfig.NAN_FALLBACK_ENABLED and torch.isnan(z).any():
            z = h
            
        output = z @ self.out_weights + self.out_bias
        return output

# üî• Í∞úÏÑ†Îêú Ï≤¥ÎπÑÏÖ∞ÌîÑ Î™®Îç∏
class ChebyshevMLP(nn.Module):
    def __init__(self, in_dim=784, hid=128, out_dim=10, 
                 c=HyperbolicConfig.BASE_CURVATURE, 
                 L=HyperbolicConfig.DEFAULT_L, 
                 t=HyperbolicConfig.DEFAULT_T):
        super().__init__()
        self.c = c
        self.L = L
        self.t = t
        
        # Í∞úÏÑ†Îêú Ï¥àÍ∏∞Ìôî
        std = HyperbolicConfig.WEIGHT_INIT_STD_IMPROVED
        self.weights1 = nn.Parameter(torch.randn(in_dim, hid) * std)
        self.bias1 = nn.Parameter(torch.zeros(hid))
        self.weights2 = nn.Parameter(torch.randn(hid, hid) * std)
        self.bias2 = nn.Parameter(torch.zeros(hid))
        self.out_weights = nn.Parameter(torch.randn(hid, out_dim) * std)
        self.out_bias = nn.Parameter(torch.zeros(out_dim))

    def forward(self, x):
        x = x.view(x.size(0), -1)
        h = x @ self.weights1 + self.bias1
        
        # Ï≤¥ÎπÑÏÖ∞ÌîÑ Í∑ºÏÇ¨ Ï†ÅÏö©
        h = rs.chebyshev_approximation(h, order=HyperbolicConfig.CHEBYSHEV_ORDER, curvature=self.c)
        
        u = h @ self.weights2 + self.bias2
        u = rs.chebyshev_approximation(
            u * HyperbolicConfig.CHEBYSHEV_SCALE, 
            order=HyperbolicConfig.CHEBYSHEV_ORDER, 
            curvature=self.c
        ) * HyperbolicConfig.CHEBYSHEV_SCALE + HyperbolicConfig.CHEBYSHEV_OFFSET
        
        z = rs.poincare_ball_layer(h, u, self.c, self.t)
        
        if HyperbolicConfig.NAN_FALLBACK_ENABLED and torch.isnan(z).any():
            z = h
            
        output = z @ self.out_weights + self.out_bias
        return output

# üöÄ ÏàòÏ†ïÎêú ÎèôÏ†Å Í≥°Î•† Î™®Îç∏ - ÌïòÏù¥ÌçºÎ≥ºÎ¶≠ Ïù∏Ïãù Î≤ÑÏ†Ñ
class ImprovedDynamicCurvatureMLP(nn.Module):
    def __init__(self, in_dim=784, hid=128, out_dim=10, 
                 c=HyperbolicConfig.BASE_CURVATURE, 
                 L=HyperbolicConfig.DEFAULT_L, 
                 t=HyperbolicConfig.DEFAULT_T):
        super().__init__()
        self.base_c = c
        self.L = L
        self.t = t
        
        # Í∞úÏÑ†Îêú Ï¥àÍ∏∞Ìôî
        std = HyperbolicConfig.WEIGHT_INIT_STD_IMPROVED
        self.weights1 = nn.Parameter(torch.randn(in_dim, hid) * std)
        self.bias1 = nn.Parameter(torch.zeros(hid))
        self.weights2 = nn.Parameter(torch.randn(hid, hid) * std)
        self.bias2 = nn.Parameter(torch.zeros(hid))
        self.out_weights = nn.Parameter(torch.randn(hid, out_dim) * std)
        self.out_bias = nn.Parameter(torch.zeros(out_dim))
        
        # üî• ÌïòÏù¥ÌçºÎ≥ºÎ¶≠ Ïù∏Ïãù Í≥°Î•† ÏòàÏ∏°Í∏∞ (Ïú†ÌÅ¥Î¶¨ÎîîÏïà ÏÑ†Ìòï Î†àÏù¥Ïñ¥ Ï†úÍ±∞)
        self.curvature_features = nn.Parameter(torch.randn(16) * 0.01)
        self.curvature_scale = nn.Parameter(torch.ones(1) * 0.1)
        
        # Í≥°Î•† Î≤îÏúÑ (ÏÉÅÏàò ÏÇ¨Ïö©)
        self.min_curvature = self.base_c * HyperbolicConfig.DYNAMIC_CURVATURE_MIN_RATIO
        self.max_curvature = self.base_c * HyperbolicConfig.DYNAMIC_CURVATURE_MAX_RATIO

    def predict_adaptive_curvature(self, x_flat):
        """ÌïòÏù¥ÌçºÎ≥ºÎ¶≠ Í∏∞ÌïòÌïôÏùÑ Í≥†Î†§Ìïú Í≥°Î•† ÏòàÏ∏°"""
        batch_size = x_flat.size(0)
        
        # 1. ÏûÖÎ†•Ïùò Í∏∞ÌïòÌïôÏ†Å ÌäπÏÑ± (ÌïòÏù¥ÌçºÎ≥ºÎ¶≠ ÎÖ∏Î¶Ñ)
        norms = torch.norm(x_flat, dim=1, keepdim=True)  # [B, 1]
        
        # 2. ÌïôÏäµ Í∞ÄÎä•Ìïú ÌäπÏßïÍ≥ºÏùò ÏÉÅÌò∏ÏûëÏö© (Ï∞®Ïõê ÏàòÏ†ï)
        # ÏûÖÎ†•ÏùÑ 16Ï∞®ÏõêÏúºÎ°ú Ï∂ïÏÜåÌïòÏó¨ featureÏôÄ Îß§Ïπ≠
        x_reduced = torch.mm(x_flat, torch.randn(x_flat.size(1), 16, device=x_flat.device))  # [B, 16]
        feature_interaction = torch.sum(x_reduced * self.curvature_features.unsqueeze(0), dim=1, keepdim=True)  # [B, 1]
        
        # 3. ÏïàÏ†ÑÌïú Í≥°Î•† Î≤îÏúÑÎ°ú Ïä§ÏºÄÏùºÎßÅ
        curvature_adjustment = torch.sigmoid(feature_interaction + self.curvature_scale)  # [B, 1]
        curvatures = self.min_curvature + (self.max_curvature - self.min_curvature) * curvature_adjustment  # [B, 1]
        
        return curvatures.squeeze(-1)  # [B]

    def forward(self, x):
        x_flat = x.view(x.size(0), -1)
        h = x_flat @ self.weights1 + self.bias1
        h = torch.tanh(h)  
        u = h @ self.weights2 + self.bias2
        u = torch.sigmoid(u)
        
        try:
            # üî• Í∞úÏÑ†Îêú ÎèôÏ†Å Í≥°Î•† ÏòàÏ∏°
            curvatures = self.predict_adaptive_curvature(x_flat)
            
            # Î∞∞ÏπòÎ≥Ñ Í∞úÎ≥Ñ Ï≤òÎ¶¨ (ÌèâÍ∑†Ìôî Ï†úÍ±∞)
            z_list = []
            for i in range(h.size(0)):
                h_i = h[i:i+1]
                u_i = u[i:i+1]
                c_i = torch.clamp(curvatures[i], self.min_curvature, self.max_curvature).item()
                
                z_i = rs.poincare_ball_layer(h_i, u_i, c_i, self.t)
                if torch.isnan(z_i).any():
                    z_i = h_i
                z_list.append(z_i)
            
            z = torch.cat(z_list, dim=0)
            
        except Exception as e:
            print(f"üîß Improved dynamic curvature failed: {e}")
            z = rs.poincare_ball_layer(h, u, self.base_c, self.t)
            if HyperbolicConfig.NAN_FALLBACK_ENABLED and torch.isnan(z).any():
                z = h
        
        output = z @ self.out_weights + self.out_bias
        return output

# üéØ Î≥¥ÏàòÏ†Å ÎèôÏ†Å Í≥°Î•† Î™®Îç∏ - Í≥°Î•† Î≤îÏúÑÎßå Ï∂ïÏÜå
class ConservativeDynamicCurvatureMLP(nn.Module):
    def __init__(self, in_dim=784, hid=128, out_dim=10, 
                 c=HyperbolicConfig.BASE_CURVATURE, 
                 L=HyperbolicConfig.DEFAULT_L, 
                 t=HyperbolicConfig.DEFAULT_T):
        super().__init__()
        self.base_c = c
        self.L = L
        self.t = t
        
        std = HyperbolicConfig.WEIGHT_INIT_STD_IMPROVED
        self.weights1 = nn.Parameter(torch.randn(in_dim, hid) * std)
        self.bias1 = nn.Parameter(torch.zeros(hid))
        self.weights2 = nn.Parameter(torch.randn(hid, hid) * std)
        self.bias2 = nn.Parameter(torch.zeros(hid))
        self.out_weights = nn.Parameter(torch.randn(hid, out_dim) * std)
        self.out_bias = nn.Parameter(torch.zeros(out_dim))
        
        # Î≥¥ÏàòÏ†Å Í≥°Î•† ÏòàÏ∏°Í∏∞ (ÏõêÎûò Î∞©Ïãù Ïú†ÏßÄÌïòÎêò Î≤îÏúÑÎßå Ï∂ïÏÜå)
        self.curvature_predictor = nn.Sequential(
            nn.Linear(in_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )
        
        # Î≥¥ÏàòÏ†Å Í≥°Î•† Î≤îÏúÑ
        self.min_curvature = self.base_c * HyperbolicConfig.CONSERVATIVE_MIN_RATIO
        self.max_curvature = self.base_c * HyperbolicConfig.CONSERVATIVE_MAX_RATIO

    def forward(self, x):
        x_flat = x.view(x.size(0), -1)
        h = x_flat @ self.weights1 + self.bias1
        h = torch.tanh(h)  
        u = h @ self.weights2 + self.bias2
        u = torch.sigmoid(u)
        
        try:
            # üî• Î≥¥ÏàòÏ†Å Í≥°Î•† ÏòàÏ∏° (ÏõêÎ≥∏ Í∑ºÏ≤ò Î≤îÏúÑ)
            c_pred = self.curvature_predictor(x_flat)
            c_range = self.min_curvature + (self.max_curvature - self.min_curvature) * c_pred
            c_avg = torch.clamp(c_range.mean(), self.min_curvature, self.max_curvature).item()
            
            z = rs.poincare_ball_layer(h, u, c_avg, self.t)
            if torch.isnan(z).any():
                z = h
                
        except Exception as e:
            print(f"üîß Conservative curvature failed: {e}")
            z = rs.poincare_ball_layer(h, u, self.base_c, self.t)
            if HyperbolicConfig.NAN_FALLBACK_ENABLED and torch.isnan(z).any():
                z = h
            
        output = z @ self.out_weights + self.out_bias
        return output

# üß™ Îã®Ïàú ÎèôÏ†Å Í≥°Î•† Î™®Îç∏ - ÌïôÏäµ Í∞ÄÎä•Ìïú Ïä§ÏºÄÏùºÎßÅ
class SimpleDynamicCurvatureMLP(nn.Module):
    def __init__(self, in_dim=784, hid=128, out_dim=10, 
                 c=HyperbolicConfig.BASE_CURVATURE, 
                 L=HyperbolicConfig.DEFAULT_L, 
                 t=HyperbolicConfig.DEFAULT_T):
        super().__init__()
        self.base_c = c
        self.L = L  
        self.t = t
        
        std = HyperbolicConfig.WEIGHT_INIT_STD_IMPROVED
        self.weights1 = nn.Parameter(torch.randn(in_dim, hid) * std)
        self.bias1 = nn.Parameter(torch.zeros(hid))
        self.weights2 = nn.Parameter(torch.randn(hid, hid) * std)
        self.bias2 = nn.Parameter(torch.zeros(hid))
        self.out_weights = nn.Parameter(torch.randn(hid, out_dim) * std)
        self.out_bias = nn.Parameter(torch.zeros(out_dim))
        
        # üéØ Í∞ÄÏû• Í∞ÑÎã®Ìïú Ï†ëÍ∑º: ÌïôÏäµ Í∞ÄÎä•Ìïú Í≥°Î•† Ïä§ÏºÄÏùº
        self.curvature_scale = nn.Parameter(torch.ones(1))

    def forward(self, x):
        x_flat = x.view(x.size(0), -1)
        h = x_flat @ self.weights1 + self.bias1
        h = torch.tanh(h)
        u = h @ self.weights2 + self.bias2
        u = torch.sigmoid(u)
        
        # üî• Îã®ÏàúÌïú Ï†ÅÏùëÏ†Å Í≥°Î•†
        adaptive_c = self.base_c * (HyperbolicConfig.CONSERVATIVE_MIN_RATIO + 
                                   (HyperbolicConfig.CONSERVATIVE_MAX_RATIO - HyperbolicConfig.CONSERVATIVE_MIN_RATIO) * 
                                   torch.sigmoid(self.curvature_scale).item())
        
        z = rs.poincare_ball_layer(h, u, adaptive_c, self.t)
        if HyperbolicConfig.NAN_FALLBACK_ENABLED and torch.isnan(z).any():
            z = h
            
        output = z @ self.out_weights + self.out_bias
        return output

# üé™ Ï¥àÎã®Ïàú ÎèôÏ†Å Í≥°Î•† Î™®Îç∏ - Í∏ÄÎ°úÎ≤å Ïä§ÏºÄÏùºÎßÅÎßå
class SuperSimpleDynamicCurvatureMLP(nn.Module):
    def __init__(self, in_dim=784, hid=128, out_dim=10, 
                 c=HyperbolicConfig.BASE_CURVATURE, 
                 L=HyperbolicConfig.DEFAULT_L, 
                 t=HyperbolicConfig.DEFAULT_T):
        super().__init__()
        self.base_c = c
        self.L = L  
        self.t = t
        
        std = HyperbolicConfig.WEIGHT_INIT_STD_IMPROVED
        self.weights1 = nn.Parameter(torch.randn(in_dim, hid) * std)
        self.bias1 = nn.Parameter(torch.zeros(hid))
        self.weights2 = nn.Parameter(torch.randn(hid, hid) * std)
        self.bias2 = nn.Parameter(torch.zeros(hid))
        self.out_weights = nn.Parameter(torch.randn(hid, out_dim) * std)
        self.out_bias = nn.Parameter(torch.zeros(out_dim))
        
        # üéØ Ï¥àÎã®Ïàú: ÌïòÎÇòÏùò Í≥°Î•† Î∞∞Ïú®Îßå ÌïôÏäµ
        self.curvature_multiplier = nn.Parameter(torch.tensor(1.0))

    def forward(self, x):
        x_flat = x.view(x.size(0), -1)
        h = x_flat @ self.weights1 + self.bias1
        h = torch.tanh(h)
        u = h @ self.weights2 + self.bias2
        u = torch.sigmoid(u)
        
        # üî• Í∞ÄÏû• Îã®ÏàúÌïú Ï†ÅÏùëÏ†Å Í≥°Î•†: base_c * learnable_multiplier
        adaptive_c = self.base_c * torch.clamp(self.curvature_multiplier, 
                                             HyperbolicConfig.CONSERVATIVE_MIN_RATIO, 
                                             HyperbolicConfig.CONSERVATIVE_MAX_RATIO)
        
        z = rs.poincare_ball_layer(h, u, adaptive_c, self.t)
        if HyperbolicConfig.NAN_FALLBACK_ENABLED and torch.isnan(z).any():
            z = h
            
        output = z @ self.out_weights + self.out_bias
        return output

# üî• ÏµúÏ†ÅÌôîÎêú ÌõàÎ†® Ìï®Ïàò
def train_epoch(model, loader, optimizer, device):
    model.train()
    total_loss = 0.0
    t0 = time.time()
    
    # üöÄ ÏÑ±Îä• ÏµúÏ†ÅÌôî
    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None
    
    for imgs, labels in loader:
        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
        optimizer.zero_grad()
        
        try:
            # Mixed precision training (CUDA only)
            if scaler is not None:
                with torch.cuda.amp.autocast():
                    logits = model(imgs)
                    loss = nn.functional.cross_entropy(logits, labels)
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=HyperbolicConfig.GRADIENT_CLIP_NORM)
                scaler.step(optimizer)
                scaler.update()
            else:
                logits = model(imgs)
                loss = nn.functional.cross_entropy(logits, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=HyperbolicConfig.GRADIENT_CLIP_NORM)
                optimizer.step()
            
            total_loss += loss.item() * imgs.size(0)
        except Exception as e:
            print(f"Training error: {e}")
            continue
            
    return total_loss / len(loader.dataset), time.time() - t0

def test_epoch(model, loader, device):
    model.eval()
    correct = 0
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)
            try:
                pred = model(imgs).argmax(dim=1)
                correct += (pred == labels).sum().item()
            except:
                continue
    return correct / len(loader.dataset)

def train_model(model_name, model, loader_train, loader_test, epochs, lr, device):
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=HyperbolicConfig.WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    print(f"\n--- {model_name} Training ---")
    test_accs = []
    
    for ep in range(1, epochs+1):
        loss, t = train_epoch(model, loader_train, optimizer, device)
        acc = test_epoch(model, loader_test, device)
        test_accs.append(acc)
        scheduler.step()
        
        print(f"[{model_name}] Epoch {ep}/{epochs} loss={loss:.4f} time={t:.2f}s acc={acc*100:.2f}%")
    
    best_acc = max(test_accs) * 100
    print(f"[{model_name}] Best accuracy: {best_acc:.2f}%")
    return best_acc

def check_reality_stone():
    print("=== Reality Stone Status Check (Improved) ===")
    try:
        x = torch.randn(4, 10)
        result = rs.poincare_ball_layer(x, x, HyperbolicConfig.BASE_CURVATURE, HyperbolicConfig.DEFAULT_T)
        print("‚úì poincare_ball_layer: OK")
        
        try:
            result = rs.chebyshev_approximation(x, order=HyperbolicConfig.CHEBYSHEV_ORDER, curvature=1.0)
            print("‚úì chebyshev_approximation: OK")
        except Exception as e:
            print(f"‚úó chebyshev_approximation: {e}")
            
        # ÎèôÏ†Å Í≥°Î•† ÌÖåÏä§Ìä∏ (Ï†ÅÏ†àÌïú Î≤îÏúÑ)
        try:
            features = torch.norm(x, dim=1, keepdim=True)
            weight = torch.randn(1, 1) * 0.1
            bias = torch.zeros(1)
            result = rs.predict_dynamic_curvature(
                features, weight, bias, 
                base_curvature=HyperbolicConfig.BASE_CURVATURE,    
                min_curvature=HyperbolicConfig.BASE_CURVATURE * HyperbolicConfig.DYNAMIC_CURVATURE_MIN_RATIO,    
                max_curvature=HyperbolicConfig.BASE_CURVATURE * HyperbolicConfig.DYNAMIC_CURVATURE_MAX_RATIO    
            )
            print(f"‚úì dynamic_curvature_pred: OK, range=[{result.min():.2e}, {result.max():.2e}]")
        except Exception as e:
            print(f"‚úó dynamic_curvature_pred: {e}")
            
        # Í≤ΩÍ≥Ñ ÌéòÎÑêÌã∞ ÌÖåÏä§Ìä∏
        try:
            penalty = rs.boundary_penalty(x, curvature=HyperbolicConfig.BASE_CURVATURE, epsilon=0.001)
            print(f"‚úì boundary_penalty: OK, value={penalty.item():.6f}")
        except Exception as e:
            print(f"‚úó boundary_penalty: {e}")
            
    except Exception as e:
        print(f"‚úó Reality Stone basic test failed: {e}")
    print("="*60)

if __name__ == "__main__":
    # ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÉÅÌÉú Ï≤¥ÌÅ¨
    check_reality_stone()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # ÏÉÅÏàò ÏÇ¨Ïö©
    batch_size = HyperbolicConfig.BATCH_SIZE
    lr = HyperbolicConfig.LEARNING_RATE
    epochs = HyperbolicConfig.EPOCHS
    
    # üöÄ Îπ†Î•∏ ÌÖåÏä§Ìä∏ Î™®Îìú (ÌïÑÏöîÏãú ÌôúÏÑ±Ìôî)
    QUICK_TEST = True  # Îπ†Î•∏ ÌÖåÏä§Ìä∏Ïö©
    if QUICK_TEST:
        epochs = 10
        print("üöÄ Quick Test Mode: 10 epochs only")
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_ds = datasets.MNIST('.', train=True, download=True, transform=transform)
    test_ds = datasets.MNIST('.', train=False, download=True, transform=transform)
    
    # üöÄ ÏÑ±Îä• ÏµúÏ†ÅÌôîÎêú DataLoader
    train_loader = torch.utils.data.DataLoader(
        train_ds, batch_size=batch_size, shuffle=True,
        num_workers=4, pin_memory=True, persistent_workers=True
    )
    test_loader = torch.utils.data.DataLoader(
        test_ds, batch_size=batch_size, shuffle=False,
        num_workers=4, pin_memory=True, persistent_workers=True
    )
    
    # üî• Îã§ÏñëÌïú Î™®Îç∏ ÎπÑÍµê (Îã§Ïù¥ÎÇòÎØπ Î™®Îç∏ Ïö∞ÏÑ† ÌÖåÏä§Ìä∏)
    models = {
        "üé™ SuperSimpleDynamic": SuperSimpleDynamicCurvatureMLP(),
        "üß™ SimpleDynamic": SimpleDynamicCurvatureMLP(),
        "üõ°Ô∏è ConservativeDynamic": ConservativeDynamicCurvatureMLP(),
        "üìä Original": GeodesicMLP(),
    }
    
    results = {}
    for name, model in models.items():
        print(f"\n{'='*50}")
        print(f"Training {name}")
        print(f"{'='*50}")
        
        model = model.to(device)
        try:
            acc = train_model(name, model, train_loader, test_loader, epochs, lr, device)
            results[name] = acc
        except Exception as e:
            print(f"‚ùå {name} failed: {e}")
            results[name] = 0.0
    
    # üéØ Í≤∞Í≥º Î∂ÑÏÑù
    print(f"\n{'='*60}")
    print("üéØ COMPREHENSIVE RESULTS")
    print(f"{'='*60}")
    
    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
    for name, acc in sorted_results:
        if acc > 95:
            status = "üî• EXCELLENT"
        elif acc > 90:
            status = "‚úÖ GOOD"
        elif acc > 80:
            status = "‚ö†Ô∏è  FAIR"
        else:
            status = "‚ùå POOR"
        print(f"{name:25}: {acc:6.2f}% {status}")
    
    # Í∞úÏÑ†ÎèÑ Î∂ÑÏÑù
    if 'üìä Original' in results and results['üìä Original'] > 0:
        orig_acc = results["üìä Original"]
        print(f"\nüìà Improvements over Original ({orig_acc:.2f}%):")
        
        for name, acc in results.items():
            if name != 'üìä Original' and acc > 0:
                improvement = acc - orig_acc
                symbol = "üî•" if improvement > 1.0 else "‚úÖ" if improvement > 0 else "‚ùå"
                print(f"{symbol} {name:25}: {improvement:+5.2f}%")
    
    print(f"\nüîç DIAGNOSIS:")
    best_name, best_acc = max(results.items(), key=lambda x: x[1])
    print(f"üèÜ Best Model: {best_name} ({best_acc:.2f}%)")
    
    if best_acc > 95:
        print("‚úÖ Excellent performance achieved!")
    elif best_acc > 90:
        print("‚úÖ Good performance - room for improvement")
    else:
        print("‚ö†Ô∏è  Performance below expectations - check implementation")
        
    print(f"\nüîß Configuration Used:")
    print(f"   Base Curvature: {HyperbolicConfig.BASE_CURVATURE}")
    print(f"   Dynamic Range: {HyperbolicConfig.DYNAMIC_CURVATURE_MIN_RATIO}x - {HyperbolicConfig.DYNAMIC_CURVATURE_MAX_RATIO}x")
    print(f"   Conservative Range: {HyperbolicConfig.CONSERVATIVE_MIN_RATIO}x - {HyperbolicConfig.CONSERVATIVE_MAX_RATIO}x")
    print(f"   Learning Rate: {HyperbolicConfig.LEARNING_RATE}")
    print(f"   Batch Size: {HyperbolicConfig.BATCH_SIZE}")