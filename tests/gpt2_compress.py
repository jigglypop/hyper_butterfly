"""GPT-2 ëª¨ë¸ ë¹„íŠ¸í•„ë“œ ì••ì¶• í…ŒìŠ¤íŠ¸ (3D í…ì„œ ì§ì ‘ ì§€ì›)"""

import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from reality_stone.layers import BitfieldLinear
import time
from tqdm import tqdm
import numpy as np


def find_all_layers(model):
    """ëª¨ë¸ì—ì„œ ëª¨ë“  Linear ë° Conv1D ë ˆì´ì–´ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    layers = []
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            layers.append((name, module, 'Linear'))
        elif hasattr(module, 'weight') and len(module.weight.shape) == 2:
            # Conv1D ë ˆì´ì–´ (transformers.pytorch_utils.Conv1D)
            if 'Conv1D' in str(type(module)):
                layers.append((name, module, 'Conv1D'))
    
    return layers


def compress_model_with_bitfield(model, basis_size=256, r_max=1.0):
    """ëª¨ë¸ì˜ ëª¨ë“  Linear/Conv1D ë ˆì´ì–´ë¥¼ BitfieldLinearë¡œ ì••ì¶•í•©ë‹ˆë‹¤."""
    compressed_layers = []
    
    # ëª¨ë“  ë ˆì´ì–´ ì°¾ê¸°
    all_layers = find_all_layers(model)
    
    print(f"ğŸ”· ë¹„íŠ¸í•„ë“œ ì••ì¶• ì‹œì‘ (basis_size={basis_size})")
    print(f"\nì••ì¶• ëŒ€ìƒ ë ˆì´ì–´: {len(all_layers)}ê°œ")
    
    for name, layer, layer_type in tqdm(all_layers, desc="ë ˆì´ì–´ ì••ì¶•"):
        try:
            if layer_type == 'Conv1D':
                # Conv1DëŠ” ê°€ì¤‘ì¹˜ê°€ ì „ì¹˜ë˜ì–´ ìˆìŒ
                weight = layer.weight.t()  # [in_features, out_features] â†’ [out_features, in_features]
                bias = layer.bias if hasattr(layer, 'bias') else None
                
                # ì„ì‹œ Linear ë ˆì´ì–´ ìƒì„±
                temp_linear = nn.Linear(weight.shape[1], weight.shape[0], bias=bias is not None)
                temp_linear.weight.data = weight
                if bias is not None:
                    temp_linear.bias.data = bias
                
                # BitfieldLinearë¡œ ë³€í™˜
                bitfield_layer = BitfieldLinear.from_linear(temp_linear, basis_size, r_max)
                
            else:  # Linear
                # ì§ì ‘ BitfieldLinearë¡œ ë³€í™˜
                bitfield_layer = BitfieldLinear.from_linear(layer, basis_size, r_max)
            
            # ëª¨ë¸ì—ì„œ ë ˆì´ì–´ êµì²´
            parent_name = '.'.join(name.split('.')[:-1])
            child_name = name.split('.')[-1]
            
            if parent_name:
                parent_module = model.get_submodule(parent_name)
                setattr(parent_module, child_name, bitfield_layer)
            else:
                setattr(model, child_name, bitfield_layer)
            
            compressed_layers.append((name, layer_type, bitfield_layer))
            
        except Exception as e:
            print(f"âš ï¸ ë ˆì´ì–´ {name} ì••ì¶• ì‹¤íŒ¨: {e}")
            continue
    
    return compressed_layers


def calculate_compression_stats(original_model, compressed_layers):
    """ì••ì¶•ë¥  í†µê³„ ê³„ì‚°"""
    original_params = 0
    compressed_params = 0
    
    for name, layer_type, bitfield_layer in compressed_layers:
        # ì›ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        original_size = bitfield_layer.in_features * bitfield_layer.out_features
        original_params += original_size
        
        # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° (22ë¹„íŠ¸ ì¸ì½”ë”©)
        compressed_size = bitfield_layer.out_features * 22 / 32  # 22ë¹„íŠ¸ë¥¼ 32ë¹„íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
        compressed_params += compressed_size
    
    compression_ratio = original_params / compressed_params if compressed_params > 0 else 0
    
    return {
        'original_params': original_params,
        'compressed_params': compressed_params,
        'compression_ratio': compression_ratio,
        'layers_compressed': len(compressed_layers)
    }


def test_compressed_model():
    """ì••ì¶•ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("GPT-2 ë¹„íŠ¸í•„ë“œ ì••ì¶• í…ŒìŠ¤íŠ¸ (3D í…ì„œ ì§ì ‘ ì§€ì›)")
    print("="*60)
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ”„ GPT-2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    
    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ì›ë³¸ ëª¨ë¸ í¬ê¸° í™•ì¸
    original_param_count = sum(p.numel() for p in model.parameters())
    print(f"ì›ë³¸ ëª¨ë¸ í¬ê¸°: {original_param_count:,} íŒŒë¼ë¯¸í„°")
    
    # ëª¨ë¸ ì••ì¶•
    print(f"\n[1] ì „ì²´ ë ˆì´ì–´ ë¹„íŠ¸í•„ë“œ ì••ì¶•")
    compressed_layers = compress_model_with_bitfield(model, basis_size=256, r_max=1.0)
    
    # ì••ì¶• í†µê³„
    stats = calculate_compression_stats(model, compressed_layers)
    print(f"\nì••ì¶• ì™„ë£Œ:")
    print(f"  - ì›ë³¸ íŒŒë¼ë¯¸í„°: {stats['original_params']:,} bytes")
    print(f"  - ì••ì¶• íŒŒë¼ë¯¸í„°: {stats['compressed_params']:,.0f} bytes")
    print(f"  - ì••ì¶•ë¥ : {stats['compression_ratio']:.1f}x" if stats['compression_ratio'] > 0 else "  - ì••ì¶•ë¥ : ê³„ì‚° ë¶ˆê°€")
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_texts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "The quick brown fox jumps over the lazy dog",
        "Python is a programming language",
        "Machine learning is transforming the world"
    ]
    
    print(f"\n=== ì „ì²´ì••ì¶• í…ŒìŠ¤íŠ¸ ===")
    
    for i, text in enumerate(test_texts):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i+1}] ì…ë ¥: '{text}'")
        
        try:
            # ì…ë ¥ í† í°í™”
            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
            
            # ëª¨ë¸ ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
            
            inference_time = time.time() - start_time
            
            # ë‹¤ìŒ í† í° ì˜ˆì¸¡
            predicted_token_id = torch.argmax(logits[0, -1, :]).item()
            predicted_token = tokenizer.decode([predicted_token_id])
            
            print(f"  ì¶œë ¥ í˜•íƒœ: {logits.shape}")
            print(f"  ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ")
            print(f"  ë‹¤ìŒ í† í° ì˜ˆì¸¡: '{predicted_token}'")
            
            # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
            generated = model.generate(
                inputs.input_ids,
                max_new_tokens=10,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id
            )
            
            generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
            print(f"  ìƒì„±ëœ í…ìŠ¤íŠ¸: '{generated_text}'")
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
    
    # ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
    print(f"\n=== ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ===")
    
    # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    batch_texts = test_texts[:2]  # ì²˜ìŒ 2ê°œ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
    
    try:
        # ë°°ì¹˜ í† í°í™”
        batch_inputs = tokenizer(
            batch_texts, 
            return_tensors='pt', 
            padding=True, 
            truncation=True,
            max_length=50
        )
        
        print(f"ë°°ì¹˜ ì…ë ¥ í˜•íƒœ: {batch_inputs.input_ids.shape}")
        
        # ë°°ì¹˜ ì¶”ë¡ 
        start_time = time.time()
        
        with torch.no_grad():
            batch_outputs = model(**batch_inputs)
            batch_logits = batch_outputs.logits
        
        batch_inference_time = time.time() - start_time
        
        print(f"ë°°ì¹˜ ì¶œë ¥ í˜•íƒœ: {batch_logits.shape}")
        print(f"ë°°ì¹˜ ì¶”ë¡  ì‹œê°„: {batch_inference_time:.3f}ì´ˆ")
        print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {batch_inference_time/len(batch_texts):.3f}ì´ˆ/ìƒ˜í”Œ")
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\n=== ìµœì¢… ê²°ê³¼ ===")
    print(f"âœ… ì••ì¶• ì„±ê³µ: {stats['layers_compressed']}ê°œ ë ˆì´ì–´")
    print(f"âœ… ì••ì¶•ë¥ : {stats['compression_ratio']:.1f}x" if stats['compression_ratio'] > 0 else "âŒ ì••ì¶•ë¥  ê³„ì‚° ì‹¤íŒ¨")
    print(f"âœ… 3D í…ì„œ ì§ì ‘ ì²˜ë¦¬: reshape ì˜¤ë²„í—¤ë“œ ì œê±°")
    print(f"âœ… ëª¨ë¸ ë™ì‘: ì •ìƒ" if 'generated_text' in locals() else "âŒ ëª¨ë¸ ë™ì‘: ì˜¤ë¥˜")


if __name__ == "__main__":
    test_compressed_model() 