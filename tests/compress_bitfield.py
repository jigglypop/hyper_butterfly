import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.pytorch_utils import Conv1D
import copy
from tqdm import tqdm

try:
    import reality_stone as rs
    from reality_stone.layers import BitfieldLinear
    RS_AVAILABLE = True
except ImportError:
    RS_AVAILABLE = False
    print("âš ï¸ RealityStoneì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit(1)


class ProperBitfieldLinear(nn.Module):
    """
    ë…¼ë¬¸ì— ì¶©ì‹¤í•œ ë¹„íŠ¸í•„ë“œ ì••ì¶• ì„ í˜• ë ˆì´ì–´
    - ê°€ì¤‘ì¹˜ ì „ì²´ë¥¼ ë¹„íŠ¸í•„ë“œë¡œ ì••ì¶•
    - ì¶”ë¡  ì‹œ ì›ë˜ normì„ ë³µì›í•˜ì—¬ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”
    """
    def __init__(self, linear_layer: nn.Linear, basis_size: int = 256):
        super().__init__()
        self.in_features = linear_layer.in_features
        self.out_features = linear_layer.out_features

        W = linear_layer.weight.data

        # 1. ê° í–‰ì˜ norm ê³„ì‚° ë° ì €ì¥
        weight_norms = torch.norm(W, p=2, dim=1)
        self.register_buffer('weight_norms', weight_norms)

        # 2. ê°€ì¤‘ì¹˜ ì •ê·œí™” (0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ë°©ì§€)
        W_normalized = W / (weight_norms.unsqueeze(1) + 1e-8)
        
        # ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ë¡œ ì„ì‹œ Linear ë ˆì´ì–´ ìƒì„±
        normalized_linear = nn.Linear(self.in_features, self.out_features, bias=False)
        normalized_linear.weight.data = W_normalized
        
        # 3. ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¹„íŠ¸í•„ë“œë¡œ ì••ì¶•
        # r_max=1.0ì€ normì´ 1 ì´í•˜ì¸ ë²¡í„°ì— ì í•©
        self.bitfield = BitfieldLinear.from_linear(
            normalized_linear,
            basis_size=basis_size,
            r_max=1.0
        )
        
        # ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
        if linear_layer.bias is not None:
            self.bias = nn.Parameter(linear_layer.bias.data.clone())
        else:
            self.register_parameter('bias', None)
    
    def forward(self, x):
        # 3D ì…ë ¥ì„ 2Dë¡œ ë³€í™˜: (B, S, F_in) -> (B*S, F_in)
        input_shape = x.shape
        if x.dim() == 3:
            x_reshaped = x.reshape(-1, self.in_features)
        else:
            x_reshaped = x
            
        # 2D í…ì„œë¡œ ì••ì¶• ë ˆì´ì–´ ì‹¤í–‰ (ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ ê¸°ì¤€)
        output_normalized = self.bitfield(x_reshaped)
        
        # 4. ì €ì¥ëœ normì„ ê³±í•˜ì—¬ ìŠ¤ì¼€ì¼ ë³µì›
        output_restored = output_normalized * self.weight_norms
        
        # 2D ì¶œë ¥ì„ ì›ë˜ 3D ì°¨ì›ìœ¼ë¡œ ë³µì›
        if x.dim() == 3:
            batch_size, seq_len, _ = input_shape
            output = output_restored.view(batch_size, seq_len, self.out_features)
        else:
            output = output_restored
            
        # ë°”ì´ì–´ìŠ¤ ì¶”ê°€
        if self.bias is not None:
            output += self.bias
        return output


def apply_bitfield_compression_to_model(model, basis_size=256, target_layers=None):
    """
    ëª¨ë¸ì˜ ì„ í˜• ë ˆì´ì–´ë¥¼ ë¹„íŠ¸í•„ë“œ ì••ì¶• ë ˆì´ì–´ë¡œ êµì²´
    
    Args:
        model: ì••ì¶•í•  ëª¨ë¸
        basis_size: ê¸°ì € ë²¡í„° í…Œì´ë¸” í¬ê¸° (ë…¼ë¬¸: 256 ê¶Œì¥)
        target_layers: ì••ì¶•í•  ë ˆì´ì–´ ì´ë¦„ íŒ¨í„´ (Noneì´ë©´ ëª¨ë‘ ì••ì¶•)
    """
    print(f"\nğŸ”· ë¹„íŠ¸í•„ë“œ ì••ì¶• ì‹œì‘ (basis_size={basis_size})")
    
    # êµì²´í•  ë ˆì´ì–´ ëª©ë¡ ìˆ˜ì§‘
    layers_to_replace = []
    total_params_before = 0
    total_params_after = 0
    
    for name, module in model.named_modules():
        # Linear ë ˆì´ì–´ì™€ transformersì˜ Conv1D ë ˆì´ì–´ ëŒ€ìƒ
        if isinstance(module, (nn.Linear, Conv1D)):
            # target_layersê°€ Noneì´ë©´ ëª¨ë“  ë ˆì´ì–´ ì••ì¶•
            if target_layers is None:
                layers_to_replace.append(name)
            # target_layersê°€ ìˆìœ¼ë©´ ì–´ëŠ í•˜ë‚˜ì˜ íŒ¨í„´ì´ë¼ë„ nameì— í¬í•¨ë˜ë©´ ì••ì¶•
            elif any(pattern in name for pattern in target_layers):
                layers_to_replace.append(name)
                # ì›ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                params = module.weight.numel()
                if hasattr(module, 'bias') and module.bias is not None:
                    params += module.bias.numel()
                total_params_before += params
    
    print(f"\nì••ì¶• ëŒ€ìƒ ë ˆì´ì–´: {len(layers_to_replace)}ê°œ")
    
    # ë ˆì´ì–´ êµì²´
    replaced_layers = []
    for name in tqdm(layers_to_replace, desc="ë ˆì´ì–´ ì••ì¶•"):
        module = model.get_submodule(name)
        
        # ìµœìƒìœ„ ëª¨ë“ˆì€ ê±´ë„ˆëœ€
        if '.' not in name:
            continue
            
        parent_name, child_name = name.rsplit('.', 1)
        parent_module = model.get_submodule(parent_name)
        
        # Conv1Dë¥¼ Linearë¡œ ë³€í™˜ (GPT-2 íŠ¹ìˆ˜ ì²˜ë¦¬)
        if isinstance(module, Conv1D):
            # transformersì˜ Conv1DëŠ” ì‹¤ì œë¡œëŠ” Linear
            # weight shape: [nx, nf] where nx=in_features, nf=out_features
            linear_equiv = nn.Linear(
                module.weight.shape[0],  # nx = in_features
                module.weight.shape[1],  # nf = out_features
                bias=(module.bias is not None)
            )
            # Conv1Dì˜ weightëŠ” ì´ë¯¸ [in_features, out_features] í˜•íƒœ
            linear_equiv.weight.data = module.weight.data.t()  # LinearëŠ” [out_features, in_features] í•„ìš”
            if module.bias is not None:
                linear_equiv.bias.data = module.bias.data
        else:
            linear_equiv = module
        
        # ë¹„íŠ¸í•„ë“œ ì••ì¶• ì ìš©
        compressed_layer = ProperBitfieldLinear(linear_equiv, basis_size)
        setattr(parent_module, child_name, compressed_layer)
        replaced_layers.append((name, compressed_layer))
        
        # ì••ì¶• í›„ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° (22ë¹„íŠ¸ Ã— out_features + ê¸°ì € í…Œì´ë¸” ê³µìœ )
        compressed_params = compressed_layer.out_features * 3  # 22ë¹„íŠ¸ â‰ˆ 3ë°”ì´íŠ¸
        if compressed_layer.bias is not None:
            compressed_params += compressed_layer.bias.numel() * 4  # ë°”ì´ì–´ìŠ¤ëŠ” float32
        total_params_after += compressed_params
    
    # ê¸°ì € í…Œì´ë¸” í¬ê¸° ì¶”ê°€ (ëª¨ë“  ë ˆì´ì–´ê°€ ê³µìœ )
    if replaced_layers:
        # ì²« ë²ˆì§¸ ì••ì¶•ëœ ë ˆì´ì–´ì˜ in_features ê°€ì ¸ì˜¤ê¸°
        _, first_compressed = replaced_layers[0]
        basis_table_size = basis_size * first_compressed.in_features * 4  # float32
        total_params_after += basis_table_size
    
    compression_ratio = total_params_after / total_params_before if total_params_before > 0 and total_params_after > 0 else 0
    print(f"\nì••ì¶• ì™„ë£Œ:")
    print(f"  - ì›ë³¸ íŒŒë¼ë¯¸í„°: {total_params_before:,} bytes")
    print(f"  - ì••ì¶• íŒŒë¼ë¯¸í„°: {total_params_after:,} bytes")
    if compression_ratio > 0:
        print(f"  - ì••ì¶•ë¥ : {compression_ratio:.3%} ({1/compression_ratio:.1f}x ì••ì¶•)")
    else:
        print(f"  - ì••ì¶•ë¥ : ê³„ì‚° ë¶ˆê°€")
    
    return model


def test_model(model, tokenizer, device, prompts, model_name, max_length=50):
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ì¸¡ì •"""
    model.to(device).eval()
    results = []
    total_time = 0.0
    
    print(f"\n=== {model_name} í…ŒìŠ¤íŠ¸ ===")
    for idx, prompt in enumerate(prompts, 1):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        start = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_length=max_length, 
                do_sample=False,  # ê²°ì •ì  ìƒì„±
                pad_token_id=tokenizer.eos_token_id
            )
        
        elapsed = time.time() - start
        gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        total_time += elapsed
        
        print(f"[{idx}] '{prompt}' -> {gen_text} ({elapsed:.3f}s)")
        results.append((prompt, gen_text, elapsed))
    
    avg_time = total_time / len(prompts) if prompts else 0
    print(f"{model_name} í‰ê·  ì‹œê°„: {avg_time:.3f}ì´ˆ")
    
    return results, avg_time


def calculate_perplexity(model, tokenizer, device, test_text):
    """í¼í”Œë ‰ì‹œí‹° ê³„ì‚°"""
    model.eval()
    inputs = tokenizer(test_text, return_tensors="pt", truncation=True, max_length=512).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        perplexity = torch.exp(loss)
    
    return perplexity.item()


def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ë¡œë“œ
    model_name = "skt/kogpt2-base-v2"
    print(f"\nëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    teacher = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, use_safetensors=True).to(device)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”", 
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
        "ì¸ê³µì§€ëŠ¥ì´ë€",
        "ë§›ìˆëŠ” ìŒì‹ì€"
    ]
    
    # í¼í”Œë ‰ì‹œí‹° í…ŒìŠ¤íŠ¸ìš© í…ìŠ¤íŠ¸
    test_text = """ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•˜ë ¤ëŠ” 
    ì»´í“¨í„° ê³¼í•™ì˜ ì„¸ë¶€ ë¶„ì•¼ì´ë‹¤. ìµœê·¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë§ì€ ì§„ì „ì´ ìˆì—ˆë‹¤."""
    
    print("\n" + "="*60)
    print("ë¹„íŠ¸í•„ë“œ ì••ì¶• ì„±ëŠ¥ ë¹„êµ")
    print("="*60)
    
    # ì›ë³¸ ëª¨ë¸ í¬ê¸°
    orig_params = sum(p.numel() for p in teacher.parameters())
    print(f"ì›ë³¸ ëª¨ë¸ í¬ê¸°: {orig_params:,} íŒŒë¼ë¯¸í„°")
    
    # 1. ë¹„íŠ¸í•„ë“œ ì••ì¶• (ì „ì²´ ë ˆì´ì–´)
    print("\n[1] ì „ì²´ ë ˆì´ì–´ ë¹„íŠ¸í•„ë“œ ì••ì¶•")
    student_full = copy.deepcopy(teacher)
    student_full = apply_bitfield_compression_to_model(student_full, basis_size=256)
    
    comp_full_results, comp_full_time = test_model(student_full, tokenizer, device, prompts, "ì „ì²´ì••ì¶•")
    comp_full_perplexity = calculate_perplexity(student_full, tokenizer, device, test_text)
    print(f"ì „ì²´ì••ì¶• í¼í”Œë ‰ì‹œí‹°: {comp_full_perplexity:.2f}")
    
    # 2. ë¹„íŠ¸í•„ë“œ ì••ì¶• (ì£¼ìš” ë ˆì´ì–´ë§Œ)
    print("\n[2] ì£¼ìš” ë ˆì´ì–´ë§Œ ë¹„íŠ¸í•„ë“œ ì••ì¶• (MLP ë ˆì´ì–´ë§Œ)")
    student_partial = copy.deepcopy(teacher)
    # MLP ë ˆì´ì–´ë§Œ ì••ì¶• (ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ í° ë ˆì´ì–´)
    # GPT-2ëŠ” transformer.h.*.mlp.c_fcì™€ transformer.h.*.mlp.c_proj íŒ¨í„´
    target_layers = ['mlp.c_']  # MLPì˜ c_fcì™€ c_projë§Œ
    student_partial = apply_bitfield_compression_to_model(
        student_partial, 
        basis_size=256,
        target_layers=target_layers
    )
    
    comp_partial_results, comp_partial_time = test_model(
        student_partial, tokenizer, device, prompts, "ë¶€ë¶„ì••ì¶•"
    )
    comp_partial_perplexity = calculate_perplexity(student_partial, tokenizer, device, test_text)
    print(f"ë¶€ë¶„ì••ì¶• í¼í”Œë ‰ì‹œí‹°: {comp_partial_perplexity:.2f}")
    
    # 3. ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë¹„êµìš©)
    print("\n[3] ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ë¹„êµìš©)")
    orig_results, orig_time = test_model(teacher, tokenizer, device, prompts, "ì›ë³¸")
    orig_perplexity = calculate_perplexity(teacher, tokenizer, device, test_text)
    print(f"ì›ë³¸ í¼í”Œë ‰ì‹œí‹°: {orig_perplexity:.2f}")
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    print(f"\nì¶”ë¡  ì†ë„:")
    print(f"  - ì›ë³¸: {orig_time:.3f}ì´ˆ")
    print(f"  - ì „ì²´ì••ì¶•: {comp_full_time:.3f}ì´ˆ ({comp_full_time/orig_time:.2f}x)")
    print(f"  - ë¶€ë¶„ì••ì¶•: {comp_partial_time:.3f}ì´ˆ ({comp_partial_time/orig_time:.2f}x)")
    
    print(f"\ní¼í”Œë ‰ì‹œí‹° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ):")
    print(f"  - ì›ë³¸: {orig_perplexity:.2f}")
    print(f"  - ì „ì²´ì••ì¶•: {comp_full_perplexity:.2f} (ì°¨ì´: +{comp_full_perplexity-orig_perplexity:.2f})")
    print(f"  - ë¶€ë¶„ì••ì¶•: {comp_partial_perplexity:.2f} (ì°¨ì´: +{comp_partial_perplexity-orig_perplexity:.2f})")
    
    print(f"\nì¶œë ¥ ì¼ì¹˜ìœ¨:")
    for name, results in [("ì „ì²´ì••ì¶•", comp_full_results), ("ë¶€ë¶„ì••ì¶•", comp_partial_results)]:
        matches = sum(1 for o, c in zip(orig_results, results) if o[1] == c[1])
        print(f"  - {name}: {matches}/{len(prompts)} ({matches/len(prompts)*100:.0f}%)")
    
    # ìƒì„¸ ì¶œë ¥ ë¹„êµ
    print(f"\nìƒì„¸ ì¶œë ¥ ë¹„êµ:")
    for i, prompt in enumerate(prompts):
        print(f"\ní”„ë¡¬í”„íŠ¸: '{prompt}'")
        print(f"  ì›ë³¸: {orig_results[i][1]}")
        print(f"  ì „ì²´: {comp_full_results[i][1]}")
        print(f"  ë¶€ë¶„: {comp_partial_results[i][1]}")


if __name__ == "__main__":
    main() 