"""GPT-2 ëª¨ë¸ ìŠ¤í”Œë¼ì¸ ì••ì¶• í…ŒìŠ¤íŠ¸"""

import torch
import torch.nn as nn
from reality_stone.layers import SplineLinear
import time
from tqdm import tqdm

def find_all_linear_layers(model):
    """ëª¨ë¸ì—ì„œ ëª¨ë“  Linear ë ˆì´ì–´ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    layers = []
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            layers.append((name, module))
        elif hasattr(module, 'weight') and len(module.weight.shape) == 2:
            # Conv1D ë ˆì´ì–´ (transformers.pytorch_utils.Conv1D)
            if 'Conv1D' in str(type(module)):
                layers.append((name, module))
    
    return layers


def compress_model_with_spline(model, k=8, ignore_layers=None, tokenizer=None):
    """
    ëª¨ë¸ì˜ Linear ë ˆì´ì–´ë¥¼ SplineLinearë¡œ ì••ì¶•í•©ë‹ˆë‹¤.
    
    Args:
        model: ì••ì¶•í•  ëª¨ë¸
        k: ìŠ¤í”Œë¼ì¸ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ (ì œì–´ì  = k+1)
        ignore_layers: ì••ì¶•í•˜ì§€ ì•Šì„ ë ˆì´ì–´ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    if ignore_layers is None:
        ignore_layers = []
        
    compressed_layers = []
    all_layers = find_all_linear_layers(model)
    
    print(f"\nğŸ”· ìŠ¤í”Œë¼ì¸ ì••ì¶• ì‹œì‘ (k={k}, ì œì–´ì ={k+1})")
    print(f"ì••ì¶• ëŒ€ìƒ ë ˆì´ì–´: {len(all_layers)}ê°œ\n")
    
    # ë¨¼ì € ë ˆì´ì–´ ì •ë³´ë¥¼ ì¶œë ¥í•´ë³´ì
    print("ë ˆì´ì–´ ì •ë³´:")
    for name, layer in all_layers[:5]:  # ì²˜ìŒ 5ê°œë§Œ
        if 'Conv1D' in str(type(layer)):
            print(f"  - {name}: Conv1D, weight shape = {layer.weight.shape}")
        else:
            print(f"  - {name}: Linear, in={layer.in_features}, out={layer.out_features}")
    print()
    
    # ì§„í–‰ ìƒí™©ì„ ìœ„í•œ tqdm ì„¤ì •
    pbar = tqdm(all_layers, desc="ë ˆì´ì–´ ì••ì¶•", ncols=120)
    
    for idx, (name, layer) in enumerate(pbar):
        if any(ignore_name in name for ignore_name in ignore_layers):
            continue
            
        try:
            # Conv1D ì²˜ë¦¬
            if 'Conv1D' in str(type(layer)):
                # GPT-2ì˜ Conv1D ê°€ì¤‘ì¹˜ shapeëŠ” (in_features, out_features)
                weight = layer.weight
                in_features = weight.shape[0]
                out_features = weight.shape[1]
                
                # from_linearì— ì „ë‹¬í•  í‘œì¤€ Linear ë ˆì´ì–´ ìƒì„±
                linear_layer = nn.Linear(in_features, out_features, bias=(layer.bias is not None))
                
                # nn.Linearì˜ ê°€ì¤‘ì¹˜ shapeëŠ” (out_features, in_features)ì´ë¯€ë¡œ ì „ì¹˜(transpose) í•„ìš”
                linear_layer.weight.data = weight.t().clone()
                if layer.bias is not None:
                    linear_layer.bias.data = layer.bias.clone()
            else:
                linear_layer = layer
                in_features = layer.in_features
                out_features = layer.out_features
            
            # ì‘ì€ ë ˆì´ì–´ëŠ” ì••ì¶• íš¨ê³¼ê°€ ì ìœ¼ë¯€ë¡œ ê±´ë„ˆëœ€
            if in_features < 64 or out_features < 64:
                pbar.set_postfix_str(f"'{name}' ê±´ë„ˆëœ€ (í¬ê¸° ì‘ìŒ)")
                continue
            
            # SplineLinearë¡œ ë³€í™˜
            pbar.set_postfix_str(f"'{name}' ì••ì¶• ì¤‘... ({in_features}x{out_features})")
            
            # ì••ì¶• ì‹œì‘ ì‹œê°„
            start_time = time.time()
            
            spline_layer = SplineLinear.from_linear(
                linear_layer, 
                k=k,
                learning_rate=0.1,  # í•™ìŠµë¥  ì¦ê°€
                steps=10,  # ìŠ¤í… ìˆ˜ ëŒ€í­ ê°ì†Œ
                use_residual=False  # residual ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì••ì¶•ë¥  í–¥ìƒ)
            )
            
            # ì••ì¶• ì‹œê°„ ì¸¡ì •
            compress_time = time.time() - start_time
            
            # ëª¨ë¸ì—ì„œ ë ˆì´ì–´ êµì²´
            parent_name = '.'.join(name.split('.')[:-1])
            child_name = name.split('.')[-1]
            parent_module = model.get_submodule(parent_name) if parent_name else model
            setattr(parent_module, child_name, spline_layer)
            
            compressed_layers.append((name, spline_layer))
            
            # ì••ì¶•ë¥  ê³„ì‚°
            compression_ratio = spline_layer.get_compression_ratio()
            pbar.set_postfix_str(f"âœ… '{name}' (ì••ì¶•ë¥ : {compression_ratio:.1f}x, {compress_time:.1f}ì´ˆ)")
            
            # 5ê°œë§ˆë‹¤ ì¤‘ê°„ ê²°ê³¼ í™•ì¸
            if len(compressed_layers) % 5 == 0 and len(compressed_layers) > 0:
                print(f"\n\nğŸ“Š ì¤‘ê°„ ì ê²€ ({len(compressed_layers)}ê°œ ë ˆì´ì–´ ì••ì¶•ë¨)")
                print(f"   - ì§„í–‰ë¥ : {idx + 1}/{len(all_layers)} ({(idx + 1) / len(all_layers) * 100:.1f}%)")
                
                # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
                if tokenizer is not None:
                    try:
                        # ëª¨ë¸ íƒ€ì… í™•ì¸
                        model_type = getattr(model.config, 'model_type', 'gpt2')
                        is_korean = 'kogpt' in model.config._name_or_path.lower() if hasattr(model.config, '_name_or_path') else False
                        
                        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ì€" if is_korean else "Hello, today is"
                        inputs = tokenizer(test_prompt, return_tensors="pt")
                        with torch.no_grad():
                            outputs = model.generate(
                                inputs.input_ids,
                                max_length=30,
                                do_sample=True,
                                temperature=0.8,
                                pad_token_id=tokenizer.eos_token_id
                            )
                        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
                        print(f"   - ìƒ˜í”Œ ìƒì„±: {generated}")
                        print(f"   - ëª¨ë¸ íƒ€ì…: {model_type}, í•œêµ­ì–´: {is_korean}")
                    except Exception as e:
                        print(f"   - ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                print()
            
        except Exception as e:
            pbar.set_postfix_str(f"âŒ '{name}' ì‹¤íŒ¨")
            print(f"\n  âŒ '{name}' ì••ì¶• ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    return compressed_layers


def calculate_compression_stats(compressed_layers):
    """ì••ì¶•ë¥  í†µê³„ ê³„ì‚°"""
    total_original_params = 0
    total_compressed_params = 0
    
    for name, spline_layer in compressed_layers:
        # ì›ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜
        original_params = spline_layer.in_features * spline_layer.out_features
        # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ìˆ˜ (ì œì–´ì )
        compressed_params = (spline_layer.k + 1) * spline_layer.in_features
        
        total_original_params += original_params
        total_compressed_params += compressed_params
        
        compression_ratio = spline_layer.get_compression_ratio()
        print(f"  - {name}: {compression_ratio:.1f}x ì••ì¶•")
    
    overall_compression = total_original_params / total_compressed_params if total_compressed_params > 0 else 0
    
    return {
        'original_params': total_original_params,
        'compressed_params': total_compressed_params,
        'compression_ratio': overall_compression,
        'layers_compressed': len(compressed_layers)
    }


def test_generation_quality(model, tokenizer, test_prompts):
    """ì••ì¶•ëœ ëª¨ë¸ì˜ ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
    
    model.eval()
    with torch.no_grad():
        for prompt in test_prompts:
            print(f"\ní”„ë¡¬í”„íŠ¸: '{prompt}'")
            
            inputs = tokenizer(prompt, return_tensors='pt')
            
            try:
                # ìƒì„±
                outputs = model.generate(
                    inputs['input_ids'],
                    max_new_tokens=30,
                    temperature=0.8,
                    do_sample=True,
                    top_k=50,
                    pad_token_id=tokenizer.pad_token_id
                )
                
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: '{generated_text}'")
            except Exception as e:
                print(f"ìƒì„± ì‹¤íŒ¨: {e}")


def test_inference_speed(model, tokenizer, num_iterations=10):
    """ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸"""
    print(f"\nâš¡ ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸ ({num_iterations}íšŒ ë°˜ë³µ)")
    
    test_text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. " * 5
    inputs = tokenizer(test_text, return_tensors='pt', max_length=128, truncation=True)
    
    model.eval()
    with torch.no_grad():
        try:
            # ì›Œë°ì—…
            for _ in range(3):
                _ = model(**inputs)
            
            # ì†ë„ ì¸¡ì •
            start_time = time.time()
            for _ in range(num_iterations):
                _ = model(**inputs)
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_iterations * 1000  # ms
            print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.2f} ms")
            return avg_time
        except Exception as e:
            print(f"ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return float('inf')


def main():
    print("="*60)
    print("ğŸ—œï¸ ëª¨ë¸ ì••ì¶• ì‹œì‘")
    print("="*60)
    
    # KoGPT2 ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“¥ KoGPT2 ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model_name = 'skt/kogpt2-base-v2'
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # safetensors í˜•ì‹ìœ¼ë¡œ ë¡œë“œ ì‹œë„, ì—†ìœ¼ë©´ ì¼ë°˜ GPT2 ì‚¬ìš©
    try:
        model = AutoModelForCausalLM.from_pretrained(model_name, use_safetensors=True)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    except Exception as e:
        print(f"âš ï¸ KoGPT2 ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ“¥ ëŒ€ì‹  GPT2 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤...")
        model_name = 'gpt2'
        from transformers import GPT2LMHeadModel, GPT2Tokenizer
        model = GPT2LMHeadModel.from_pretrained(model_name)
        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    # ì›ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜
    original_params = sum(p.numel() for p in model.parameters())
    print(f"ì›ë³¸ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {original_params:,}")
    
    # ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\n--- ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")
    
    # ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    if 'kogpt2' in model_name.lower():
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ì€",
            "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”",
            "í•œêµ­ì˜ ì „í†µ ë¬¸í™”ëŠ”",
        ]
    else:
        test_prompts = [
            "Hello, today is",
            "The future of AI is",
            "Once upon a time",
        ]
    
    print("\nì›ë³¸ ëª¨ë¸ ìƒì„± ì˜ˆì‹œ:")
    test_generation_quality(model, tokenizer, test_prompts[:1])
    
    original_speed = test_inference_speed(model, tokenizer)
    
    # ëª¨ë¸ ì••ì¶•
    print("\n" + "="*60)
    print("ğŸ—œï¸ ëª¨ë¸ ì••ì¶• ì‹œì‘")
    print("="*60)
    
    # ì„ë² ë”© ë ˆì´ì–´ëŠ” ì••ì¶•í•˜ì§€ ì•ŠìŒ
    ignore_layers = ['wte', 'wpe', 'ln_f']
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ k=8ë§Œ ì‚¬ìš©
    k = 8
    
    print(f"\n\n### k={k} í…ŒìŠ¤íŠ¸ ###")
    
    # ëª¨ë¸ ë³µì‚¬ (ì›ë³¸ ìœ ì§€)
    import copy
    compressed_model = copy.deepcopy(model)
    
    # ì••ì¶•
    compressed_layers = compress_model_with_spline(
        compressed_model, 
        k=k,
        ignore_layers=ignore_layers,
        tokenizer=tokenizer
    )
    
    if compressed_layers:
        # ì••ì¶• í†µê³„
        print(f"\nğŸ“Š ì••ì¶• í†µê³„ (k={k}):")
        stats = calculate_compression_stats(compressed_layers)
        print(f"  - ì••ì¶•ëœ ë ˆì´ì–´ ìˆ˜: {stats['layers_compressed']}")
        print(f"  - ì›ë³¸ íŒŒë¼ë¯¸í„°: {stats['original_params']:,}")
        print(f"  - ì••ì¶• íŒŒë¼ë¯¸í„°: {stats['compressed_params']:,}")
        print(f"  - ì „ì²´ ì••ì¶•ë¥ : {stats['compression_ratio']:.1f}x")
        
        # ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print(f"\nì••ì¶• ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸ (k={k}):")
        try:
            test_generation_quality(compressed_model, tokenizer, test_prompts)
        except Exception as e:
            print(f"ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        
        print("\nâš¡ ì†ë„ í…ŒìŠ¤íŠ¸...")
        try:
            compressed_speed = test_inference_speed(compressed_model, tokenizer)
            if compressed_speed != float('inf'):
                speedup = original_speed / compressed_speed
                print(f"ì†ë„ í–¥ìƒ: {speedup:.2f}x")
        except Exception as e:
            print(f"ì†ë„ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        compressed_params = sum(p.numel() for p in compressed_model.parameters())
        memory_reduction = (1 - compressed_params / original_params) * 100
        print(f"ë©”ëª¨ë¦¬ ì ˆê°: {memory_reduction:.1f}%")
    else:
        print("\nâš ï¸ ì••ì¶•ëœ ë ˆì´ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main() 