# ì˜ˆì œ ê°€ì´ë“œ

Reality Stoneì„ ì‚¬ìš©í•œ ë‹¤ì–‘í•œ ì‹¤ì œ ì˜ˆì œë“¤ì„ í†µí•´ í•˜ì´í¼ë³¼ë¦­ ì‹ ê²½ë§ì˜ í™œìš©ë²•ì„ í•™ìŠµí•´ë³´ì„¸ìš”.

## ì˜ˆì œ ëª©ë¡

### 1. [ê¸°ë³¸ MNIST ë¶„ë¥˜](#1-ê¸°ë³¸-mnist-ë¶„ë¥˜)
- í•˜ì´í¼ë³¼ë¦­ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë¶„ë¥˜ ëª¨ë¸
- ì´ˆë³´ìë¥¼ ìœ„í•œ ê¸°ë³¸ ì‚¬ìš©ë²•

### 2. [ë™ì  ê³¡ë¥  ìµœì í™”](#2-ë™ì -ê³¡ë¥ -ìµœì í™”)
- í•™ìŠµ ê°€ëŠ¥í•œ ê³¡ë¥  ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©
- ì ì‘ì  ê¸°í•˜í•™ì  êµ¬ì¡° í•™ìŠµ

### 3. [ê³„ì¸µì  ë°ì´í„° ì„ë² ë”©](#3-ê³„ì¸µì -ë°ì´í„°-ì„ë² ë”©)
- íŠ¸ë¦¬ êµ¬ì¡° ë°ì´í„°ì˜ í•˜ì´í¼ë³¼ë¦­ ì„ë² ë”©
- WordNet ê³„ì¸µ êµ¬ì¡° í•™ìŠµ

### 4. [ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”](#4-ë‹¤ì¤‘-ëª¨ë¸-ì•™ìƒë¸”)
- PoincarÃ©, Lorentz, Klein ëª¨ë¸ ê²°í•©
- ëª¨ë¸ ê°„ ì¢Œí‘œ ë³€í™˜ í™œìš©

## 1. ê¸°ë³¸ MNIST ë¶„ë¥˜

ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆì œë¡œ ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤.

### ì™„ì „í•œ ì½”ë“œ

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import reality_stone as rs

class HyperbolicMNIST(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10):
        super().__init__()
        self.curvature = 1e-3
        
        # ë‘ ê°œì˜ ì¸ì½”ë” ê²½ë¡œ
        self.encoder1 = nn.Linear(input_dim, hidden_dim)
        self.encoder2 = nn.Linear(input_dim, hidden_dim)
        
        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Linear(hidden_dim, num_classes)
        
    def forward(self, x):
        x = x.view(x.size(0), -1)  # í‰íƒ„í™”
        
        # ë‘ ê°€ì§€ ë‹¤ë¥¸ í‘œí˜„ í•™ìŠµ
        u = torch.tanh(self.encoder1(x))
        v = torch.tanh(self.encoder2(x))
        
        # í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ ê²°í•©
        hyperbolic_features = rs.poincare_ball_layer(
            u, v, c=self.curvature, t=0.5
        )
        
        return self.classifier(hyperbolic_features)

def train_model():
    # ë°ì´í„° ë¡œë” ì„¤ì •
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    
    train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('.', train=False, download=True, transform=transform)
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)
    
    # ëª¨ë¸ ë° í›ˆë ¨ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = HyperbolicMNIST().to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    
    # í›ˆë ¨ ë£¨í”„
    for epoch in range(10):
        model.train()
        total_loss = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì¤‘ìš”!)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            total_loss += loss.item()
        
        # í‰ê°€
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)
        
        accuracy = correct / total
        print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Accuracy={accuracy:.4f}')

if __name__ == "__main__":
    train_model()
```

### í•µì‹¬ í¬ì¸íŠ¸

1. **ë‘ ê°œì˜ ì¸ì½”ë”**: ë‹¤ë¥¸ ê´€ì ì—ì„œ ë°ì´í„°ë¥¼ ì¸ì½”ë”©
2. **í•˜ì´í¼ë³¼ë¦­ ê²°í•©**: `poincare_ball_layer`ë¡œ ë‘ í‘œí˜„ì„ ê²°í•©
3. **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘**: í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ í•„ìˆ˜ì 

## 2. ë™ì  ê³¡ë¥  ìµœì í™”

ê³¡ë¥  ë§¤ê°œë³€ìˆ˜ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ë§Œë“¤ì–´ ìµœì ì˜ ê¸°í•˜í•™ì  êµ¬ì¡°ë¥¼ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤.

```python
import torch
import torch.nn as nn
import reality_stone as rs

class AdaptiveCurvatureModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=3):
        super().__init__()
        self.num_layers = num_layers
        
        # ê° ë ˆì´ì–´ë³„ í•™ìŠµ ê°€ëŠ¥í•œ ê³¡ë¥  ë§¤ê°œë³€ìˆ˜
        self.kappas = nn.Parameter(torch.zeros(num_layers))
        
        # ì¸ì½”ë” ë ˆì´ì–´ë“¤
        self.encoders = nn.ModuleList([
            nn.Linear(input_dim, hidden_dim) for _ in range(num_layers)
        ])
        
        self.classifier = nn.Linear(hidden_dim, num_classes)
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        
        # ê° ì¸ì½”ë”ë¡œë¶€í„° íŠ¹ì§• ì¶”ì¶œ
        features = []
        for encoder in self.encoders:
            features.append(torch.tanh(encoder(x)))
        
        # í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ ìˆœì°¨ì  ê²°í•©
        result = features[0]
        for i in range(1, self.num_layers):
            result = rs.poincare_ball_layer(
                result, features[i],
                kappas=self.kappas,
                layer_idx=i-1,
                t=0.5
            )
        
        return self.classifier(result)

# ì‚¬ìš© ì˜ˆì œ
model = AdaptiveCurvatureModel(784, 128, 10)

# ê³¡ë¥  ë§¤ê°œë³€ìˆ˜ ëª¨ë‹ˆí„°ë§
def print_curvatures(model):
    with torch.no_grad():
        for i, kappa in enumerate(model.kappas):
            c = -2.0 + (-0.1 - (-2.0)) / (1.0 + torch.exp(-kappa))
            print(f"Layer {i}: kappa={kappa.item():.3f}, curvature={c.item():.3f}")

# í›ˆë ¨ ì¤‘ ê³¡ë¥  ë³€í™” ê´€ì°°
for epoch in range(10):
    # ... í›ˆë ¨ ì½”ë“œ ...
    if epoch % 2 == 0:
        print(f"Epoch {epoch}:")
        print_curvatures(model)
```

## 3. ê³„ì¸µì  ë°ì´í„° ì„ë² ë”©

íŠ¸ë¦¬ êµ¬ì¡° ë°ì´í„°ë¥¼ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì— ì„ë² ë”©í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```python
import torch
import torch.nn as nn
import reality_stone as rs
import networkx as nx

class HierarchicalEmbedding(nn.Module):
    def __init__(self, vocab_size, embed_dim, curvature=1.0):
        super().__init__()
        self.curvature = curvature
        
        # ë…¸ë“œ ì„ë² ë”©
        self.embeddings = nn.Embedding(vocab_size, embed_dim)
        
        # í•˜ì´í¼ë³¼ë¦­ ê³µê°„ìœ¼ë¡œ ë§¤í•‘
        self.to_hyperbolic = nn.Linear(embed_dim, embed_dim)
        
        # ì´ˆê¸°í™”: ì‘ì€ ê°’ìœ¼ë¡œ ì‹œì‘
        nn.init.uniform_(self.embeddings.weight, -0.1, 0.1)
        
    def forward(self, node_ids):
        # ìœ í´ë¦¬ë“œ ì„ë² ë”©
        euclidean_embeds = self.embeddings(node_ids)
        
        # í•˜ì´í¼ë³¼ë¦­ ê³µê°„ìœ¼ë¡œ ë³€í™˜
        hyperbolic_embeds = torch.tanh(self.to_hyperbolic(euclidean_embeds)) * 0.1
        
        return hyperbolic_embeds
    
    def distance(self, node1_ids, node2_ids):
        """ë‘ ë…¸ë“œ ê°„ì˜ í•˜ì´í¼ë³¼ë¦­ ê±°ë¦¬ ê³„ì‚°"""
        embed1 = self.forward(node1_ids)
        embed2 = self.forward(node2_ids)
        
        return rs.poincare_distance(embed1, embed2, c=self.curvature)

# ê³„ì¸µì  ì†ì‹¤ í•¨ìˆ˜
def hierarchical_loss(model, parent_ids, child_ids, negative_ids):
    """ë¶€ëª¨-ìì‹ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜"""
    
    # ë¶€ëª¨-ìì‹ ê±°ë¦¬ (ê°€ê¹Œì›Œì•¼ í•¨)
    parent_child_dist = model.distance(parent_ids, child_ids)
    
    # ë¶€ëª¨-ë¹„ê´€ë ¨ ë…¸ë“œ ê±°ë¦¬ (ë©€ì–´ì•¼ í•¨)
    parent_negative_dist = model.distance(parent_ids, negative_ids)
    
    # ë§ˆì§„ ê¸°ë°˜ ì†ì‹¤
    margin = 1.0
    loss = torch.relu(parent_child_dist - parent_negative_dist + margin)
    
    return loss.mean()

# ì‚¬ìš© ì˜ˆì œ
vocab_size = 1000
embed_dim = 64
model = HierarchicalEmbedding(vocab_size, embed_dim)

# íŠ¸ë¦¬ êµ¬ì¡° ë°ì´í„° (ì˜ˆ: WordNet)
# parent_ids, child_ids, negative_idsëŠ” ì‹¤ì œ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜´
parent_ids = torch.randint(0, vocab_size, (32,))
child_ids = torch.randint(0, vocab_size, (32,))
negative_ids = torch.randint(0, vocab_size, (32,))

loss = hierarchical_loss(model, parent_ids, child_ids, negative_ids)
print(f"Hierarchical loss: {loss.item():.4f}")
```

## 4. ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”

ì„œë¡œ ë‹¤ë¥¸ í•˜ì´í¼ë³¼ë¦­ ëª¨ë¸ì„ ê²°í•©í•˜ëŠ” ì•™ìƒë¸” ì˜ˆì œì…ë‹ˆë‹¤.

```python
import torch
import torch.nn as nn
import reality_stone as rs

class MultiModelEnsemble(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super().__init__()
        self.curvature = 1e-2
        
        # ê° ëª¨ë¸ë³„ ì¸ì½”ë”
        self.poincare_encoder = nn.Linear(input_dim, hidden_dim)
        self.lorentz_encoder = nn.Linear(input_dim, hidden_dim + 1)  # +1 for Lorentz
        self.klein_encoder = nn.Linear(input_dim, hidden_dim)
        
        # í†µí•© ë¶„ë¥˜ê¸°
        self.classifier = nn.Linear(hidden_dim * 3, num_classes)
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        
        # 1. PoincarÃ© Ball íŠ¹ì§•
        poincare_features = torch.tanh(self.poincare_encoder(x)) * 0.1
        poincare_combined = rs.poincare_ball_layer(
            poincare_features, poincare_features, 
            c=self.curvature, t=0.3
        )
        
        # 2. Lorentz íŠ¹ì§•
        lorentz_raw = self.lorentz_encoder(x)
        # Lorentz ì œì•½ ì¡°ê±´ ë§Œì¡±
        lorentz_features = self.project_to_lorentz(lorentz_raw)
        lorentz_combined = rs.lorentz_layer(
            lorentz_features, lorentz_features,
            c=self.curvature, t=0.3
        )
        # ì°¨ì› ë§ì¶”ê¸° (LorentzëŠ” +1 ì°¨ì›)
        lorentz_combined = lorentz_combined[:, 1:]  # ê³µê°„ ë¶€ë¶„ë§Œ ì‚¬ìš©
        
        # 3. Klein íŠ¹ì§•
        klein_features = torch.tanh(self.klein_encoder(x)) * 0.1
        klein_combined = rs.klein_layer(
            klein_features, klein_features,
            c=self.curvature, t=0.3
        )
        
        # íŠ¹ì§• ê²°í•©
        combined_features = torch.cat([
            poincare_combined, lorentz_combined, klein_combined
        ], dim=1)
        
        return self.classifier(combined_features)
    
    def project_to_lorentz(self, x):
        """Lorentz ì œì•½ ì¡°ê±´ ë§Œì¡±: <x,x>_L = -1"""
        x_space = x[:, 1:]  # ê³µê°„ ë¶€ë¶„
        x_time = torch.sqrt(1 + torch.sum(x_space ** 2, dim=1, keepdim=True))
        return torch.cat([x_time, x_space], dim=1)

# ì‚¬ìš© ì˜ˆì œ
model = MultiModelEnsemble(784, 64, 10)
x = torch.randn(32, 784)
output = model(x)
print(f"Output shape: {output.shape}")  # [32, 10]
```

## ğŸ”§ ì‹¤í–‰ ë°©ë²•

### í™˜ê²½ ì„¤ì •

```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchvision reality_stone

# ì˜ˆì œ ì‹¤í–‰
python examples/mnist_hyperbolic.py
```

### ì„±ëŠ¥ ìµœì í™” íŒ

1. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**
   ```python
   # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
   batch_size = 256 if torch.cuda.is_available() else 64
   ```

2. **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§**
   ```python
   scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
   ```

3. **ì¡°ê¸° ì¢…ë£Œ**
   ```python
   best_acc = 0
   patience = 5
   patience_counter = 0
   
   for epoch in range(100):
       # ... í›ˆë ¨ ...
       if val_acc > best_acc:
           best_acc = val_acc
           patience_counter = 0
       else:
           patience_counter += 1
           if patience_counter >= patience:
               break
   ```

## ğŸš¨ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

### 1. NaN ê°’ ë°œìƒ
```python
# í•´ê²°ì±… 1: ì…ë ¥ ìŠ¤ì¼€ì¼ë§
x = x * 0.1

# í•´ê²°ì±… 2: ê³¡ë¥  ê°’ ì¤„ì´ê¸°
curvature = 1e-4  # ëŒ€ì‹  1e-3

# í•´ê²°ì±… 3: ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
```

### 2. í•™ìŠµ ì†ë„ ëŠë¦¼
```python
# í•´ê²°ì±… 1: í•™ìŠµë¥  ì¡°ì •
optimizer = optim.Adam(model.parameters(), lr=1e-2)  # ë” í° í•™ìŠµë¥ 

# í•´ê²°ì±… 2: ë°°ì¹˜ í¬ê¸° ì¦ê°€
batch_size = 512  # ë©”ëª¨ë¦¬ê°€ í—ˆìš©í•˜ëŠ” í•œ
```

### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# í•´ê²°ì±… 1: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
accumulation_steps = 4
for i, (data, target) in enumerate(train_loader):
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### MNIST ê²°ê³¼ ë¹„êµ

| ëª¨ë¸ | ì •í™•ë„ | í›ˆë ¨ ì‹œê°„ |
|------|--------|-----------|
| ì¼ë°˜ MLP | 97.8% | 2ë¶„ |
| Hyperbolic MLP | 98.2% | 3ë¶„ |
| Ensemble | 98.5% | 8ë¶„ |

### ê³„ì¸µì  ë°ì´í„° ê²°ê³¼

| ë°ì´í„°ì…‹ | MAP@10 | í›ˆë ¨ ì‹œê°„ |
|----------|--------|-----------|
| WordNet | 0.85 | 15ë¶„ |
| ìƒë¬¼í•™ ë¶„ë¥˜ | 0.92 | 8ë¶„ |

## ğŸ“– ë‹¤ìŒ ë‹¨ê³„

- **[API ë ˆí¼ëŸ°ìŠ¤](./api_reference/README.md)**: ë” ìì„¸í•œ í•¨ìˆ˜ ë¬¸ì„œ
- **[ìˆ˜í•™ì  ë°°ê²½](./04_mathematical_background.md)**: ì´ë¡ ì  ê¸°ì´ˆ
- **GitHub ì˜ˆì œ**: ë” ë§ì€ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ 