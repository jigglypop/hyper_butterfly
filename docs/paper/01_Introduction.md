## 1. 서론: 표현의 기하학과 압축의 새로운 지평

### 1.1. 거대 모델 시대의 두 가지 딜레마: 크기와 기하학

#### 1.1.1. 크기의 딜레마: 지수적 성장의 한계

지난 10년간 딥러닝 분야는 파라미터 규모를 확장하며 비약적인 발전을 이루었다. GPT-3의 175B 파라미터에서 시작하여, PaLM의 540B, 그리고 최근 GPT-4와 같은 모델들은 1조 개 이상의 파라미터를 가질 것으로 추정된다. 이러한 지수적 성장은 다음과 같은 구체적인 문제들을 야기한다:

1. **메모리 요구량**: GPT-3 175B 모델은 FP16 정밀도로 저장 시 약 350GB의 메모리를 필요로 한다. 이는 대부분의 GPU(예: NVIDIA A100 80GB)로도 단일 디바이스에 로드할 수 없는 규모다. 더욱이, 추론 시에는 활성화(activation) 메모리까지 고려해야 하므로 실제 요구량은 이보다 훨씬 크다.

2. **연산량과 에너지**: 단일 추론을 위해 수조 번의 부동소수점 연산(FLOPs)이 필요하며, 이는 데이터센터 수준의 전력 소비로 이어진다. 구체적으로, GPT-3의 단일 추론은 약 0.4kWh의 전력을 소비하는 것으로 추정되며, 이는 평균 가정이 1시간 동안 사용하는 전력과 맞먹는다.

3. **배포의 현실적 제약**: 엣지 디바이스나 모바일 환경에서의 배포는 사실상 불가능하며, 클라우드 기반 서비스도 막대한 인프라 비용으로 인해 접근성이 제한된다.

최근 주목받는 Mixture-of-Experts (MoE) 아키텍처는 이 문제에 대한 부분적 해결책을 제시했다. MoE는 입력에 따라 전문가(expert) 서브네트워크의 일부만을 활성화하여 연산량을 줄인다. 예를 들어, Switch Transformer는 1.6조 개의 파라미터를 가지지만, 각 토큰당 실제로는 약 12B 파라미터만 활성화된다. 

그러나 MoE는 근본적인 한계를 가진다:
- **메모리 병목**: 비활성 전문가들도 여전히 메모리에 상주해야 하므로, 전체 모델 크기는 줄어들지 않는다.
- **불균형한 로드**: 특정 전문가에 토큰이 집중되는 현상으로 인해 실제 연산 효율성이 이론적 수치에 미치지 못한다.
- **통신 오버헤드**: 분산 환경에서 전문가 간 라우팅은 상당한 네트워크 지연을 발생시킨다.

#### 1.1.2. 기하학의 딜레마: 평평한 세계의 한계

두 번째이자 더 근본적인 문제는 **기하학의 딜레마**다. 현재 대부분의 딥러닝 모델들은 데이터를 암묵적으로 유클리드 공간(Euclidean space)에 임베딩한다고 가정한다. 이는 다음과 같은 구체적인 문제들을 야기한다:

1. **계층 구조의 왜곡**: 언어의 구문 트리(parse tree)를 생각해보자. "The cat sat on the mat"라는 문장에서 단어들은 명확한 계층 구조를 형성한다. 그러나 이를 유클리드 공간의 벡터로 표현하면, 계층적 거리 관계가 왜곡된다. 예를 들어, 구문적으로 가까운 "cat"과 "sat"의 거리가 의미적으로만 가까운 "cat"과 "mat"의 거리보다 멀어질 수 있다.

2. **지수적 관계의 선형화 문제**: WordNet과 같은 의미 계층에서, 하위 개념의 수는 깊이에 따라 지수적으로 증가한다. 유클리드 공간에서는 이러한 지수적 성장을 표현하기 위해 차원이 선형적으로 증가해야 하는 모순이 발생한다.

3. **거리 역설**: 고차원 유클리드 공간에서는 모든 점들이 서로 비슷한 거리를 가지는 "차원의 저주" 현상이 발생한다. 이는 의미적 유사도를 거리로 표현하려는 임베딩의 근본 목적과 충돌한다.

### 1.2. 신경과학적 영감: 뇌의 압축 메커니즘

RBE의 핵심 아이디어는 포유류 뇌의 기억 시스템, 특히 해마-피질 상호작용에서 영감을 받았다. 최근 신경과학 연구들은 뇌가 정보를 효율적으로 압축하고 저장하는 놀라운 메커니즘을 밝혀내고 있다:

#### 1.2.1. 해마의 인덱싱 메커니즘

해마는 대뇌피질의 방대한 정보를 극도로 압축된 "인덱스"로 변환한다. 이 과정은 다음과 같은 특징을 보인다:

- **희소 표상 (Sparse Representation)**: CA3 영역의 피라미드 세포들은 전체의 2-5%만 활성화되어 정보를 표현
- **패턴 분리 (Pattern Separation)**: 유사한 입력을 구별 가능한 희소 코드로 변환
- **패턴 완성 (Pattern Completion)**: 부분적 단서로부터 전체 기억을 재구성

이는 RBE가 신경망 가중치를 비트필드로 압축하고, 기저 함수를 통해 재구성하는 과정과 놀라울 정도로 유사하다.

#### 1.2.2. 그리드 셀과 쌍곡 기하학

내후각피질(Entorhinal Cortex)의 그리드 셀은 육각형 격자 패턴으로 공간을 표현한다. 흥미롭게도, 이 패턴은:

- **쌍곡 공간의 테셀레이션**과 수학적으로 동형
- **계층적 구조**를 효율적으로 인코딩
- **다중 스케일** 표상을 동시에 지원

이러한 발견은 RBE가 쌍곡 기하학을 채택한 이론적 근거를 제공한다.

#### 1.2.3. 신경 리만 다양체

최근 연구들은 신경 활동이 단순한 유클리드 공간이 아닌 **곡률을 가진 리만 다양체**에서 일어난다는 증거를 제시한다:

$$\mathcal{M}_{\text{neural}} = (\mathbb{R}^n, g_{ij}(x))$$

여기서 계량 텐서 $g_{ij}$는 시냅스 연결 강도와 관련되며, 학습은 이 기하학적 구조를 변형시키는 과정으로 해석된다. RBE는 이러한 신경과학적 통찰을 공학적으로 구현한 것이다.

### 1.3. 제안: 리만 기하학 기저 인코딩 (RBE)

본 논문은 위의 두 딜레마를 동시에 해결하는 혁신적인 접근법, **리만 기하학 기저 인코딩(Riemannian Basis Encoding, RBE)**을 제안한다. RBE는 다음과 같은 핵심 통찰에 기반한다:

**"가중치 행렬은 단순한 숫자의 배열이 아니라, 하나의 기하학적 공간에서 다른 공간으로의 변환(transformation)을 나타낸다. 이 변환의 본질적 구조는 해당 공간의 기하학적 특성에 의해 결정되며, 이를 활용하면 극도로 효율적인 표현이 가능하다."**

#### 1.3.1. RBE의 핵심 구성 요소

RBE는 거대한 가중치 행렬 $W \in \mathbb{R}^{m \times n}$을 다음의 세 가지 핵심 요소로 분해한다:

1. **기저 테이블 (Basis Table) $\mathcal{B} = \{b_j\}_{j=1}^B$**: 
   - 대상 리만 공간(예: 푸앵카레 볼, 로렌츠 모델)에 최적화된 $B$개의 단위 방향 벡터들
   - 각 기저는 해당 공간의 대칭성과 기하학적 특성을 반영하도록 신중히 선택됨
   - 전체 모델에서 공유되므로 메모리 오버헤드는 무시할 만함

2. **기저 청사진 (Basis Blueprint) $W_{\text{codes}}$**:
   - 각 가중치 행에 대해 22-32비트의 비트필드로 인코딩된 압축 표현
   - 비트필드 구조: `[amp_fine|cat|sub|idx|sign|deriv|amp]`
     - `idx` (8비트): 256개 기저 중 하나를 선택
     - `cat, sub` (각 2비트): 64가지 리만 기저 함수 중 하나를 선택
     - `amp, amp_fine` (총 18비트): 선택된 기저 방향의 크기를 정밀하게 표현
     - `sign, deriv` (각 1비트): 부호와 미분 차수
   - 원본 대비 100-1000배의 압축률 달성

3. **잔차 행렬 (Residual Matrix) $W_{\text{res}}$**:
   - 청사진만으로 포착되지 않는 미세한 디테일을 보정
   - FP8 또는 INT8 정밀도로 양자화하여 추가 압축
   - 원본 가중치의 5-10% 수준의 작은 값들로 구성

#### 1.3.2. 수학적 정식화

가중치 행렬의 각 행 $w_i$는 다음과 같이 정확하게 재구성된다:

$$w_i = \underbrace{s_i \cdot b_{\text{idx}_i}}_{\text{기저 청사진 기여}} + \underbrace{w_{\text{res}, i}}_{\text{잔차 기여}}$$

여기서:
- $b_{\text{idx}_i} \in \mathcal{B}$: 비트필드에서 디코딩된 인덱스가 가리키는 기저 벡터
- $s_i = \mathcal{F}_{\text{cat}_i, \text{sub}_i}(\text{amp}_i, \text{amp\_fine}_i)$: 선택된 리만 기저 함수가 계산한 스케일 값
- $w_{\text{res}, i}$: 해당 행의 잔차 벡터

#### 1.3.3. RBE의 핵심 기여

`Reality Stone` 프레임워크를 통해 구현된 RBE는 다음과 같은 획기적인 기여를 제공한다:

1. **극한의 압축률과 정확도 보존**:
   - GPT-2 규모 모델에서 186배 압축률 달성
   - 압축 후에도 98.6% 이상의 정확도 유지
   - 이론적으로 350B 모델을 6.9GB로 압축 가능

2. **압축 상태 직접 추론 (Compressed-Domain Inference)**:
   - 가중치를 복원하지 않고 비트필드에서 직접 행렬 연산 수행
   - GPU의 공유 메모리와 텐서 코어를 최대한 활용하는 CUDA 커널
   - 기존 대비 3-4배의 추론 속도 향상

3. **학습 가능한 압축 (Quantization-Aware Training, QAT)**:
   - 비트 연산의 대각 야코비안 해석을 통한 미분 가능성 확보
   - Straight-Through Estimator를 활용한 엔드투엔드 학습
   - 압축으로 인한 성능 저하를 학습 과정에서 자동 보상

4. **쌍곡 BERT (Hb-BERT)의 실증적 성공**:
   - 표준 BERT를 쌍곡 공간으로 이전하고 RBE 적용
   - 언어의 계층적 구조를 더 효과적으로 포착
   - 동일 파라미터 수 대비 우수한 성능 달성

본 논문의 나머지 부분은 다음과 같이 구성된다. 2장에서는 리만 다양체, 특히 쌍곡 기하학 모델들의 수학적 기초와 신경과학적 연관성을 상세히 다룬다. 3장에서는 RBE의 핵심 알고리즘, 즉 기저 함수 설계, 비트필드 인코딩 과정, 압축 도메인 추론, 그리고 QAT를 통한 학습 과정을 단계별로 설명한다. 4장에서는 정보 이론과 신호 처리 관점에서 RBE의 이론적 최적성을 분석한다. 5장에서는 Rust/CUDA로 구현된 `Reality Stone` 시스템의 아키텍처와 최적화 기법을 소개한다. 6장에서는 다양한 모델과 데이터셋에 대한 광범위한 실험을 통해 RBE의 효과를 검증하며, 특히 `Hb-BERT`의 SOTA 달성 가능성을 탐구한다. 마지막으로 7장에서는 본 연구의 의의와 향후 연구 방향을 제시한다. 