## 4. 이론적 분석 및 보장

본 장에서는 3장에서 제안한 방법론들의 수학적 정합성과 안정성을 엄밀히 증명한다. 각 정리는 제안된 방법이 단순한 경험적 트릭이 아니라, 견고한 수학적 원리 위에서 작동함을 보장하는 역할을 한다.

### 4.1 Hyper-Butterfly 레이어의 보편적 근사 능력

**의미**: 이 정리는 Hyper-Butterfly 레이어를 충분히 쌓으면, 이론적으로 어떤 복잡한 함수든 원하는 정밀도로 흉내 낼 수 있음을 보장한다. 즉, 이 레이어가 단순히 빠른 연산자를 넘어, 충분한 표현력을 갖춘 '만능 함수 근사기'로서 작동할 수 있음을 의미한다.

**정리 4.1 (Universal Approximation Theorem for Hyper-Butterfly)**
Hyper-Butterfly 레이어들의 집합으로 구성된 함수 대수(algebra) $\mathcal{F}=\{\exp_0^c \circ B \circ \log_0^c\}$는 컴팩트 리만 다양체 $M$ 위의 연속 함수 공간 $C(M)$에서 조밀(dense)하다. 즉, 임의의 연속 함수 $h \in C(M)$와 임의의 $\varepsilon > 0$에 대해, 다음을 만족하는 $f \in \mathcal{F}$가 존재한다.
$$ \sup_{x \in M} |f(x) - h(x)| < \varepsilon $$

**증명 개요**:
이 증명은 강력한 수학적 도구인 **Stone-Weierstrass 정리**를 기반으로 한다. 어떤 함수들의 집합(함수 대수 $\mathcal{F}$)이 공간의 모든 연속 함수를 근사할 수 있음을 보이려면, 다음 세 가지 조건이 만족됨을 보이면 충분하다.

1.  **대수 구조 형성**: $f, g \in \mathcal{F}$이면, $f+g$와 $f \cdot g$도 (더 깊거나 넓은) Hyper-Butterfly 네트워크로 표현 가능해야 한다. 이는 네트워크의 병렬 및 직렬 결합을 통해 쉽게 만족된다.
2.  **상수 함수 포함**: $f(x)=c$ 와 같은 상수 함수를 만들 수 있어야 한다. 이는 Butterfly 변환을 항등 행렬로 두고 bias 항을 추가함으로써 가능하다.
3.  **점 분리 (Point-Separating)**: 공간 위의 임의의 두 점 $x_1 \neq x_2$에 대해, $f(x_1) \neq f(x_2)$가 되도록 하는 함수 $f \in \mathcal{F}$를 항상 찾을 수 있어야 한다. $\log_0^c$가 단사함수(일대일 함수)이고, 선형 변환 $B$를 풀랭크(full-rank) 행렬로 선택하면 두 점을 다른 곳으로 보낼 수 있으며, $\exp_0^c$ 역시 단사함수이므로 이 조건은 만족된다.

위 세 조건이 모두 충족되므로, Stone-Weierstrass 정리에 의해 Hyper-Butterfly 네트워크는 보편적 근사 능력을 가진다.

### 4.2 수치적 안정성 분석

**의미**: 이 분석은 Hyper-Butterfly 연산이 수치적으로 안정적임을 보장한다. 즉, 입력의 작은 변화가 출력의 폭발적인 변화로 이어지지 않으며(조건수 바운딩), 딥러닝 학습 과정에서 그래디언트가 소실되거나 폭발하지 않고 안정적으로 전파됨(그래디언트 안정성)을 의미한다. 이는 모델이 안정적으로 학습될 수 있다는 '품질 보증서'와 같다.

**정리 4.2 (Condition Number Bounding)**
Hyper-Butterfly 함수 $f(x) = \exp_0^c(B(\log_0^c(x)))$의 야코비안 행렬 $Df(x)$의 조건수 $\kappa(f, x) = \|Df(x)\|_2 \|(Df(x))^{-1}\|_2$는 유한한 값으로 상한이 정해진다(bounded).

**증명 개요**:
$f$는 세 함수의 합성($f = \exp_0^c \circ B \circ \log_0^c$)이므로, 미분의 연쇄 법칙에 의해 야코비안은 $Df(x) = D\exp_0^c(v) \cdot B \cdot D\log_0^c(x)$ 이다 (단, $v=B(\log_0^c(x))$). 조건수는 각 요소의 조건수의 곱보다 작거나 같다.
$$ \kappa(f,x) \le \kappa(\exp_0^c) \cdot \kappa(B) \cdot \kappa(\log_0^c) $$
-   $\kappa(B)$: $B$는 파라미터가 학습되는 일반적인 선형 변환이므로 조건수가 유한하다.
-   $\kappa(\log_0^c)$, $\kappa(\exp_0^c)$: 이 함수들의 미분값은 $\|x\|$가 1에 가까워질 때 무한해질 수 있지만, 푸앵카레 원반의 정의상 모든 점은 $\|x\| < 1$인 내부에 존재하므로 항상 유한한 값을 가진다.
따라서 전체 조건수는 유한한 값들의 곱이므로 유한하며, 이는 수치적 안정성을 의미한다.

**정리 4.3 (Gradient Stability)**
Hyper-Butterfly 레이어의 역전파 과정에서 그래디언트의 노름은 폭발하거나 소실되지 않고 안정적으로 전파된다. 즉, 손실 함수 $L$에 대해 출력 그래디언트 $\nabla_y L$과 입력 그래디언트 $\nabla_x L$ 사이에는 다음 관계가 성립한다.
$$ \|\nabla_x L\|_2 \le C_g \|\nabla_y L\|_2 $$
여기서 $C_g$는 네트워크의 깊이나 차원에 무관한 상수이다.

**증명 개요**:
역전파 알고리즘에 따르면, 입력에 대한 그래디언트는 출력에 대한 그래디언트와 야코비안의 전치 행렬의 곱으로 계산된다: $\nabla_x L = (Df(x))^T \nabla_y L$. 양변에 노름을 취하고 행렬 노름의 성질($\|Ax\| \le \|A\|\|x\|$)을 적용하면 다음과 같다.
$$ \|\nabla_x L\|_2 = \|(Df(x))^T \nabla_y L\|_2 \le \|(Df(x))^T\|_2 \|\nabla_y L\|_2 = \|Df(x)\|_2 \|\nabla_y L\|_2 $$
$Df(x)$의 스펙트럼 노름 $\|Df(x)\|_2$는 조건수 분석에서 유한하게 바운딩됨을 보였으므로, 이 값을 $C_g$로 둘 수 있다. 따라서 입력 그래디언트의 크기는 출력 그래디언트의 크기에 비례하여 안정적으로 제어된다.

### 4.3 압축 방법론의 오차 분석

**의미**: 이 분석은 스플라인과 같은 방법으로 가중치 행렬을 압축할 때, 제어점의 수를 늘리면 표현 오차가 얼마나 빠르게 줄어드는지를 이론적으로 보장한다. 이는 적은 수의 파라미터로도 원본 가중치를 매우 효과적으로 근사할 수 있음을 시사한다.

**정리 4.4 (Approximation Error for Splines)**
충분히 매끄러운 함수 $W(t): [0,1] \to \mathbb{R}^n$를 $k$개의 제어점을 사용하여 $d$차 스플라인으로 보간할 때, 최대 오차는 제어점 간의 간격 $h \approx 1/k$에 대해 다음과 같이 바운딩된다.
$$ \sup_{t \in [0,1]} \|W(t) - S(t)\| \le C \cdot h^{d+1} = O(k^{-(d+1)}) $$
여기서 $C$는 $W(t)$의 $(d+1)$차 도함수에 의존하는 상수이다.

**해설**:
이 정리는 스플라인 압축의 효율성을 직접적으로 보여준다. 가중치 행렬의 행들을 하나의 매끄러운 곡선 $W(t)$로 간주할 때, 이 곡선을 $k$개의 제어점으로 근사하는 오차는 $k$가 커짐에 따라 $k^{-(d+1)}$ 비율로 매우 빠르게 감소한다. 예를 들어, 3차 스플라인($d=3$)을 사용하면 제어점 수를 2배로 늘릴 때마다 오차는 $2^4=16$배씩 줄어든다. 이는 스플라인 압축이 이론적으로 매우 효율적인 표현 방식임을 의미한다. 이 원리는 유클리드 공간뿐만 아니라, 측지선 보간을 사용하는 리만 다양체 위에서도 유사하게 적용된다. 