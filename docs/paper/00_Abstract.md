## Abstract

대규모 언어 모델(LLM)의 발전은 막대한 연산 및 메모리 요구량을 동반하며, 모델의 배포와 접근성에 근본적인 장벽이 되고 있다. 본 연구에서는 이러한 문제를 해결하기 위해, 신경망의 가중치 공간에 내재된 기하학적 구조를 활용하는 새로운 압축 패러다임, **리만 기하학 기저 인코딩(Riemannian Basis Encoding, RBE)** 을 제안한다.

RBE의 핵심 아이디어는 거대한 가중치 행렬을 직접 저장하는 대신, 해당 리만 공간(예: 쌍곡 공간)을 효율적으로 표현하는 소수의 **기저 함수(Basis Functions)** 들과 그 조합법을 담은 **기저 청사진(Basis Blueprint)**, 그리고 근사 오차를 보정하는 미세한 **잔차(Residual)** 로 분해하는 것이다. 이 청사진은 극도로 압축된 비트 필드(Bitfield) 형태로 인코딩되어, 메모리 사용량을 획기적으로 줄인다.

본 논문은 RBE 프레임워크인 `Reality Stone`의 설계와 구현을 상세히 기술한다. 주요 기여는 다음과 같다: **(1)** 리만 기하학, 특히 푸앵카레(Poincaré), 로렌츠(Lorentz), 클라인(Klein) 모델에 최적화된 기저 함수 집합을 정의하고, 이를 선택적으로 조합하여 복잡한 가중치 분포를 표현하는 RBE 알고리즘을 개발했다. **(2)** 가중치 행을 단 22비트의 청사진과 작은 잔차 행렬로 분해하여 **186배의 압축률**을 달성하면서도 **98.6% 이상의 정확도**를 보존함을 실험적으로 입증했다. **(3)** 압축된 청사진과 잔차로부터 직접 추론하는 고성능 Rust/CUDA 커널을 구현하여, 기존 모델 대비 **3-4배의 추론 가속**을 달성했다.

`Reality Stone`은 단순한 압축 기술을 넘어, 모델의 가중치를 기하학적 구조의 관점에서 재해석하는 새로운 접근법을 제시한다. 이론적 분석을 통해 350B 파라미터 모델을 약 6.9GB로 압축할 수 있는 잠재력을 보였으며, 이는 최첨단 AI의 접근성을 혁신적으로 높여 개인용 기기에서의 구동 가능성을 연다. 본 연구는 리만 기하학과 딥러닝의 융합이 모델 효율성을 극대화할 수 있는 강력한 경로임을 증명한다. 