## 3. 방법론: Reality Stone의 핵심 구성 요소

`Reality Stone`은 연산 효율성과 파라미터 효율성을 달성하기 위해 두 가지 핵심 기술, 즉 Hyper-Butterfly 레이어와 스플라인 가중치 압축을 사용한다.

### 3.1 Hyper-Butterfly 레이어: 스펙트럼 기반 고속 연산자

Hyper-Butterfly 레이어는 기존 신경망의 밀집 행렬 곱셈($y=Wx$)을 저비용, 저파라미터 연산으로 대체하기 위해 설계되었다. 이 구조는 고속 푸리에 변환(FFT)의 나비(Butterfly) 연산 구조에서 영감을 받았다. **FFT가 복잡한 신호를 간단한 주기 함수(사인, 코사인)의 합으로 분해하듯, Hyper-Butterfly는 복잡한 선형 변환을 간단한 회전과 스케일링의 조합(버터플라이 연산)으로 분해**하여 $O(N \log N)$ 복잡도를 달성한다.

쌍곡 공간은 휘어져 있어 직접적인 선형 변환이 불가능하므로, 다음과 같은 3단계 과정을 거친다. 이는 **"휘어진 공간의 데이터를 평평한 작업대로 옮겨(Log Map) 계산하고, 다시 원래의 휘어진 공간으로 되돌려놓는(Exp Map)"** 과정으로 비유할 수 있다.

1.  **로그 사상 (Log Map)**: 입력 $x$를 쌍곡 다양체(푸앵카레 원반)의 점에서 원점의 접공간(유클리드 공간)에 있는 벡터 $u$로 매핑한다.
    $$
    u = \log_0^c(x) = \underbrace{\frac{\tanh^{-1}(\sqrt{c}\|x\|)}{\sqrt{c}\|x\|}}_{\text{스케일링 팩터}} \cdot x
    $$
    -   $\|x\|$: 원점으로부터의 유클리드 거리
    -   $\sqrt{c}\|x\|$: 곡률을 고려한 거리
    -   $\tanh^{-1}(\cdot)$: 유클리드 길이를 쌍곡 공간의 실제 길이로 변환하는 함수. 전체 스케일링 팩터는 $x$를 접공간 벡터 $u$로 변환할 때 필요한 크기 조정을 수행한다.

2.  **버터플라이 변환 (Butterfly Transform)**: 평평한 접공간 상의 벡터 $u$에 연속적인 버터플라이 행렬 곱을 적용하여 목표 벡터 $v$를 얻는다. $N=2^L$일 때, $L$개의 희소 행렬 $B_1, \dots, B_L$의 곱으로 전체 선형 변환 $W$를 근사한다.
    $$
    v = (B_L B_{L-1} \cdots B_1) u \approx W u
    $$
    각 버터플라이 행렬 $B_\ell$은 $2 \times 2$ 블록 대각 행렬 형태로, 파라미터 수가 $O(N)$에 불과하다. 따라서 전체 변환의 파라미터와 연산 복잡도는 $O(N \log N)$이 된다.

3.  **지수 사상 (Exp Map)**: 변환된 벡터 $v$를 다시 쌍곡 다양체 위의 점 $y$로 매핑한다.
    $$
    y = \exp_0^c(v) = \underbrace{\tanh\bigl(\sqrt{c}\|v\|\bigr) \frac{v}{\sqrt{c}\|v\|}}_{\text{스케일링 팩터}}
    $$
    -   $\tanh(\cdot)$: 접공간 벡터의 길이를 쌍곡 공간의 점의 유클리드 길이로 변환하는 함수. 로그 사상의 역과정이다.

이 구조를 통해 밀집 행렬 $W \in \mathbb{R}^{N \times N}$이 요구하는 $O(N^2)$개의 파라미터와 연산량을 $O(N \log N)$ 수준으로 크게 절감할 수 있다.

### 3.2 스플라인 가중치 압축: 기하학적 파라미터화

대규모 언어 모델의 가중치 행렬 $W \in \mathbb{R}^{m \times n}$은 수백만에서 수십억 개의 파라미터를 포함한다. 스플라인 압축은 이 거대한 파라미터의 집합을 하나의 연속적인 기하학적 형태로 보고 압축하는 방법론이다.

핵심 아이디어는 **수백만 개의 별자리 좌표($m$개의 행 벡터 $\{W_{i,:}\}_{i=1}^m$)를 모두 저장하는 대신, 그 별들을 잇는 부드러운 '궤적'(스플라인 곡선)을 그리는 몇 개의 주요 별(제어점) 좌표만 저장**하는 것이다.

1.  **파라미터화 (Parameterization)**:
    -   **제어점(Control Points)**: 곡선의 모양을 결정하는 $(k+1)$개의 제어점 $\{P_j\}_{j=0}^k \subset \mathbb{R}^n$을 학습 파라미터로 정의한다 ($k \ll m$).
    -   **스플라인 함수 (Spline Function)**: 이 제어점들을 보간(interpolate)하여 매끄러운 곡선 $S(t): [0, k] \to \mathbb{R}^n$을 생성한다.

2.  **가중치 복원 (Weight Reconstruction)**:
    -   원본 가중치 행렬의 각 행 $W_{i,:}$는 스플라인 곡선 위의 특정 파라미터 $t_i$에 해당하는 점 $S(t_i)$로 복원된다. $t_i$는 곡선의 시작점($t=0$)부터 끝점($t=k$)까지의 경로를 등분하여 각 행 벡터가 위치할 지점을 지정한다.
        $$
        W_{i,:} = S(t_i), \quad \text{where} \quad t_i = \frac{i-1}{m-1} \cdot k \quad \text{for } i=1, \dots, m
        $$
    -   따라서, 학습과 추론 시에는 $(k+1) \times n$개의 제어점 파라미터만 저장하고, 실제 연산이 필요할 때마다 이로부터 전체 가중치 행렬 $W$를 복원하여 사용한다.

3.  **리만 스플라인으로의 확장 (Riemannian Spline)**:
    -   위 과정은 평평한 유클리드 공간을 가정한다. 이를 **휘어진 리만 다양체(예: 쌍곡 공간)로 확장**할 수 있다. 이는 "평평한 종이 위에서 자를 대고 점들을 잇는 것"과 "둥근 지구 표면 위에서 최단 항로(측지선)를 따라 점들을 잇는 것"의 차이와 같다.
    -   리만 스플라인은 제어점 사이를 직선이 아닌 **측지선(geodesic)으로 보간**하여, 데이터의 내재적 기하학 구조를 존중하는 더 정교한 압축을 가능하게 한다.

### 3.3 압축률 분석

스플라인 압축 방식의 압축률은 원본 파라미터 수 대비 제어점의 파라미터 수로 결정된다.

-   **원본 파라미터 수**: $N_{orig} = m \times n$
-   **압축 후 파라미터 수**: $N_{comp} = (k+1) \times n$ (제어점)
-   **압축률 (Compression Ratio)**:
    $$
    \text{Ratio} = \frac{N_{comp}}{N_{orig}} = \frac{(k+1)n}{mn} = \frac{k+1}{m}
    $$

예를 들어, 3072개의 행 벡터를 가진 레이어($m=3072$)에 대해 제어점을 50개($k+1=50$)만 사용한다면, 압축률은 $50 / 3072 \approx 1.6\%$가 된다. 즉, 파라미터 수를 98% 이상 절감할 수 있다.

**중요한 전제 조건**: 이와 같은 높은 압축률은 모델 학습 초기부터 가중치가 스플라인 곡선 위에 존재한다는 제약 하에 훈련될 때 달성 가능하다. 이미 학습된 모델에 사후 적용할 경우 정보 손실이 발생할 수 있다.

### 3.4 HeightField 방식: 안정성을 중시한 파라미터화

스플라인 방식이 높은 압축률을 제공하는 반면, 모든 가중치를 하나의 곡선 위에 놓는 강한 제약으로 인해 표현력 손실이 발생할 수 있다. HeightField 방식은 압축률을 일부 희생하는 대신, 원본 가중치의 자유도를 거의 그대로 보존하여 안정성을 극대화하는 접근법이다.

-   **아이디어**: **산의 모든 지점의 3차원 좌표(x, y, z)를 저장하는 대신, 2차원 지도 상의 위치(x, y)와 그 위치에 해당하는 고도(height)를 알려주는 '고도 지도(height map)'를 만드는 것**과 유사하다. 가중치 행렬의 각 행 $W_{i,:} \in \mathbb{R}^n$을 $(n-1)$차원의 좌표 벡터 $u_i \in \mathbb{R}^{n-1}$와, 그 좌표에 의해 결정되는 높이(고도) 값 $h(u_i) \in \mathbb{R}$의 결합으로 본다. 즉, $W_{i,:} = [u_i, h(u_i)]$.
-   **파라미터화**:
    -   학습 가능한 파라미터는 $m \times (n-1)$개의 좌표 $u_i$와, 높이 함수 $h(\cdot)$ 자체를 근사하는 작은 MLP의 가중치 $P$개이다.
    -   총 파라미터 수: $N_{comp} = m \times (n-1) + P$.
-   **압축률**: MLP의 크기 $P$가 $m$에 비해 매우 작다고 가정하면,
$$ \text{Ratio} \approx \frac{m(n-1)}{mn} = 1 - \frac{1}{n} $$

이는 $n=768$일 경우 약 99.87%로, 압축 효과는 미미하지만 원본 가중치의 정보를 거의 손실 없이 파라미터화할 수 있다는 장점이 있다. 