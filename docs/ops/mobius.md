# MÃ¶bius ì—°ì‚° (Poincare Disk Model)

í¬ì¸ì¹´ë ˆ ë””ìŠ¤í¬ ëª¨ë¸ì—ì„œì˜ í•µì‹¬ ì—°ì‚°ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤. MÃ¶bius ë³€í™˜ì€ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œì˜ ê¸°ë³¸ì ì¸ ì‚°ìˆ  ì—°ì‚°ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“ ìˆ˜í•™ì  ë°°ê²½

### í¬ì¸ì¹´ë ˆ ë””ìŠ¤í¬ ëª¨ë¸
í¬ì¸ì¹´ë ˆ ë””ìŠ¤í¬ $\mathbb{D}^n = \{x \in \mathbb{R}^n : \|x\| < 1\}$ëŠ” í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì„ ìœ í´ë¦¬ë“œ ë‹¨ìœ„ ì›íŒì— ì‚¬ì˜í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

**ê³„ëŸ‰ í…ì„œ**:
$$g_{ij} = \frac{4\delta_{ij}}{(1-\|x\|^2)^2}$$

**ê³¡ë¥  ë§¤ê°œë³€ìˆ˜**: $c > 0$ (ì–‘ìˆ˜ ê³¡ë¥ )

### MÃ¶bius ë§ì…ˆ (MÃ¶bius Addition)

ë‘ ì  $u, v \in \mathbb{D}^n$ì— ëŒ€í•œ MÃ¶bius ë§ì…ˆ:

$$u \oplus_c v = \frac{(1 + 2c\langle u,v \rangle + c\|v\|^2)u + (1-c\|u\|^2)v}{1 + 2c\langle u,v \rangle + c^2\|u\|^2\|v\|^2}$$

**ê¸°í•˜í•™ì  ì˜ë¯¸**: 
- $u$ë¥¼ ì›ì ìœ¼ë¡œ ì´ë™ì‹œí‚¤ëŠ” ë“±ê±°ë¦¬ ë³€í™˜ í›„ $v$ë¥¼ ë”í•˜ëŠ” ì—°ì‚°
- í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œì˜ "í‰í–‰ì´ë™"

**íŠ¹ìˆ˜ ê²½ìš°**:
- $u = 0$ì¼ ë•Œ: $0 \oplus_c v = v$
- $c = 0$ì¼ ë•Œ: ìœ í´ë¦¬ë“œ ë§ì…ˆìœ¼ë¡œ ìˆ˜ë ´

### MÃ¶bius ìŠ¤ì¹¼ë¼ ê³±ì…ˆ (MÃ¶bius Scalar Multiplication)

ì  $u \in \mathbb{D}^n$ê³¼ ìŠ¤ì¹¼ë¼ $r \in \mathbb{R}$ì— ëŒ€í•´:

$$r \otimes_c u = \frac{1}{\sqrt{c}} \tanh\left(r \cdot \text{artanh}(\sqrt{c}\|u\|)\right) \frac{u}{\|u\|}$$

**ê¸°í•˜í•™ì  ì˜ë¯¸**:
- ì›ì ì—ì„œ $u$ ë°©í–¥ìœ¼ë¡œì˜ ì¸¡ì§€ì„ ìƒì—ì„œ ê±°ë¦¬ ìŠ¤ì¼€ì¼ë§
- $r > 1$: ì›ì ì—ì„œ ë©€ì–´ì§
- $0 < r < 1$: ì›ì ì— ê°€ê¹Œì›Œì§

## ğŸ”§ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### íŒŒì¼ êµ¬ì¡°
```
src/core/ops/
â”œâ”€â”€ mobius_cpu.cpp      # CPU êµ¬í˜„
â””â”€â”€ mobius_cuda.cu      # CUDA êµ¬í˜„

src/include/ops/
â””â”€â”€ mobius.h            # í•¨ìˆ˜ ì„ ì–¸
```

### CPU êµ¬í˜„ (`mobius_cpu.cpp`)

```cpp
torch::Tensor mobius_add_cpu(torch::Tensor u, torch::Tensor v, float c) {
    // ì•ˆì „í•œ ê³¡ë¥  í´ë¦¬í•‘
    float safe_c = std::max(c, 1e-6f);
    
    // ë‚´ì  ê³„ì‚°: <u,v>
    auto uv_dot = torch::sum(u * v, -1, true);
    
    // ë…¸ë¦„ ì œê³± ê³„ì‚°
    auto u_norm_sq = torch::sum(u * u, -1, true);
    auto v_norm_sq = torch::sum(v * v, -1, true);
    
    // ë¶„ì ê³„ì‚°
    auto numerator_u = u * (1 + 2 * safe_c * uv_dot + safe_c * v_norm_sq);
    auto numerator_v = v * (1 - safe_c * u_norm_sq);
    auto numerator = numerator_u + numerator_v;
    
    // ë¶„ëª¨ ê³„ì‚°
    auto denominator = 1 + 2 * safe_c * uv_dot + 
                      safe_c * safe_c * u_norm_sq * v_norm_sq;
    
    // ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ í´ë¦¬í•‘
    denominator = torch::clamp(denominator, 1e-6);
    
    return numerator / denominator;
}
```

**í•µì‹¬ ìµœì í™”**:
1. **ì•ˆì „í•œ ê³¡ë¥ **: `c`ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ìˆ˜ì¹˜ ì˜¤ì°¨ ë°œìƒ ë°©ì§€
2. **ë²¡í„°í™” ì—°ì‚°**: ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ broadcasting í™œìš©
3. **ìˆ˜ì¹˜ì  ì•ˆì •ì„±**: ë¶„ëª¨ê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒì„ ë°©ì§€

### CUDA êµ¬í˜„ (`mobius_cuda.cu`)

```cuda
__global__ void mobius_add_kernel(
    const float* u, const float* v, float* result, 
    float c, int batch_size, int dim
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid >= batch_size) return;
    
    const float* u_batch = u + tid * dim;
    const float* v_batch = v + tid * dim;
    float* result_batch = result + tid * dim;
    
    // ë‚´ì ê³¼ ë…¸ë¦„ ê³„ì‚°
    float uv_dot = 0.0f, u_norm_sq = 0.0f, v_norm_sq = 0.0f;
    
    for (int i = 0; i < dim; i++) {
        uv_dot += u_batch[i] * v_batch[i];
        u_norm_sq += u_batch[i] * u_batch[i];
        v_norm_sq += v_batch[i] * v_batch[i];
    }
    
    // MÃ¶bius ë§ì…ˆ ê³µì‹ ì ìš©
    float c_safe = fmaxf(c, 1e-6f);
    float factor1 = 1.0f + 2.0f * c_safe * uv_dot + c_safe * v_norm_sq;
    float factor2 = 1.0f - c_safe * u_norm_sq;
    float denom = 1.0f + 2.0f * c_safe * uv_dot + 
                  c_safe * c_safe * u_norm_sq * v_norm_sq;
    denom = fmaxf(denom, 1e-6f);
    
    for (int i = 0; i < dim; i++) {
        result_batch[i] = (factor1 * u_batch[i] + factor2 * v_batch[i]) / denom;
    }
}
```

**CUDA ìµœì í™”**:
1. **Coalesced Memory Access**: ì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´
2. **Thread-Level Parallelism**: ë°°ì¹˜ë³„ ë³‘ë ¬ ì²˜ë¦¬
3. **Shared Memory í™œìš©**: ì°¨í›„ ìµœì í™”ì—ì„œ í™œìš© ê°€ëŠ¥

## âš¡ ì„±ëŠ¥ íŠ¹ì„±

### ê³„ì‚° ë³µì¡ë„
- **ì‹œê°„ ë³µì¡ë„**: $O(nd)$ (ë°°ì¹˜ í¬ê¸° $n$, ì°¨ì› $d$)
- **ê³µê°„ ë³µì¡ë„**: $O(nd)$
- **CUDA ì²˜ë¦¬ëŸ‰**: ~10GB/s (RTX 3090 ê¸°ì¤€)

### ìˆ˜ì¹˜ì  ì•ˆì •ì„± ê³ ë ¤ì‚¬í•­

1. **ê²½ê³„ ê·¼ì²˜ ë¬¸ì œ**: $\|x\| \rightarrow 1$ì¼ ë•Œ ë°œì‚° ê°€ëŠ¥ì„±
   ```cpp
   // ì•ˆì „í•œ ê²½ê³„ í´ë¦¬í•‘
   auto norm = torch::norm(x, 2, -1, true);
   auto clipped = torch::where(norm >= 0.99, 
                              x * 0.99 / norm, x);
   ```

2. **ì‘ì€ ê³¡ë¥  ë¬¸ì œ**: $c \rightarrow 0$ì¼ ë•Œ ìˆ˜ì¹˜ ì˜¤ì°¨
   ```cpp
   float safe_c = std::max(c, 1e-6f);
   ```

3. **ì–¸ë”í”Œë¡œìš°/ì˜¤ë²„í”Œë¡œìš° ë°©ì§€**: 
   ```cpp
   auto result = torch::clamp(mobius_result, -1e6, 1e6);
   ```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

### ìˆ˜í•™ì  ì„±ì§ˆ ê²€ì¦

1. **í•­ë“±ì› ì„±ì§ˆ**: $0 \oplus_c x = x$
2. **êµí™˜ë²•ì¹™**: $u \oplus_c v = v \oplus_c u$ (ì¼ë°˜ì ìœ¼ë¡œ ì„±ë¦½í•˜ì§€ ì•ŠìŒ)
3. **ê²°í•©ë²•ì¹™**: $(u \oplus_c v) \oplus_c w \neq u \oplus_c (v \oplus_c w)$
4. **ì—­ì› ì¡´ì¬**: $u \oplus_c (-u \oplus_c 0) = 0$

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
import torch
import time

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
batch_size = 1000
dim = 512
x = torch.randn(batch_size, dim) * 0.1
y = torch.randn(batch_size, dim) * 0.1

# CPU ë²¤ì¹˜ë§ˆí¬
start = time.time()
for _ in range(100):
    result_cpu = mobius_add_cpu(x, y, 1.0)
cpu_time = time.time() - start

# CUDA ë²¤ì¹˜ë§ˆí¬ (GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ)
if torch.cuda.is_available():
    x_gpu = x.cuda()
    y_gpu = y.cuda()
    torch.cuda.synchronize()
    
    start = time.time()
    for _ in range(100):
        result_gpu = mobius_add_cuda(x_gpu, y_gpu, 1.0)
    torch.cuda.synchronize()
    gpu_time = time.time() - start
    
    print(f"CPU: {cpu_time:.4f}s, GPU: {gpu_time:.4f}s")
    print(f"Speedup: {cpu_time/gpu_time:.2f}x")
```

## ğŸ”— ê´€ë ¨ í•¨ìˆ˜ë“¤

- `mobius_scalar_cpu/cuda`: MÃ¶bius ìŠ¤ì¹¼ë¼ ê³±ì…ˆ
- `mobius_distance`: í•˜ì´í¼ë³¼ë¦­ ê±°ë¦¬ ê³„ì‚°  
- `exp_map_poincare`: ì§€ìˆ˜ ë§µí•‘
- `log_map_poincare`: ë¡œê·¸ ë§µí•‘

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

1. **Hyperbolic Neural Networks** - Ganea et al. (2018)
2. **PoincarÃ© Embeddings** - Nickel & Kiela (2017)  
3. **Geometry of Matrix Decompositions** - Absil et al. (2008)
4. **Riemannian Computing in Computer Vision** - Turaga et al. (2011) 